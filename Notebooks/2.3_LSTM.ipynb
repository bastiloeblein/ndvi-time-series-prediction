{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd46774d-ba61-488a-a70d-905cce6033f0",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTM networks are deep learning neural network. They use a gated structure to store information for extended periods using use memory cells. [@Vasilakos2022; @Reddy2018; @Guo2024].\n",
    "These memory cells include gates that regulate the information flow within the network. The structure of a memory cell includes four components: the forget gate, the input gate, the output gate and a cell state. [@Rhif2020]. \n",
    "- The forget gate decides which information from the previous cell state should be discarded.\n",
    "- The new information to be added to the cell state is determined by the input gate.\n",
    "- The cell state is updated by combining the previous cell state information from the forget gate and the new input gate information [@Sherif2023].\n",
    "- What the current cell state will output is decided by the output gate.\n",
    "Thus, the network can maintain and update its internal states to learn which information to retain and which to forget [@Reddy2018; @Sherif2023].\n",
    "\n",
    "## Overview\n",
    "- Loading the necessary packages\n",
    "- Defining the base directory\n",
    "- Initially, a parameter search is conducted using GridSearch (5.5 Gridsearch). During this process, data for a cube is loaded and prepared to enable the model to determine the optimal parameters.\n",
    "- Subsequently, the code for the final LSTM model is executed:\n",
    "    - Load and normalize the data for all utilized cubes\n",
    "    - Create sequences for the model\n",
    "    - Prepare the data for model training: flatten, reshape into the correct dimensions, combine data from all cubes, and mask out NaN values\n",
    "    - Construct the model\n",
    "    - Train the model\n",
    "    - Generate predictions for the test dataset\n",
    "    - Denormalize the predictions\n",
    "    - Save the predictions\n",
    "    \n",
    "## Requirements\n",
    "\n",
    "The notebook was build using GPU resources and the Kernel Python 3.9 TensorFlow 2.6.6 CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c17ff2-c3c8-4841-9cef-14ba011aa267",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6855b92-1cf7-47e0-a906-c4d2a96242d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7995f99-8b1d-4677-a8de-1a4cd0e5719f",
   "metadata": {},
   "source": [
    "## Define base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47558789-c03f-4181-babf-84167286623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe\n"
     ]
    }
   ],
   "source": [
    "# Define base_dir for consistent path management\n",
    "notebook_dir = Path(os.getcwd()).resolve()\n",
    "base_dir = notebook_dir\n",
    "print(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90368c3-15a1-4ec9-aa5f-f3ea2995da45",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "To find the best parameters for the model with the given data, we performed a Gridsearch.\n",
    "Unfortunately, the computational capacity was insufficient to conduct the Gridsearch for all 4 cubes.\n",
    "Thus, we only used one cube as data basis (Cube 665).\n",
    "In the following, the data of the cube is loaded and prepared for the model.\n",
    "It was then tested with all parameter combinations.\n",
    "The best parameters are presented as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422b01d-c8e2-4933-9795-5d2eb38e75aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_665_train.nc with shape: (292, 128, 128)\n",
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1203_train.nc with shape: (292, 128, 128)\n",
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_80_train.nc with shape: (292, 128, 128)\n",
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1301_train.nc with shape: (292, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1203_test.nc with shape: (93, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_80_test.nc with shape: (93, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1301_test.nc with shape: (93, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_665_test.nc with shape: (93, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    \"\"\"\n",
    "    Load and extract NDVI data from .nc files.\n",
    "\n",
    "    Parameters:\n",
    "    train_file (str): Path to the training data file.\n",
    "    test_file (str): Path to the testing data file.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Training and test NDVI data.\n",
    "    \"\"\"\n",
    "    ds_train = xr.open_dataset(train_file)\n",
    "    ndvi_train = ds_train['NDVI'].values\n",
    "    ds_test = xr.open_dataset(test_file)\n",
    "    ndvi_test = ds_test['NDVI'].values\n",
    "    return ndvi_train, ndvi_test\n",
    "\n",
    "def prepare_data(ndvi_train, ndvi_test, sequence_length, pred_length):\n",
    "    \"\"\"\n",
    "    Normalize and prepare data for LSTM model training.\n",
    "\n",
    "    Parameters:\n",
    "    ndvi_train (np.ndarray): Training NDVI data.\n",
    "    ndvi_test (np.ndarray): Test NDVI data.\n",
    "    sequence_length (int): Length of input sequences.\n",
    "    pred_length (int): Length of prediction sequences.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Prepared training and test data, input shape, and output units.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    ndvi_train_normalized = scaler.fit_transform(ndvi_train.reshape(-1, 1)).reshape(ndvi_train.shape)\n",
    "    ndvi_test_normalized = scaler.transform(ndvi_test.reshape(-1, 1)).reshape(ndvi_test.shape)\n",
    "\n",
    "    def create_sequences(data, seq_length, pred_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length - pred_length + 1):\n",
    "            X.append(data[i:i + seq_length])\n",
    "            y.append(data[i + seq_length:i + seq_length + pred_length])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X_train, Y_train = create_sequences(ndvi_train_normalized, sequence_length, pred_length)\n",
    "    X_test, Y_test = create_sequences(ndvi_test_normalized, sequence_length, pred_length)\n",
    "\n",
    "    X_train_flattened = X_train.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
    "    Y_train_flattened = Y_train.reshape(Y_train.shape[0], Y_train.shape[1], -1)\n",
    "    X_test_flattened = X_test.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
    "    Y_test_flattened = Y_test.reshape(Y_test.shape[0], Y_test.shape[1], -1)\n",
    "\n",
    "    X_train_flattened = np.nan_to_num(X_train_flattened, nan=0)\n",
    "    Y_train_flattened = np.nan_to_num(Y_train_flattened, nan=0)\n",
    "    X_test_flattened = np.nan_to_num(X_test_flattened, nan=0)\n",
    "    Y_test_flattened = np.nan_to_num(Y_test_flattened, nan=0)\n",
    "\n",
    "    input_shape = (X_train_flattened.shape[1], X_train_flattened.shape[2])\n",
    "    output_units = Y_train_flattened.shape[1] * Y_train_flattened.shape[2]\n",
    "\n",
    "    return X_train_flattened, Y_train_flattened, X_test_flattened, Y_test_flattened, input_shape, output_units\n",
    "\n",
    "def build_model(input_shape, output_units, params):\n",
    "    \"\"\"\n",
    "    Build and compile the LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of the input data.\n",
    "    output_units (int): Number of output units.\n",
    "    params (dict): Dictionary containing model parameters.\n",
    "\n",
    "    Returns:\n",
    "    Sequential: Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    lstm_units = params['lstm_units']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    learning_rate = params['learning_rate']\n",
    "    optimizer_name = params['optimizer']\n",
    "    use_batch_norm = params['use_batch_norm']\n",
    "\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Masking(mask_value=0))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True))\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_units))\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(params, input_shape, output_units, X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate the LSTM model based on provided parameters.\n",
    "\n",
    "    Parameters:\n",
    "    params (dict): Dictionary containing model parameters.\n",
    "    input_shape (tuple): Shape of the input data.\n",
    "    output_units (int): Number of output units.\n",
    "    X_train (np.ndarray): Training input data.\n",
    "    Y_train (np.ndarray): Training target data.\n",
    "    X_test (np.ndarray): Test input data.\n",
    "    Y_test (np.ndarray): Test target data.\n",
    "\n",
    "    Returns:\n",
    "    dict: Training result including parameters and loss.\n",
    "    \"\"\"\n",
    "    model = build_model(input_shape, output_units, params)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], -1), epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    loss = model.evaluate(X_test, Y_test.reshape(Y_test.shape[0], -1), verbose=0)\n",
    "    \n",
    "    result = {\n",
    "        'lstm_units': params['lstm_units'],\n",
    "        'dropout_rate': params['dropout_rate'],\n",
    "        'batch_size': params['batch_size'],\n",
    "        'epochs': params['epochs'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'optimizer': params['optimizer'],\n",
    "        'use_batch_norm': params['use_batch_norm'],\n",
    "        'loss': loss\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "# Paths to training and test data\n",
    "train_file = base_dir / 'data/data_interpolated/ds_B_Cube_665_train.nc'\n",
    "test_file = base_dir / 'data/data_test/Cube_665_test.nc'\n",
    "\n",
    "# Example sequence and prediction lengths\n",
    "sequence_length = 70\n",
    "pred_length = 23\n",
    "\n",
    "# Load and prepare data\n",
    "ndvi_train, ndvi_test = load_data(train_file, test_file)\n",
    "X_train, Y_train, X_test, Y_test, input_shape, output_units = prepare_data(ndvi_train, ndvi_test, sequence_length, pred_length)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'lstm_units': [50, 100],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [30, 50],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'use_batch_norm': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Create grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Loop through all parameter combinations\n",
    "for params in grid:\n",
    "    result = train_and_evaluate(params, input_shape, output_units, X_train, Y_train, X_test, Y_test)\n",
    "    results.append(result)\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Find the best parameter combination\n",
    "best_params = results_df.loc[results_df['loss'].idxmin()]\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce89a9-1774-4c11-b34c-7c0361107ac6",
   "metadata": {},
   "source": [
    "The result is a list with the best values for each parameter examined. These are used for the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786e3e1-e2e5-4718-b411-23378bb6e3ed",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "### Load and normalize the data \n",
    "\n",
    "First, all Files from the defined directory are loaded. \n",
    "\n",
    "Data needs to be normalized for an LSTM to ensure that the network converges faster during training. This avoids issues related to vanishing or exploding gradients. Normalization also helps to ensure that all input features contribute equally to the learning process to improve the model's performance and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3785f1-39e0-4760-aff5-d769f21f73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_665_train.nc with shape: (292, 128, 128)\n",
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1203_train.nc with shape: (292, 128, 128)\n",
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_80_train.nc with shape: (292, 128, 128)\n",
      "Loaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1301_train.nc with shape: (292, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1203_test.nc with shape: (93, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_80_test.nc with shape: (93, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1301_test.nc with shape: (93, 128, 128)\n",
      "Loaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_665_test.nc with shape: (93, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized train data shape: (292, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n",
      "Normalized test data shape: (93, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "def load_nc_file(file_path):\n",
    "    \"\"\"\n",
    "    Load a single .nc file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the .nc file.\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: Loaded dataset.\n",
    "    \"\"\"\n",
    "    return xr.open_dataset(file_path)\n",
    "\n",
    "def extract_ndvi(ds):\n",
    "    \"\"\"\n",
    "    Extract NDVI data from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    ds (xarray.Dataset): Dataset containing NDVI data.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Extracted NDVI values.\n",
    "    \"\"\"\n",
    "    return ds['NDVI'].values\n",
    "\n",
    "# Example: Path to the data\n",
    "train_dir = base_dir / 'data/data_interpolated/'\n",
    "test_dir = base_dir / 'data/data_test/'\n",
    "\n",
    "# List of training and test files\n",
    "train_files = glob.glob(str(train_dir / '*.nc'))\n",
    "test_files = glob.glob(str(test_dir / '*.nc'))\n",
    "\n",
    "# Initialize training and test data lists\n",
    "ndvi_train_list = []\n",
    "ndvi_test_list = []\n",
    "\n",
    "# Load training data\n",
    "for file in train_files:\n",
    "    ds = load_nc_file(file)\n",
    "    ndvi_train = extract_ndvi(ds)\n",
    "    ndvi_train_list.append(ndvi_train)\n",
    "    print(f\"Loaded train file: {file} with shape: {ndvi_train.shape}\")\n",
    "\n",
    "# Load test data\n",
    "for file in test_files:\n",
    "    ds = load_nc_file(file)\n",
    "    ndvi_test = extract_ndvi(ds)\n",
    "    ndvi_test_list.append(ndvi_test)\n",
    "    print(f\"Loaded test file: {file} with shape: {ndvi_test.shape}\")\n",
    "\n",
    "def normalize_data(ndvi_data):\n",
    "    \"\"\"\n",
    "    Normalize the NDVI data using MinMaxScaler.\n",
    "\n",
    "    Parameters:\n",
    "    ndvi_data (np.ndarray): NDVI data to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Normalized NDVI data.\n",
    "    MinMaxScaler: Scaler used for normalization.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    flattened_data = ndvi_data.reshape(-1, 1)\n",
    "    normalized_data = scaler.fit_transform(flattened_data).reshape(ndvi_data.shape)\n",
    "    return normalized_data, scaler\n",
    "\n",
    "# Normalize training and test data\n",
    "ndvi_train_normalized_list = []\n",
    "ndvi_test_normalized_list = []\n",
    "scalers = []  # List of scalers for each file\n",
    "\n",
    "for ndvi_train in ndvi_train_list:\n",
    "    normalized_data, scaler = normalize_data(ndvi_train)\n",
    "    ndvi_train_normalized_list.append(normalized_data)\n",
    "    scalers.append(scaler)\n",
    "    print(f\"Normalized train data shape: {normalized_data.shape}\")\n",
    "\n",
    "for ndvi_test in ndvi_test_list:\n",
    "    normalized_data, scaler = normalize_data(ndvi_test)\n",
    "    ndvi_test_normalized_list.append(normalized_data)\n",
    "    print(f\"Normalized test data shape: {normalized_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2e2ec-37b3-42c1-b412-43981a34b8af",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "The following is carried out here: \n",
    "- Creation of the sequences that the model needs for training: For seasonal data, the sequence length should typically capture at least one full seasonal cycle. This allows the model to learn the repeating patterns and trends effectively. Thus, we decided to set the sequence length to 70 (365 days divided by 5 days temporal resolution = 73). Out test dataset only contains 93 timesteps, because the data of the Minicubes did not have enough valid values in the first 1,5 years. As a result, we were only able to predict the next 23 (93 - 70 = 23) timesteps because our data set did not cover enough years. \n",
    "- Flattening the data and dividing it into sequences: Flattening and reshaping ensure the data is compatible with LSTM input requirements:\n",
    "    - The variable X should be a 3D array of shape (num_samples, time_steps, num_features).\n",
    "        - num_samples: Number of training samples.\n",
    "        - time_steps: Length of the sequence to be fed into the LSTM.\n",
    "        - num_features: Number of features at each time step.\n",
    "    - The variable Y should be a 2D array of shape (num_samples, num_targets).\n",
    "        - num_targets: Number of targets to predict\n",
    "- Combination of the data from all cubes\n",
    "- Masking nan values: LSTMs cannot handle NaN values because they disrupt the computations required for training. They can lead to invalid loss values and prevent the network from learning properly.\n",
    "- Reshape the combined data arrays to the required dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e77edd-74e7-4245-a182-12a2b1103156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in X_train_combined: False\n",
      "NaN in Y_train_combined: False\n",
      "NaN in X_test_combined: False\n",
      "NaN in Y_test_combined: False\n",
      "Final X_train_combined shape: (800, 70, 16384)\n",
      "Final Y_train_combined shape: (800, 376832)\n",
      "Final X_test_combined shape: (4, 70, 16384)\n",
      "Final Y_test_combined shape: (4, 376832)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 70\n",
    "pred_length = 23\n",
    "\n",
    "def create_sequences(data, seq_length, pred_length):\n",
    "    \"\"\"\n",
    "    Create sequences and targets for the model.\n",
    "\n",
    "    Parameters:\n",
    "    data (np.ndarray): Input data for sequence creation.\n",
    "    seq_length (int): Length of the input sequences.\n",
    "    pred_length (int): Length of the prediction sequences.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Array of input sequences.\n",
    "    np.ndarray: Array of target sequences.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - pred_length + 1):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length:i + seq_length + pred_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def flatten_data(data):\n",
    "    \"\"\"\n",
    "    Flattens the height and width dimensions into a single feature dimension.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): The data to be flattened.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The flattened data.\n",
    "    \"\"\"\n",
    "    return data.reshape(data.shape[0], data.shape[1], -1)\n",
    "\n",
    "def prepare_data(ndvi_data_list, sequence_length, pred_length):\n",
    "    \"\"\"\n",
    "    Prepares the data for model training or testing with sequences.\n",
    "    \n",
    "    Args:\n",
    "        ndvi_data_list (list): List of NDVI data arrays.\n",
    "        sequence_length (int): The length of the sequence.\n",
    "        pred_length (int): The length of the prediction.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Flattened input and target sequences.\n",
    "    \"\"\"\n",
    "    X_list, Y_list = [], []\n",
    "    for ndvi_data in ndvi_data_list:\n",
    "        X, Y = create_sequences(ndvi_data, sequence_length, pred_length)\n",
    "        X_list.append(flatten_data(X))\n",
    "        Y_list.append(flatten_data(Y))\n",
    "    return X_list, Y_list\n",
    "\n",
    "def combine_data(X_list, Y_list):\n",
    "    \"\"\"\n",
    "    Combines and flattens the list of data arrays into single arrays.\n",
    "    \n",
    "    Args:\n",
    "        X_list (list): List of input sequences.\n",
    "        Y_list (list): List of target sequences.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined and flattened input and target sequences.\n",
    "    \"\"\"\n",
    "    X_combined = np.concatenate(X_list, axis=0)\n",
    "    Y_combined = np.concatenate(Y_list, axis=0)\n",
    "    return X_combined, Y_combined\n",
    "\n",
    "def check_nan(data, name):\n",
    "    \"\"\"\n",
    "    Checks for NaN values in the data.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): The data to be checked.\n",
    "        name (str): The name of the data for printing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    nan_in_data = np.any(np.isnan(data))\n",
    "    print(f\"NaN in {name}: {nan_in_data}\")\n",
    "\n",
    "def reshape_combined_data(X, Y, sequence_length):\n",
    "    \"\"\"\n",
    "    Reshapes the combined data arrays to the required dimensions.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Combined input sequences.\n",
    "        Y (numpy.ndarray): Combined target sequences.\n",
    "        sequence_length (int): The length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Reshaped input and target sequences.\n",
    "    \"\"\"\n",
    "    X = X.reshape((X.shape[0], sequence_length, -1))\n",
    "    Y = Y.reshape((Y.shape[0], -1))\n",
    "    return X, Y\n",
    "\n",
    "# Prepare training data\n",
    "X_train_list, Y_train_list = prepare_data(ndvi_train_normalized_list, sequence_length, pred_length)\n",
    "\n",
    "# Prepare testing data\n",
    "X_test_list, Y_test_list = prepare_data(ndvi_test_normalized_list, sequence_length, pred_length)\n",
    "\n",
    "# Combine training data\n",
    "X_train_combined, Y_train_combined = combine_data(X_train_list, Y_train_list)\n",
    "\n",
    "# Combine testing data\n",
    "X_test_combined, Y_test_combined = combine_data(X_test_list, Y_test_list)\n",
    "\n",
    "# Replace NaNs with zeros\n",
    "X_train_combined = np.nan_to_num(X_train_combined, nan=0)\n",
    "Y_train_combined = np.nan_to_num(Y_train_combined, nan=0)\n",
    "X_test_combined = np.nan_to_num(X_test_combined, nan=0)\n",
    "Y_test_combined = np.nan_to_num(Y_test_combined, nan=0)\n",
    "\n",
    "# Check for NaNs in the data\n",
    "check_nan(X_train_combined, \"X_train_combined\")\n",
    "check_nan(Y_train_combined, \"Y_train_combined\")\n",
    "check_nan(X_test_combined, \"X_test_combined\")\n",
    "check_nan(Y_test_combined, \"Y_test_combined\")\n",
    "\n",
    "# Reshape combined data\n",
    "X_train_combined, Y_train_combined = reshape_combined_data(X_train_combined, Y_train_combined, sequence_length)\n",
    "X_test_combined, Y_test_combined = reshape_combined_data(X_test_combined, Y_test_combined, sequence_length)\n",
    "\n",
    "# Print final shapes of combined data\n",
    "print(f\"Final X_train_combined shape: {X_train_combined.shape}\")\n",
    "print(f\"Final Y_train_combined shape: {Y_train_combined.shape}\")\n",
    "print(f\"Final X_test_combined shape: {X_test_combined.shape}\")\n",
    "print(f\"Final Y_test_combined shape: {Y_test_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a902ce-7eb1-4bf8-96f5-67947624e47a",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "Here the model is created and compiled. \n",
    "For the model are used four cubes (see 1_Data_Preprocessing).\n",
    "After GridSearch, the best hyperparameters are set to the model.\n",
    "A callback for Early stopping is also added: Early stopping is a regularization technique used during training of neural networks to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20ffe378-da34-443e-84fa-0894a489a418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 15:33:01.781971: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 15:33:01.982418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:05:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 70, 16384)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 70, 50)            3287000   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 70, 50)            200       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 70, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 376832)            19218432  \n",
      "=================================================================\n",
      "Total params: 22,526,032\n",
      "Trainable params: 22,525,832\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lstm_units = 50\n",
    "dropout_rate = 0.2\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "optimizer = 'rmsprop'\n",
    "use_batch_norm = True\n",
    "\n",
    "# Define input and output shapes\n",
    "input_shape = (X_train_combined.shape[1], X_train_combined.shape[2])  # (n_steps, num_pixels * num_features)\n",
    "output_units = Y_train_combined.shape[1]  # num_pixels * num_features\n",
    "\n",
    "# Model creation\n",
    "def create_lstm_model(input_shape, output_units, lstm_units, dropout_rate, use_batch_norm, learning_rate):\n",
    "    \"\"\"\n",
    "    Creates and compiles an LSTM model with the specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data (n_steps, num_pixels * num_features).\n",
    "        output_units (int): Number of output units (num_pixels * num_features).\n",
    "        lstm_units (int): Number of LSTM units.\n",
    "        dropout_rate (float): Dropout rate for regularization.\n",
    "        use_batch_norm (bool): Whether to use batch normalization.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        model (Sequential): Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Masking(mask_value=0))\n",
    "    model.add(LSTM(lstm_units, return_sequences=True))\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    if use_batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_units))\n",
    "\n",
    "    # Create optimizer instance\n",
    "    optimizer_instance = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=optimizer_instance, loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the LSTM model\n",
    "model = create_lstm_model(input_shape, output_units, lstm_units, dropout_rate, use_batch_norm, learning_rate)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Callback for Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27979e-5122-478f-aef4-c9cb64090bc9",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "The code trains the LSTM model using the combined training data (X_train_combined and Y_train_combined) with early stopping to halt training if performance on the validation split (20% of the training data) stops improving. After training, it evaluates the model's performance and prints the test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc1767e-7682-4cf0-85f3-238fda65b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 15:33:18.306972: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 15:33:23.339418: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 10s 204ms/step - loss: 0.3286 - val_loss: 0.3497\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.2917 - val_loss: 0.3134\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.2443 - val_loss: 0.2575\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.1949 - val_loss: 0.2546\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.1454 - val_loss: 0.1960\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0958 - val_loss: 0.1884\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0634 - val_loss: 0.1391\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0467 - val_loss: 0.1035\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0398 - val_loss: 0.0580\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 0.0356 - val_loss: 0.0411\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0343 - val_loss: 0.0530\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0312 - val_loss: 0.0555\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0337 - val_loss: 0.0460\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0347 - val_loss: 0.0447\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 1s 67ms/step - loss: 0.0313 - val_loss: 0.0450\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0513\n",
      "Test loss: 0.051280610263347626\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_combined, Y_train_combined, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Evaluation\n",
    "loss = model.evaluate(X_test_combined, Y_test_combined, verbose=1)\n",
    "print(f\"Test loss: {loss}\")\n",
    "\n",
    "# Save the model\n",
    "# model.save('LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a87579-497b-4af5-ba83-814e0161c61a",
   "metadata": {},
   "source": [
    "### Prediction and Denormalization\n",
    "\n",
    "After the prediction is made, the results are denormalized using the inverse transformation of the scaler to revert them back to their original scale. This ensures that the test data and the model's predictions are in the same scale as the original dataset for accurate comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d9f0ff-a8f6-480c-84de-930b8958d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# model = load_model('LSTM.h5')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_combined)\n",
    "\n",
    "# Denormalize the predictions\n",
    "def denormalize_data(scaler, normalized_data):\n",
    "    return scaler.inverse_transform(normalized_data.reshape(-1, 1)).reshape(normalized_data.shape)\n",
    "\n",
    "# Ensuring that the test data and the predictions are transformed back to their original scale\n",
    "last_scaler = scalers[-1]\n",
    "Y_test_denormalized = denormalize_data(last_scaler, Y_test_combined)\n",
    "predictions_denormalized = denormalize_data(last_scaler, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bb642-8999-429e-a1b2-98476c17a02b",
   "metadata": {},
   "source": [
    "### Save predictions\n",
    "\n",
    "Here, the dimensions and time steps are configured. The mean squared error (MSE) between true and predicted values is calculated. Finally, the reshaped predicted values along with the MSE were saved into NetCDF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71ea1ad-f35d-4302-9605-8d776e4d0d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cube_1203_test.nc': 'data/test_predictions/LSTM_predicted_Cube_1203_test.nc', 'Cube_80_test.nc': 'data/test_predictions/LSTM_predicted_Cube_80_test.nc', 'Cube_1301_test.nc': 'data/test_predictions/LSTM_predicted_Cube_1301_test.nc', 'Cube_665_test.nc': 'data/test_predictions/LSTM_predicted_Cube_665_test.nc'}\n",
      "MSE values: [0.092216894, 0.20980994, 0.29330707, 0.13620757]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "time_steps = 23\n",
    "x_dim = 128\n",
    "y_dim = 128\n",
    "\n",
    "output_dir = 'data/test_predictions'\n",
    "\n",
    "# Generate time stamps\n",
    "time_stamps = pd.date_range(start='2021-07-03', periods=time_steps, freq='5D')\n",
    "\n",
    "# Mapping of file names to test samples\n",
    "file_to_sample = {}\n",
    "\n",
    "# Names of test files corresponding to the predictions\n",
    "test_file_names = [os.path.basename(f) for f in test_files]\n",
    "\n",
    "# Reshape combined data back to the original form\n",
    "Y_test_combined = Y_test_combined.reshape((Y_test_combined.shape[0], time_steps, x_dim, y_dim))\n",
    "\n",
    "# Calculate MSE values\n",
    "mse_list = []\n",
    "for i in range(len(test_files)):\n",
    "    true_values = Y_test_combined[i].reshape(-1)\n",
    "    predicted_values = predictions_denormalized[i].reshape(-1)\n",
    "    mse = mean_squared_error(true_values, predicted_values)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Extract test samples and map to file names\n",
    "for i, file_name in enumerate(test_file_names):\n",
    "    sample = predictions_denormalized[i].reshape(time_steps, x_dim, y_dim)\n",
    "    mse = mse_list[i]  # MSE value for the current file\n",
    "    \n",
    "    # Create output file path\n",
    "    output_file = os.path.join(output_dir, 'LSTM_predicted_' + file_name)\n",
    "    \n",
    "    with nc.Dataset(output_file, 'w', format='NETCDF4') as ds:\n",
    "        ds.createDimension('time', time_steps)\n",
    "        ds.createDimension('x', x_dim)\n",
    "        ds.createDimension('y', y_dim)\n",
    "        \n",
    "        # Save time coordinate as Datetime64\n",
    "        time = ds.createVariable('time', 'f8', ('time',))\n",
    "        time.units = 'days since 1970-01-01 00:00:00'\n",
    "        time.calendar = 'gregorian'\n",
    "        time[:] = nc.date2num(time_stamps.to_pydatetime(), units=time.units, calendar=time.calendar)\n",
    "        \n",
    "        # Save X and Y coordinates as integers\n",
    "        x = ds.createVariable('x', 'i4', ('x',))\n",
    "        y = ds.createVariable('y', 'i4', ('y',))\n",
    "        ndvi = ds.createVariable('NDVI', 'f4', ('time', 'x', 'y'))\n",
    "        mse_var = ds.createVariable('MSE', 'f4')  # Variable for the MSE value\n",
    "        \n",
    "        x[:] = np.arange(x_dim)\n",
    "        y[:] = np.arange(y_dim)\n",
    "        \n",
    "        ndvi[:, :, :] = sample\n",
    "        mse_var.assignValue(mse)  # Save MSE value\n",
    "        \n",
    "    # Save mapping\n",
    "    file_to_sample[file_name] = output_file\n",
    "\n",
    "# Check mappings and MSE values\n",
    "print(file_to_sample)\n",
    "print(\"MSE values:\", mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7addd385-d83c-47b9-996d-7509e8d24db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
