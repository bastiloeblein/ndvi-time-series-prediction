{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df521529-f663-4578-9a5f-1b73899cfa52",
   "metadata": {},
   "source": [
    "# SARIMA Model \n",
    "The Seasonal Autoregressive Integrated Moving Average (SARIMA) model is a robust and widely used tool in time series forecasting. SARIMA extends the ARIMA model by incorporating seasonal components, making it suitable for data with seasonal patterns. The model includes parameters for autoregression (p), differencing (d), and moving average (q), along with their seasonal counterparts (P, D, Q). These parameters are crucial for capturing both the trend and seasonal variations in the data, thus providing more accurate predictions for time series that exhibit seasonality [@Sim2022][@Azad2022]. The SARIMA modelâ€™s ability to handle non-stationary data through differencing and its inclusion of seasonal parameters make it highly effective for analyzing complex time series data.\n",
    "\n",
    "### SARIMA(p,d,q)(P,D,Q)(S) with: \n",
    "- p: number of lag observations included in the model\n",
    "- d: number of times the raw observations are differenced to make the time series stationary\n",
    "- q: number of lagged forecast errors in the prediction equation\n",
    "- P: number of lag observations in the seasonal part of the model\n",
    "- D: number of times the observations are differenced over the seasonal period to make the series stationary\n",
    "- Q: number of lagged forecast errors in the seasonal part of the model\n",
    "- S: length of the seasonal cycle \n",
    "\n",
    "## Description and Overview\n",
    "\n",
    "- Load Packages\n",
    "- Define base directory\n",
    "- Grid Search for Cube 655 (most complete cube)\n",
    "    - Stationarity Check: Augmented Dickey Fuller Test\n",
    "    - Differencing for non-stationary pixels\n",
    "    - Grid search for p,q,P and Q to define most common parameter combination\n",
    "    - Get most common parameter combination for cube 665\n",
    "- Fit SARIMA model and make predictions\n",
    "    - Stationarity Check: Augmented Dickey Fuller Test\n",
    "    - Differencing for non-stationary pixels\n",
    "    - Fit SARIMA model to each pixel in the four most complete cubes\n",
    "    - Calculate training and test mean squared error (mse) and best akaike information criterion (aic)\n",
    "    - Save fitted and predicted NDVI values to seperate .nc-files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4a6c5-252c-41cc-aff2-318a153ebc29",
   "metadata": {},
   "source": [
    "## Load packages\n",
    "First we import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bfb100e-cffa-4104-8858-1c8fe1008f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e5679-7238-4eb1-86c5-9f38cafdfa12",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458b4ab4-b3d2-45c3-9a99-56c475df59c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cgoehler/team-extra/ndvi-time-series-prediction\n"
     ]
    }
   ],
   "source": [
    "# Define base_dir for consistent path management\n",
    "notebook_dir = Path(os.getcwd()).resolve()\n",
    "base_dir = notebook_dir.parent\n",
    "print(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b06dca-4d5b-4219-afdd-83a7b8f19a4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grid Search for Cube 655 (most complete cube)\n",
    "Selecting the best parameter combinations for a SARIMA model involves a systematic approach. Initially, the data must be checked for stationarity using tests like the Augmented Dickey-Fuller (ADF) test. Once stationarity is confirmed, a grid search is applied in order to identify suitable values for p, q, P and Q parameters. Due to limited computing power we apply the grid search only for 30 random pixels in the most complete cube (Cube 665). \n",
    "\n",
    "The final model is selected based on the Akaike Information Criterion (AIC), which helps in identifying the model with the best fit by minimizing this value [@Sim2022]. The Training Mean Squared Error (MSE) is used to validate the effectiveness of the model, by minimizing it. This systematic approach ensures that the chosen SARIMA model effectively captures the underlying patterns in the NDVI time series data.\n",
    "\n",
    "Therefore we follow those steps: \n",
    "- Iterate over each of the 30 random pixel in the cube, ignoring pixels that only contain NaNs\n",
    "    - Apply an Augmented Dickey Fuller Test & Differencing for d (0 or 1), D = 1 in order to check for stationarity and apply differencing\n",
    "    - Grid Search for p, q, P & Q by applying different parameter combination to the SARIMA model\n",
    "- Get most common parameter combination for the 30 random pixel in cube 665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b897d0c-4a2b-469a-8c6e-634cafa78c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common parameter combination is: ((1, 0, 0), (0, 1, 0, 73)) with a count of 20\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Function to perform the Augmented Dickey-Fuller test\n",
    "def adf_test(time_series):\n",
    "    \"\"\"\n",
    "    Perform the Augmented Dickey-Fuller test.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (np.ndarray): The time series data to be tested.\n",
    "\n",
    "    Returns:\n",
    "    float: The ADF test statistic.\n",
    "    float: The p-value of the test.\n",
    "    \"\"\"\n",
    "    result = adfuller(time_series)\n",
    "    adf_statistic = result[0]\n",
    "    p_value = result[1]\n",
    "    return adf_statistic, p_value\n",
    "\n",
    "\n",
    "# Function to determine differencing orders\n",
    "def determine_differencing(time_series, seasonal_period):\n",
    "    \"\"\"\n",
    "    Determine the differencing orders for a given time series.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (xarray.DataArray): The time series data.\n",
    "    seasonal_period (int): The seasonal period for the time series.\n",
    "\n",
    "    Returns:\n",
    "    int: Order of non-seasonal differencing (d).\n",
    "    int: Order of seasonal differencing (D).\n",
    "    \"\"\"\n",
    "    time_series_values = time_series.values.flatten()\n",
    "    if len(time_series_values) < 2:\n",
    "        raise ValueError(\"Time series must have more than one data point.\")\n",
    "    adf_statistic, p_value = adf_test(time_series_values)\n",
    "    d = 1 if p_value >= 0.05 else 0\n",
    "    D = 1\n",
    "    return d, D\n",
    "\n",
    "\n",
    "# Function to fit SARIMAX model and return fitted model, training MSE, and AIC\n",
    "def fit_sarimax_model(time_series, order, seasonal_order):\n",
    "    \"\"\"\n",
    "    Fit a SARIMAX model and return the fitted model, training MSE, and AIC.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (np.ndarray): The time series data to fit.\n",
    "    order (tuple): The order of the SARIMAX model (p, d, q).\n",
    "    seasonal_order (tuple): The seasonal order of the SARIMAX model (P, D, Q, s).\n",
    "\n",
    "    Returns:\n",
    "    SARIMAXResults: The fitted SARIMAX model.\n",
    "    float: The mean squared error of the training data.\n",
    "    float: The AIC of the fitted model.\n",
    "    \"\"\"\n",
    "    model = SARIMAX(\n",
    "        time_series,\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "    )\n",
    "    results = model.fit(disp=False)\n",
    "    fitted_values = results.fittedvalues\n",
    "    training_mse = mean_squared_error(time_series, fitted_values)\n",
    "    aic = results.aic\n",
    "    return results, training_mse, aic\n",
    "\n",
    "\n",
    "# Function to perform grid search\n",
    "def grid_search_sarimax(\n",
    "    time_series, p_range, d_range, q_range, P_range, D_range, Q_range, S_range\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a grid search to find the best SARIMAX model parameters.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (np.ndarray): The time series data to fit.\n",
    "    p_range (range): Range of p values to consider.\n",
    "    d_range (range): Range of d values to consider.\n",
    "    q_range (range): Range of q values to consider.\n",
    "    P_range (range): Range of P values to consider.\n",
    "    D_range (range): Range of D values to consider.\n",
    "    Q_range (range): Range of Q values to consider.\n",
    "    S_range (list): List of seasonal periods to consider.\n",
    "\n",
    "    Returns:\n",
    "    SARIMAXResults: The best fitted SARIMAX model.\n",
    "    tuple: The best parameter combination.\n",
    "    float: The mean squared error of the best model.\n",
    "    float: The AIC of the best model.\n",
    "    \"\"\"\n",
    "    best_aic = float(\"inf\")\n",
    "    best_mse = float(\"inf\")\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    for p, d, q, P, D, Q, S in itertools.product(\n",
    "        p_range, d_range, q_range, P_range, D_range, Q_range, S_range\n",
    "    ):\n",
    "        try:\n",
    "            order = (p, d, q)\n",
    "            seasonal_order = (P, D, Q, S)\n",
    "            model, training_mse, aic = fit_sarimax_model(\n",
    "                time_series, order, seasonal_order\n",
    "            )\n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_mse = training_mse\n",
    "                best_params = (order, seasonal_order)\n",
    "                best_model = model\n",
    "        except Exception as e:\n",
    "            print(f\"Error with parameters {order} and {seasonal_order}: {e}\")\n",
    "            continue\n",
    "    return best_model, best_params, best_mse, best_aic\n",
    "\n",
    "\n",
    "# Define the seasonal period\n",
    "seasonal_period = 73\n",
    "\n",
    "# Define parameter ranges for grid search\n",
    "p_range = range(0, 2)\n",
    "q_range = range(0, 2)\n",
    "P_range = range(0, 2)\n",
    "Q_range = range(0, 2)\n",
    "S_range = [seasonal_period]\n",
    "\n",
    "# Generate 30 random pixel coordinates\n",
    "grid_size = 128\n",
    "random_pixel_pairs = [\n",
    "    (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n",
    "    for _ in range(30)\n",
    "]\n",
    "\n",
    "# Specific file for Cube_665\n",
    "data_folder_train = base_dir / \"data\" / \"data_train\"\n",
    "data_folder_test = base_dir / \"data\" / \"data_test\"\n",
    "nc_file_train = \"ds_B_Cube_665_train.nc\"\n",
    "nc_file_test = \"Cube_665_test.nc\"\n",
    "\n",
    "# Open the specific dataset\n",
    "ds_train = xr.open_dataset(os.path.join(data_folder_train, nc_file_train))\n",
    "ndvi_data_train = ds_train[\"NDVI\"]\n",
    "\n",
    "ds_test = xr.open_dataset(os.path.join(data_folder_test, nc_file_test))\n",
    "ndvi_data_test = ds_test[\"NDVI\"]\n",
    "\n",
    "# Initialize arrays to store predictions and MSEs\n",
    "predictions = np.full((30, 93), np.nan)\n",
    "train_mse_array = np.full(30, np.nan)\n",
    "test_mse_array = np.full(30, np.nan)\n",
    "\n",
    "# List to store parameter combinations\n",
    "parameter_combinations = []\n",
    "\n",
    "# Iterate over the selected pixels\n",
    "for idx, (x, y) in enumerate(random_pixel_pairs):\n",
    "    try:\n",
    "        ndvi_pixel_train = ndvi_data_train.isel(x=x, y=y)\n",
    "        if np.all(np.isnan(ndvi_pixel_train)):\n",
    "            continue  # Skip pixels with only NaNs\n",
    "\n",
    "        d, D = determine_differencing(ndvi_pixel_train, seasonal_period)\n",
    "        d_range = [d]\n",
    "        D_range = [D]\n",
    "        best_model, best_params, best_mse, best_aic = grid_search_sarimax(\n",
    "            ndvi_pixel_train.values.flatten(),\n",
    "            p_range,\n",
    "            d_range,\n",
    "            q_range,\n",
    "            P_range,\n",
    "            D_range,\n",
    "            Q_range,\n",
    "            S_range,\n",
    "        )\n",
    "\n",
    "        # Append the best parameter combination\n",
    "        parameter_combinations.append(best_params)\n",
    "\n",
    "        # Forecast\n",
    "        forecast_steps = 93  # Fixed forecast steps to 93\n",
    "        forecast = best_model.get_forecast(steps=forecast_steps).predicted_mean\n",
    "\n",
    "        # Store predictions\n",
    "        predictions[idx, :] = forecast\n",
    "\n",
    "        # Calculate training MSE\n",
    "        train_mse_array[idx] = best_mse\n",
    "\n",
    "        # Test data for the same pixel\n",
    "        ndvi_pixel_test = ndvi_data_test.isel(x=x, y=y)\n",
    "        test_time_index = pd.date_range(\n",
    "            start=\"2021-07-03\", periods=len(ndvi_pixel_test.time), freq=\"5D\"\n",
    "        )\n",
    "        test_series = pd.Series(ndvi_pixel_test.values.flatten(), index=test_time_index)\n",
    "\n",
    "        # Create a time index for the forecast data\n",
    "        original_time_index = pd.date_range(\n",
    "            start=\"2017-07-04\", periods=len(ndvi_pixel_train.time), freq=\"5D\"\n",
    "        )\n",
    "        forecast_time_index = pd.date_range(\n",
    "            start=original_time_index[-1] + pd.DateOffset(days=5),\n",
    "            periods=forecast_steps,\n",
    "            freq=\"5D\",\n",
    "        )\n",
    "        forecast_series = pd.Series(forecast, index=forecast_time_index)\n",
    "\n",
    "        # Ensure the forecast and test data have the same time dimension\n",
    "        common_indices = forecast_series.index.intersection(test_series.index)\n",
    "\n",
    "        # Subset forecast and test series to only include the common indices\n",
    "        aligned_forecast_series = forecast_series.loc[common_indices]\n",
    "        aligned_test_series = test_series.loc[common_indices]\n",
    "\n",
    "        # Drop NaNs from both series\n",
    "        aligned_forecast_series = aligned_forecast_series.dropna()\n",
    "        aligned_test_series = aligned_test_series.dropna()\n",
    "\n",
    "        # Align the series again to ensure matching lengths after dropping NaNs\n",
    "        final_common_indices = aligned_forecast_series.index.intersection(\n",
    "            aligned_test_series.index\n",
    "        )\n",
    "        aligned_forecast_series = aligned_forecast_series.loc[final_common_indices]\n",
    "        aligned_test_series = aligned_test_series.loc[final_common_indices]\n",
    "\n",
    "        # Ensure lengths match after alignment\n",
    "        if len(aligned_forecast_series) != len(aligned_test_series):\n",
    "            raise ValueError(\n",
    "                \"Aligned forecast and test series have different lengths after dropping NaNs\"\n",
    "            )\n",
    "\n",
    "        # Calculate test MSE\n",
    "        test_mse = mean_squared_error(aligned_test_series, aligned_forecast_series)\n",
    "        test_mse_array[idx] = test_mse\n",
    "\n",
    "        # print(f\"Pixel ({x}, {y}): Best Params: {best_params}, Best Train MSE: {best_mse}, Best AIC: {best_aic}, Test MSE: {test_mse}\")\n",
    "    except Exception as e:\n",
    "        # print(f\"Failed to process pixel ({x}, {y}): {e}\")\n",
    "        continue\n",
    "\n",
    "# Find the most common parameter combination\n",
    "most_common_params = Counter(parameter_combinations).most_common(1)[0]\n",
    "print(\n",
    "    f\"The most common parameter combination is: {most_common_params[0]} with a count of {most_common_params[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dda163-5cbe-45d2-a014-5ede86859b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fit SARIMA model and make predictions\n",
    "We fit the SARIMA model to the NDVI time series of each pixel of the four most complete cubes. Predictions of the NDVI are made for the next 93 time steps (2021-07-03 to 2022-10-06). The model's performance is evaluated based on training and test mean squared error (MSEs). We iterate over all pixel in the selected four most complete cubes (I: Cube 665, II: Cube 80, III: Cube 1203, IV: Cube 1301).\n",
    "\n",
    "The analysis includes the following steps:\n",
    "- Augmented Dickey Fuller (ADF) Test is carried out to check each pixel for stationarity. The function returns the ADF test statistics and the p-value. \n",
    "- After that the differencing order is determined, which is required to make the time series stationary based on the ADF test restults. \n",
    "    - The non-seasonal differencing parameter d is set to 1 (for non-stationary pixel) or 0 (for stationary pixel)\n",
    "    - The seasonal differencing parameter D is set to 1 for all pixel. Research has shown that the value of D is typically determined by the seasonality present in the data and can be set to 1 in order to effectively capture the seasonal patterns in the NDVI series [@Silva2018].    This aligns with the general approach in time series analysis, where (D = 1) is often sufficient to remove seasonality, especially when the data exhibits clear and repetitive seasonal cycles.\n",
    "- Fit the SARIMA Model with ```SARIMAX((p,d,q)(P,D,Q)(S))```\n",
    "    - The ```SARIMAX()``` function of the ```statsmodels``` package fits a pixelwise SARIMA model to the NDVI time series of each pixel and returns the fitted model, training MSE and AIC\n",
    "    - We predict the next 93 time steps (from 2021-07-03 to 2022-10-06) of the NDVI time series for each pixel in each of the cubes\n",
    "    - Parameter combination of p, q, P and Q are predefined: ```p, q, P, Q = 1, 0, 0, 0``` as this was the most common parameter combination in the grid search for the 30 random pixel in the most complete cube 665\n",
    "        - Explanation of the different parameters:\n",
    "            - p=1: the model uses value of the previous time step to predict the current value\n",
    "            - d=0: no differencing is applied (the time series is stationary), **d=1** differencing is applied (the time series is non-stationary)\n",
    "            - q=0: the model uses no previous forecast error to predict the current value\n",
    "            - P=0: no seasonal autoregressive terms are used\n",
    "            - D=1: the model includes one seasonal differencing component\n",
    "            - Q=0: no seasonal moving average terms are used\n",
    "            - S=73: in our data with a frequence of 5 days we have 73 time steps a year \n",
    "- The training MSE (between training and fitted data), the best training AIC and the test MSE (between predicted and test data) are calculated\n",
    "- The fitted values together with the training MSE and best training AIC are saved in a separate netcdf file (```'/data/data_fitted/SARIMA_fitted_Cube_XY.nc'```) per cube\n",
    "- The predicted values together with the test MSE are saved in a separate netcdf file (```'/data/data_predictions/SARIMA_predicted_Cube_XY.nc'```) per cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4215062-7507-4a0f-999a-2a213c358fd9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ds_B_Cube_665_train.nc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m order \u001b[38;5;241m=\u001b[39m (p, d, q)\n\u001b[1;32m    134\u001b[0m seasonal_order \u001b[38;5;241m=\u001b[39m (P, D, Q, seasonal_period)\n\u001b[0;32m--> 136\u001b[0m model, training_mse, aic \u001b[38;5;241m=\u001b[39m \u001b[43mfit_sarimax_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mndvi_pixel_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal_order\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Store d\u001b[39;00m\n\u001b[1;32m    141\u001b[0m d_array[x, y] \u001b[38;5;241m=\u001b[39m d\n",
      "Cell \u001b[0;32mIn[17], line 67\u001b[0m, in \u001b[0;36mfit_sarimax_model\u001b[0;34m(time_series, order, seasonal_order)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mFit a SARIMAX model and return the fitted model, training MSE, and AIC.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mfloat: The AIC of the fitted model.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m model \u001b[38;5;241m=\u001b[39m SARIMAX(\n\u001b[1;32m     61\u001b[0m     time_series,\n\u001b[1;32m     62\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     enforce_invertibility\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m fitted_values \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mfittedvalues\n\u001b[1;32m     69\u001b[0m training_mse \u001b[38;5;241m=\u001b[39m mean_squared_error(time_series, fitted_values)\n",
      "File \u001b[0;32m~/.conda/envs/scwenv/lib/python3.12/site-packages/statsmodels/tsa/statespace/mlemodel.py:727\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[0;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth\n\u001b[0;32m--> 727\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlefit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincludes_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m           \u001b[49m\u001b[43mcov_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov_kwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m res\u001b[38;5;241m.\u001b[39mmlefit \u001b[38;5;241m=\u001b[39m mlefit\n\u001b[1;32m    731\u001b[0m res\u001b[38;5;241m.\u001b[39mmle_retvals \u001b[38;5;241m=\u001b[39m mlefit\u001b[38;5;241m.\u001b[39mmle_retvals\n",
      "File \u001b[0;32m~/.conda/envs/scwenv/lib/python3.12/site-packages/statsmodels/tsa/statespace/mlemodel.py:885\u001b[0m, in \u001b[0;36mMLEModel.smooth\u001b[0;34m(self, params, transformed, includes_fixed, complex_step, cov_type, cov_kwds, return_ssm, results_class, results_wrapper_class, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Get the state space output\u001b[39;00m\n\u001b[0;32m--> 885\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmooth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Wrap in a results object\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_results(params, result, return_ssm, cov_type,\n\u001b[1;32m    889\u001b[0m                           cov_kwds, results_class,\n\u001b[1;32m    890\u001b[0m                           results_wrapper_class)\n",
      "File \u001b[0;32m~/.conda/envs/scwenv/lib/python3.12/site-packages/statsmodels/tsa/statespace/kalman_smoother.py:403\u001b[0m, in \u001b[0;36mKalmanSmoother.smooth\u001b[0;34m(self, smoother_output, smooth_method, results, run_filter, prefix, complex_step, update_representation, update_filter, update_smoother, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03mApply the Kalman smoother to the statespace model.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mSmootherResults object\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Create the results object\u001b[39;00m\n\u001b[1;32m    406\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_class(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/scwenv/lib/python3.12/site-packages/statsmodels/tsa/statespace/kalman_filter.py:924\u001b[0m, in \u001b[0;36mKalmanFilter._filter\u001b[0;34m(self, filter_method, inversion_method, stability_method, conserve_memory, filter_timing, tolerance, loglikelihood_burn, complex_step)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_state(prefix\u001b[38;5;241m=\u001b[39mprefix, complex_step\u001b[38;5;241m=\u001b[39mcomplex_step)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m \u001b[43mkfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kfilter\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# definition of functions for Augmented Dickey Fuller Test, Differencing, Fitting SARIMAX model\n",
    "def adf_test(time_series):\n",
    "    \"\"\"\n",
    "    Perform the Augmented Dickey-Fuller test.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (np.ndarray): The time series data to be tested.\n",
    "\n",
    "    Returns:\n",
    "    float: The ADF test statistic.\n",
    "    float: The p-value of the test.\n",
    "    \"\"\"\n",
    "    result = adfuller(time_series)\n",
    "    adf_statistic = result[0]\n",
    "    p_value = result[1]\n",
    "    return adf_statistic, p_value\n",
    "\n",
    "\n",
    "def determine_differencing(time_series, seasonal_period):\n",
    "    \"\"\"\n",
    "    Determine the differencing orders for a given time series.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (xarray.DataArray): The time series data.\n",
    "    seasonal_period (int): The seasonal period for the time series.\n",
    "\n",
    "    Returns:\n",
    "    int: Order of non-seasonal differencing (d).\n",
    "    int: Order of seasonal differencing (D).\n",
    "    \"\"\"\n",
    "    time_series_values = time_series.values.flatten()\n",
    "    if len(time_series_values) < 2:\n",
    "        raise ValueError(\"Time series must have more than one data point.\")\n",
    "    adf_statistic, p_value = adf_test(time_series_values)\n",
    "    if p_value >= 0.05:\n",
    "        d = 1\n",
    "    else:\n",
    "        d = 0\n",
    "    D = 1\n",
    "    return d, D\n",
    "\n",
    "\n",
    "def fit_sarimax_model(time_series, order, seasonal_order):\n",
    "    \"\"\"\n",
    "    Fit a SARIMAX model and return the fitted model, training MSE, and AIC.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (np.ndarray): The time series data to fit.\n",
    "    order (tuple): The order of the SARIMAX model (p, d, q).\n",
    "    seasonal_order (tuple): The seasonal order of the SARIMAX model (P, D, Q, s).\n",
    "\n",
    "    Returns:\n",
    "    SARIMAXResults: The fitted SARIMAX model.\n",
    "    float: The mean squared error of the training data.\n",
    "    float: The AIC of the fitted model.\n",
    "    \"\"\"\n",
    "    model = SARIMAX(\n",
    "        time_series,\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "    )\n",
    "    results = model.fit(disp=False)\n",
    "    fitted_values = results.fittedvalues\n",
    "    training_mse = mean_squared_error(time_series, fitted_values)\n",
    "    aic = results.aic\n",
    "    return results, training_mse, aic\n",
    "\n",
    "\n",
    "# Define the seasonal period\n",
    "seasonal_period = 73\n",
    "\n",
    "# Define fixed SARIMAX parameters\n",
    "p, q, P, Q = 1, 0, 0, 0\n",
    "\n",
    "# Folder with 100 NetCDF datasets\n",
    "data_folder_train = base_dir / \"data\" / \"data_train\"\n",
    "data_folder_test = base_dir / \"data\" / \"data_test\"\n",
    "nc_files_train = [f for f in os.listdir(data_folder_train) if f.endswith(\".nc\")]\n",
    "nc_files_test = [f for f in os.listdir(data_folder_test) if f.endswith(\".nc\")]\n",
    "\n",
    "# Ensure that the test files correspond to the training files\n",
    "if len(nc_files_train) != len(nc_files_test):\n",
    "    raise ValueError(\n",
    "        \"The number of training files does not match the number of test files\"\n",
    "    )\n",
    "\n",
    "# Iterate over all NetCDF files\n",
    "for nc_file_train, nc_file_test in zip(nc_files_train, nc_files_test):\n",
    "    print(f\"Processing file {nc_file_train}\")\n",
    "    ds_train = xr.open_dataset(os.path.join(data_folder_train, nc_file_train))\n",
    "    ndvi_data_train = ds_train[\"NDVI\"]\n",
    "\n",
    "    ds_test = xr.open_dataset(os.path.join(data_folder_test, nc_file_test))\n",
    "    ndvi_data_test = ds_test[\"NDVI\"]\n",
    "\n",
    "    # Initialize arrays to store predictions, fitted values, MSEs, and AICs\n",
    "    predictions = np.full(\n",
    "        (ndvi_data_test.sizes[\"x\"], ndvi_data_test.sizes[\"y\"], 93), np.nan\n",
    "    )\n",
    "    fitted_values_array = np.full(\n",
    "        (\n",
    "            ndvi_data_train.sizes[\"x\"],\n",
    "            ndvi_data_train.sizes[\"y\"],\n",
    "            len(ndvi_data_train.time),\n",
    "        ),\n",
    "        np.nan,\n",
    "    )\n",
    "    train_mse_array = np.full(\n",
    "        (ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan\n",
    "    )\n",
    "    train_aic_array = np.full(\n",
    "        (ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan\n",
    "    )\n",
    "    test_mse_array = np.full(\n",
    "        (ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan\n",
    "    )\n",
    "    d_array = np.full((ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan)\n",
    "\n",
    "    # Iterate over all pixels in the dataset\n",
    "    for x in range(ndvi_data_train.sizes[\"x\"]):\n",
    "        for y in range(ndvi_data_train.sizes[\"y\"]):\n",
    "            try:\n",
    "                ndvi_pixel_train = ndvi_data_train.isel(x=x, y=y)\n",
    "                if np.all(np.isnan(ndvi_pixel_train)):\n",
    "                    continue  # Skip pixels with only NaNs\n",
    "\n",
    "                d, D = determine_differencing(ndvi_pixel_train, seasonal_period)\n",
    "                order = (p, d, q)\n",
    "                seasonal_order = (P, D, Q, seasonal_period)\n",
    "\n",
    "                model, training_mse, aic = fit_sarimax_model(\n",
    "                    ndvi_pixel_train.values.flatten(), order, seasonal_order\n",
    "                )\n",
    "\n",
    "                # Store d\n",
    "                d_array[x, y] = d\n",
    "\n",
    "                # Store fitted values\n",
    "                fitted_values_array[x, y, :] = model.fittedvalues\n",
    "\n",
    "                # Store training MSE and AIC\n",
    "                train_mse_array[x, y] = training_mse\n",
    "                train_aic_array[x, y] = aic\n",
    "\n",
    "                # Forecast\n",
    "                forecast_steps = 93  # Fixed forecast steps to 93\n",
    "                forecast = model.get_forecast(steps=forecast_steps).predicted_mean\n",
    "\n",
    "                # Store predictions\n",
    "                predictions[x, y, :] = forecast\n",
    "\n",
    "                # Test data for the same pixel\n",
    "                ndvi_pixel_test = ndvi_data_test.isel(x=x, y=y)\n",
    "                test_time_index = pd.date_range(\n",
    "                    start=\"2021-07-03\", periods=len(ndvi_pixel_test.time), freq=\"5D\"\n",
    "                )\n",
    "                test_series = pd.Series(\n",
    "                    ndvi_pixel_test.values.flatten(), index=test_time_index\n",
    "                )\n",
    "\n",
    "                # Create a time index for the forecast data\n",
    "                original_time_index = pd.date_range(\n",
    "                    start=\"2017-07-04\", periods=len(ndvi_pixel_train.time), freq=\"5D\"\n",
    "                )\n",
    "                forecast_time_index = pd.date_range(\n",
    "                    start=original_time_index[-1] + pd.DateOffset(days=5),\n",
    "                    periods=forecast_steps,\n",
    "                    freq=\"5D\",\n",
    "                )\n",
    "                forecast_series = pd.Series(forecast, index=forecast_time_index)\n",
    "\n",
    "                # Ensure the forecast and test data have the same time dimension\n",
    "                common_indices = forecast_series.index.intersection(test_series.index)\n",
    "\n",
    "                # Subset forecast and test series to only include the common indices\n",
    "                aligned_forecast_series = forecast_series.loc[common_indices]\n",
    "                aligned_test_series = test_series.loc[common_indices]\n",
    "\n",
    "                # Drop NaNs from both series\n",
    "                aligned_forecast_series = aligned_forecast_series.dropna()\n",
    "                aligned_test_series = aligned_test_series.dropna()\n",
    "\n",
    "                # Align the series again to ensure matching lengths after dropping NaNs\n",
    "                final_common_indices = aligned_forecast_series.index.intersection(\n",
    "                    aligned_test_series.index\n",
    "                )\n",
    "                aligned_forecast_series = aligned_forecast_series.loc[\n",
    "                    final_common_indices\n",
    "                ]\n",
    "                aligned_test_series = aligned_test_series.loc[final_common_indices]\n",
    "\n",
    "                # Ensure lengths match after alignment\n",
    "                if len(aligned_forecast_series) != len(aligned_test_series):\n",
    "                    raise ValueError(\n",
    "                        \"Aligned forecast and test series have different lengths after dropping NaNs\"\n",
    "                    )\n",
    "\n",
    "                # Calculate test MSE\n",
    "                test_mse = mean_squared_error(\n",
    "                    aligned_test_series, aligned_forecast_series\n",
    "                )\n",
    "                test_mse_array[x, y] = test_mse\n",
    "\n",
    "                # print(f\"  Pixel ({x}, {y}): d={d}, D={D}, Training MSE: {training_mse}, AIC: {aic}, Test MSE: {test_mse}\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"  Failed to process pixel ({x}, {y}) in file {nc_file_train}: {e}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    fitted_output_dir = base_dir / \"data\" / \"data_fitted\"\n",
    "    predicted_output_dir = base_dir / \"data\" / \"data_predictions\" \n",
    "    os.makedirs(fitted_output_dir, exist_ok=True)\n",
    "    os.makedirs(predicted_output_dir, exist_ok=True)\n",
    "\n",
    "    # Save fitted values and training MSEs/AICs to a new NetCDF file\n",
    "    fitted_output_ds = xr.Dataset(\n",
    "        {\n",
    "            \"fitted_values\": ([\"x\", \"y\", \"time\"], fitted_values_array),\n",
    "            \"train_mse\": ([\"x\", \"y\"], train_mse_array),\n",
    "            \"train_aic\": ([\"x\", \"y\"], train_aic_array),\n",
    "            \"d\": ([\"x\", \"y\"], d_array),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": ndvi_data_train.coords[\"time\"],\n",
    "            \"x\": ndvi_data_train.coords[\"x\"],\n",
    "            \"y\": ndvi_data_train.coords[\"y\"],\n",
    "        },\n",
    "    )\n",
    "    fitted_output_file = os.path.join(\n",
    "        fitted_output_dir, f'SARIMA_fitted_{nc_file_train.split(\".\")[0]}.nc'\n",
    "    )\n",
    "    fitted_output_ds.to_netcdf(fitted_output_file)\n",
    "    print(f\"Fitted values saved to '{fitted_output_file}'\")\n",
    "\n",
    "    # Save predictions and test MSEs to a new NetCDF file\n",
    "    prediction_output_ds = xr.Dataset(\n",
    "        {\n",
    "            \"predictions\": ([\"x\", \"y\", \"time\"], predictions),\n",
    "            \"test_mse\": ([\"x\", \"y\"], test_mse_array),\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": pd.date_range(start=\"2021-07-03\", periods=93, freq=\"5D\"),\n",
    "            \"x\": ndvi_data_train.coords[\"x\"],\n",
    "            \"y\": ndvi_data_train.coords[\"y\"],\n",
    "        },\n",
    "    )\n",
    "    prediction_output_file = os.path.join(\n",
    "        predicted_output_dir, f'SARIMA_predicted_{nc_file_train.split(\".\")[0]}.nc'\n",
    "    )\n",
    "    prediction_output_ds.to_netcdf(prediction_output_file)\n",
    "    print(f\"Predictions saved to '{prediction_output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scwenv",
   "language": "python",
   "name": "scwenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
