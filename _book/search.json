[
    {
        "objectID": "index.html",
        "href": "index.html",
        "title": "Time series prediction of NDVI based on Sentinel-2 data of mini cubes: A comparison of SARIMA, Random Forest and LSTM",
        "section": "",
        "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
    },
    {
        "objectID": "Notebooks/0_Introduction.html#introduction-and-motivation",
        "href": "Notebooks/0_Introduction.html#introduction-and-motivation",
        "title": "1\u00a0 Welcome!",
        "section": "1.1 Introduction and Motivation",
        "text": "1.1 Introduction and Motivation\nVegetation plays a pivotal role in the Earth\u2019s ecosystem. Healthy vegetation is a fundamental part of a equable ecosystem that is characterized by a high biodiversity and different ecosystem services (Cardinale et al. 2012).\nUnderstanding and predicting the changes in vegetation are therefore crucial for addressing climate change, managing natural resources, and ensuring sustainable agricultural practices. event. ref?\nThe Normalized Difference Vegetation Index (NDVI) as a key vegetation index is used to monitor the health and vitality of vegetation. It can also provide information on biomass production in the context of the carbon cycle. Various vegetation properties can be estimated such as biomass, chlorophyll concentration in leaves, plant productivity, fractional vegetation cover, and plant stress (Huang et al. 2020). To detect shifting trends of growing seasons or in vegetation activity, long-term analyzes of NDVI data are useful [Higgins, Conradi, and Muhoko (2023)}.\nThese trends and changes can be analyzed with remote sensing time series data (Wang et al. 2016). As satellite data represents spectral reflectance patterns of vegetation, it can be transferred in various vegetation indices Wang et al. (2016). Those vegetation indices help to estimate seasonal dynamics in the vegetation phenology and thereby give us information about the health conditions of vegetation (Wang et al. 2016).\nSeveral models have been implemented for NDVI prediction, including statistical models, machine learning models, and deep learning models. In this project, our aim is to compare three different models for calculating ndvi predictions from time series data. We decided to use the Seasonal Autoregressive Integrated Moving Average model (SARIMA), the Random Forest model and a Long Short-Term Memory Network. Thus, our research question will be \u201cHow do SARIMA, Random Forest, and LSTM models compare in predicting the Normalized Difference Vegetation Index (NDVI) using Sentinel-2 mini cubes data?\u201d."
    },
    {
        "objectID": "Notebooks/0_Introduction.html#methods",
        "href": "Notebooks/0_Introduction.html#methods",
        "title": "1\u00a0 Welcome!",
        "section": "1.2 Methods",
        "text": "1.2 Methods\nSARIMA is a linear model with memory. It is widely used due to their simplicity and interpretability, because it captures temporal dependencies linearly (Shumway et al. 2017). Random Forest is a nonlinear model without memory. These models have shown improved performance by handling non-linearities better and being more robust against overfitting (Breiman 2001). LSTM as a nonlinear model with memory, can learn temporal patterns and nonlinear relationships. These deep learning models have demonstrated superior performance in NDVI prediction due to their ability to model intricate temporal dependencies (Cavalli et al. 2021)."
    },
    {
        "objectID": "Notebooks/0_Introduction.html#data",
        "href": "Notebooks/0_Introduction.html#data",
        "title": "1\u00a0 Welcome!",
        "section": "1.3 Data",
        "text": "1.3 Data\nIn Earth System Science, Earth System Data Cubes are becoming more important as a data source. Their interdisciplinary nature support data-intensive analyzes on a global scale (Mahecha et al. 2020). As a special type, minicubes have a higher spatiotemporal resolution compared to global data cubes. Thus, their spatial resolution is smaller (Montero et al. 2023). For analyzes of small areas, minicubes may be preferable. They offer enhanced detail accuracy and resolution, ranging from sub-meters to meters. This makes it possible to monitor local vegetation changes. Depending on the project, working with minicubes enable to overcome storage and memory limitations. They can be processed into smaller and manageable subsets, allowing more effective sampling strategies (Montero Loaiza et al. 2023).\nDetailed information about the Minicubes will be shown in the Chapter 1_Data_Preprocessing.\n\n\n\n\nBreiman, Leo. 2001. \u201cRandom Forests.\u201d Machine Learning 45: 5\u201332.\n\n\nCardinale, Bradley J., J. Emmett Duffy, Andrew Gonzalez, David U. Hooper, Charles Perrings, Patrick Venail, Anita Narwani, et al. 2012. \u201cBiodiversity Loss and Its Impact on Humanity.\u201d Nature 486 (June): 59\u201367. https://doi.org/10.1038/nature11148.\n\n\nCavalli, Stefano, Gabriele Penzotti, Michele Amoretti, Stefano Caselli, et al. 2021. \u201cA Machine Learning Approach for NDVI Forecasting Based on Sentinel-2 Data.\u201d In ICSOFT, 473\u201380.\n\n\nHiggins, Steven I., Timo Conradi, and Edward Muhoko. 2023. \u201cShifts in Vegetation Activity of Terrestrial Ecosystems Attributable to Climate Trends.\u201d Nature Geoscience 16 (2): 147\u201353. https://doi.org/10.1038/s41561-022-01114-x.\n\n\nHuang, Sha, Lina Tang, Joseph P. Hupy, Yang Wang, and Guofan Shao. 2020. \u201cA Commentary Review on the Use of Normalized Difference Vegetation Index (NDVI) in the Era of Popular Remote Sensing.\u201d Journal of Forestry Research 32 (1): 1\u20136. https://doi.org/10.1007/s11676-020-01155-1.\n\n\nMahecha, Miguel D., Fabian Gans, Gunnar Brandt, Rune Christiansen, Sarah E. Cornell, Normann Fomferra, Guido Kraemer, et al. 2020. \u201cEarth System Data Cubes Unravel Global Multivariate Dynamics.\u201d Earth System Dynamics 11 (1): 201\u201334. https://doi.org/10.5194/esd-11-201-2020.\n\n\nMontero, David, C\u00e9sar Aybar, Miguel D. Mahecha, Francesco Martinuzzi, Maximilian S\u00f6chting, and Sebastian Wieneke. 2023. \u201cA Standardized Catalogue of Spectral Indices to Advance the Use of Remote Sensing in Earth System Research.\u201d Scientific Data 10 (1). https://doi.org/10.1038/s41597-023-02096-0.\n\n\nMontero Loaiza, David, Guido Kraemer, Anca Anghelea, Cesar Aybar Camacho, Gunnar Brandt, Gustau Camps-Valls, Felix Cremer, et al. 2023. \u201cData Cubes for Earth System Research: Challenges Ahead,\u201d July. https://doi.org/10.31223/x58m2v.\n\n\nReddy, D. Sushma, and P. Rama Chandra Prasad. 2018. \u201cPrediction of Vegetation Dynamics Using NDVI Time Series Data and LSTM.\u201d Modeling Earth Systems and Environment 4 (April): 409\u201319. https://doi.org/10.1007/s40808-018-0431-3.\n\n\nShumway, Robert H, David S Stoffer, Robert H Shumway, and David S Stoffer. 2017. \u201cARIMA Models.\u201d Time Series Analysis and Its Applications: With R Examples, 75\u2013163.\n\n\nWang, Siyuan, Bojuan Yang, Qichun Yang, Linlin Lu, Xiaoyue Wang, and Yaoyao Peng. 2016. \u201cTemporal Trends and Spatial Variability of Vegetation Phenology over the Northern Hemisphere During 1982-2012.\u201d PLoS ONE 11 (June). https://doi.org/10.1371/journal.pone.0157134."
    },
    {
        "objectID": "Notebooks/1_Data_Preprocessing_Final.html#deepextremes-minicubes",
        "href": "Notebooks/1_Data_Preprocessing_Final.html#deepextremes-minicubes",
        "title": "2\u00a0 Overview",
        "section": "2.1 DeepExtremes Minicubes",
        "text": "2.1 DeepExtremes Minicubes\nThe DeepExtremes project provides a dataset of minicubes, which are small, manageable data cubes that contain various types of remote sensing data. Each minicube is a 3D array with dimensions (495 x 128 x 128) representing time, latitude, and longitude, and they are designed to facilitate the study of extreme events and their impacts on the Earth system.\n\n2.1.1 Key Features of the Minicubes:\n\nSpatial Resolution: Each minicube covers an area with a spatial resolution of 20 meters.\nTemporal Coverage: Data spans from January 1, 2016, to October 10, 2022, with a time resolution of 5 days.\nSpectral Bands: The minicubes include reflectance data from several Sentinel-2 spectral bands (B02, B03, B04, B05, B06, B07, B8A), as well as additional layers such as the Scene Classification Layer (SCL) and cloud masks.\nAdditional Variables: ERA5 reanalysis data (e.g., evaporation, surface pressure, temperature), DEM (Digital Elevation Model) data, and event labels for extreme occurrences.\n\nEach minicube is rich with metadata, including details on the geographic location, creation date, data processing steps, and variable-specific attributes. This metadata ensures the data\u2019s integrity, traceability, and usability for scientific analysis."
    },
    {
        "objectID": "Notebooks/1_Data_Preprocessing_Final.html#description-and-overview",
        "href": "Notebooks/1_Data_Preprocessing_Final.html#description-and-overview",
        "title": "2\u00a0 Overview",
        "section": "2.2 Description and Overview",
        "text": "2.2 Description and Overview\n\n2.2.1 1. Data Import\n\nImport Data: The dataset is imported from an S3 bucket using AWS credentials.\nRemove Invalid Cubes: Cubes with invalid data are filtered out.\nData Distribution: Cubes are split into training, validation, and test sets based on a predefined split table.\n\n\n\n2.2.2 2. Dataset Initialization\n\nInitialize Dataset: The dataset is initialized with minicubes, each having dimensions of 495 (time periods) x 128 x 128 (pixels).\nNDVI Calculation: The Python class that initializes the dataset calculates the NDVI for each pixel, derived from the spectral bands B8A (near-infrared) and B04 (red).\n\n\n\n2.2.3 3. Data Processing\n\nChunking Data: Data is processed in chunks to prevent memory overflow.\nNDVI Preprocessing: NDVI values are preprocessed, and pixels with an average NDVI below 0.2 are masked.\nData Saving: Processed data is saved in float32 format to save memory.\n\n\n\n2.2.4 4. Sanity Checks\n\nCube Integrity: Verify the shape and dimensions of each cube.\nNDVI Distribution: Analyze the distribution of NDVI values.\nLow NDVI Values: Ensure no pixels have an average NDVI below 0.2.\nCube Class Comparison: Compare cube classes with NDVI values and masked values.\n\n\n\n2.2.5 5. Train/Test Split\n\nIdentify Suitable Starting Date: Determine the starting date for training data and set the time period for train and test data.\nPerform Train and Test Split: Split the data into training and test sets and save it to the appropriate directories.\n\n\n\n2.2.6 6. Handling of Missing Data\n\nApproach A: Handling NaNs with Outliers and Cloud Mask Integration\nApproach B: Interpolation with STL Decomposition"
    },
    {
        "objectID": "Notebooks/1_Data_Preprocessing_Final.html#data-import-1",
        "href": "Notebooks/1_Data_Preprocessing_Final.html#data-import-1",
        "title": "2\u00a0 Overview",
        "section": "2.3 1. Data Import",
        "text": "2.3 1. Data Import\nFirst we import the necessary packages and the deep-extremes-minicubes dataset from the S3 Bucket, remove invalid cubes and split the dataset.\n\n# Define base_dir for consistent path management\nfrom pathlib import Path\nimport os\n\nnotebook_dir = Path(os.getcwd()).resolve()\n\nbase_dir = notebook_dir.parent \n\nprint(base_dir)\n\n/home/cgoehler/team-extra/ndvi-time-series-prediction\n\n\n\n# Import necessary packages and custom functions\nimport sys\nsys.path.insert(0, os.path.join(base_dir, \"src\", \"data_processing\"))\nimport s3fs\nimport itertools\nimport zarr\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xarray as xr\nimport numpy as np\nfrom import_cubes import *\nfrom helper import *\nimport torch\nfrom my_loader import DeepCubeTSDatasetBasti\nfrom process_ndvi import *\nfrom sanity_checks import *\nfrom stl_interpolation import *\nfrom statsmodels.tsa.seasonal import STL\n\n\n# AWS Credentials\nAWS_ACCESS_KEY_ID = \"***\"\nAWS_SECRET_ACCESS_KEY = \"***\"\nAWS_DEFAULT_REGION = \"eu-central-1\"\n\n\n# Initialize S3FileSystem\nminicubefs = s3fs.S3FileSystem(key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY)\n\n\n# Read registry from S3FileSystem\nbucket_path = \"s3://deepextremes-minicubes/1.2.2\"\nregistry_df = read_registry(bucket_path, minicubefs)\nregistry_df.shape\n\n(5593, 1)\n\n\n\n# Remove bad cubes\nprint(\"Initial number of Cubes: \", registry_df.shape[0])\nregistry_df_filtered = remove_cubes(registry_df)\nprint(\"Number of Cubes after removal: \", registry_df_filtered.shape[0])\n\nInitial number of Cubes:  5593\nNumber of Cubes after removal:  5397\n\n\nTo ensure a balanced data distribution over the entire globe while reducing the total number of cubes, we use the split provided in the split_table.csv file.\nSubsequently, we will only utilize the cubes within the trainingset. Since this is a time-series prediction, our train/validation/test split will be applied separately to each cube by splitting the time series of each cube individually.\n\n# Split cubes in training, validation and testset\nsplit_table_path = base_dir / \"csvs\" / \"split_table.csv\"\ntraincubes, valcubes, testcubes = split_datasets(\n    cube_registry=registry_df_filtered, split_table_path=split_table_path\n)\n\n/home/bastiloeblein/team-extra/ndvi-time-series-prediction/Notebooks/../src/data_processing/import_cubes.py:88: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  cube_registry[\"mc_id\"] = cube_registry[\"mc_id\"].apply(preprocess_mc_id)\n\n\n\nprint(f\"Number of entries in the training dataset: {len(traincubes)}\")\nprint(f\"Number of entries in the validation dataset: {len(valcubes)}\")\nprint(f\"Number of entries in the test dataset: {len(testcubes)}\")\n\nNumber of entries in the training dataset: 3052\nNumber of entries in the validation dataset: 666\nNumber of entries in the test dataset: 97\n\n\n\n# Define the S3 bucket name\ns3_bucket = \"deepextremes-minicubes/1.2.2\"\n\n\n# Get quantile data\nquantile_data = get_var_quantiles()\n# dict(itertools.islice(quantile_data.items(), 1))"
    },
    {
        "objectID": "Notebooks/1_Data_Preprocessing_Final.html#dataset-initialization-1",
        "href": "Notebooks/1_Data_Preprocessing_Final.html#dataset-initialization-1",
        "title": "2\u00a0 Overview",
        "section": "2.4 2. Dataset Initialization",
        "text": "2.4 2. Dataset Initialization\nWe initialize the dataset containing minicubes (each with dimension 495 (time periods) x 128 x 128 (pixels)) with additional information on pixel-wise NDVI and cube classes (e.g.\u00a0soil, meadow).\nDue to data quality and memory problems we decided to only use 100 cubes. Therefore we identified the 100 cubes, which have the most time periods containing &gt;80% pixels with not nan values.\n\nvalid_cubes_path = base_dir / \"csvs\" / \"Final_Cubes.csv\"\nvalid_df =pd.read_csv(valid_cubes_path, sep = \";\")\nvalid_indices = valid_df[\"Cube_ID\"].to_list()\n\n\n# Initialize dataset\ndataset = DeepCubeTSDatasetBasti(\n    minicubefs, bucket_path, traincubes, quantile_data, valid_indices\n)\n\n\ndataset.__repr__\n\n&lt;bound method DeepCubeTSDatasetBasti.__repr__ of &lt;(128 * 128 data points per cube): 16.384 * 100 (number of cubes) = 1638400 (Total number of data points)&gt;&gt;"
    },
    {
        "objectID": "Notebooks/1_Data_Preprocessing_Final.html#data-processing-1",
        "href": "Notebooks/1_Data_Preprocessing_Final.html#data-processing-1",
        "title": "2\u00a0 Overview",
        "section": "2.5 3. Data Processing",
        "text": "2.5 3. Data Processing\nThe data is processed in chunks to manage memory usage.\nNDVI values are preprocessed to mask pixels where NDVI prediction is not meaningful (e.g.\u00a0pixels that do not contain any vegetation). Therefore, we will mask pixels that have an average NDVI value below 0.2 (https://earthobservatory.nasa.gov/features/MeasuringVegetation) over the entire time period.\n\n# Initialize an empty dictionary to store the NDVI data\nndvi_data = {}\n\n# Process the cubes for the valid indices\nfor i in valid_indices:\n    print(i)\n    ndvi_tensor, cloudmask_tensor, lat, lon, cube_class = dataset.__getitem__(i)\n    \n    # Calculate the average NDVI for each pixel\n    average_ndvi = calculate_average_ndvi_for_each_pixel(ndvi_tensor)\n\n    # Mask low NDVI values\n    ndvi_masked = mask_low_ndvi_values(average_ndvi, ndvi_tensor)\n\n    # Convert the PyTorch tensors to NumPy arrays\n    ndvi_data = ndvi_masked.numpy()\n    cloud_mask_data = cloudmask_tensor.numpy()\n    \n    # Replace all -9999.0 values with NaN in the NDVI data\n    ndvi_data = np.where(ndvi_data == -9999.0, np.nan, ndvi_data)\n\n    # Generate the dates\n    dates = pd.date_range(start='2016-01-01', periods=495, freq='5D')\n    \n    # Create xarray DataArrays\n    ndvi = xr.DataArray(ndvi_data, coords=[dates, range(128), range(128)], dims=['time', 'x', 'y'], name='NDVI')\n    cloud_mask = xr.DataArray(cloud_mask_data, coords=[dates, range(128), range(128)], dims=['time', 'x', 'y'], name='Cloud_Mask')\n    \n    # Combine into a Dataset\n    data = xr.Dataset({\n        'NDVI': ndvi,\n        'Cloud_Mask': cloud_mask\n    })\n\n\n    # Add lat and lon as attributes\n    data.attrs['lat'] = lat\n    data.attrs['lon'] = lon\n    data.attrs['class'] = cube_class\n\n    # Add metadata to the variables and dataset\n    data['NDVI'].attrs['description'] = 'Normalized Difference Vegetation Index'\n    data['Cloud_Mask'].attrs['description'] = 'Cloud Mask (0=clear, 1=cloudy)'\n    data.attrs['source'] = f'Cube_{i}'\n\n    # Save to NetCDF\n    save_dir = base_dir / \"data\" / \"data_final_updated\"\n    data.to_netcdf(save_dir / f'Cube_{i}.nc')\n\n\n\n\ndata\n\n\n2.5.1 4. Sanity Checks\nAt last we conduct some sanity checks:\n\nCube Integrity: Ensure that the shape and dimensions of each cube are consistent and correct.\nVerify that the number of time steps, rows, and columns match the expected dimensions.\nDistribution of NDVI Values: Analyze the distribution of NDVI values to ensure they fall within the expected range [0, 1].\nCheck for any unexpected outliers or anomalies in the data.\nNo Pixels with Average NDVI Below 0.2: Ensure that there are no pixels with an average NDVI value below 0.2 over the entire time period.\nComparison with Cube Class: Compare the cube class with the overall average NDVI (considering all pixels and time periods).\nAdditionally, compare the number of masked values with the cube class to check for consistency.\n\n\n# Lists to store errors\nerror_list = []\nbad_cubes = []\n\n# Path to the directory with NDVI chunks\ndata_dir = base_dir / \"data\" / \"data_final_updated\"\ndata_list = os.listdir(data_dir)\nprint(len(data_list))\n\n# Loop pver all NetCDF files\nfor nc_file in data_list:\n    if nc_file.endswith('.nc'):\n        nc_path = os.path.join(data_dir, nc_file)\n        print(f\"Processing {nc_path}...\")\n\n        # Load the currend NetCDF file\n        data = xr.open_dataset(nc_path)\n        ndvi_data = data['NDVI'].values\n        \n        cube_class = data.attrs.get('class')\n        \n        # Expected shape of the NDVI cubes\n        expected_shape = (495, 128, 128)\n\n        try:\n            # Step 1: Check cube integrity\n            check_cube_integrity(ndvi_data, expected_shape)\n\n            # Step 2: Check the distribution of NDVI values\n            check_ndvi_distribution(ndvi_data)\n\n            # Step 3: Ensure no pixels have an average NDVI below 0.2 and calculate additional metrics\n            overall_avg_ndvi, num_masked_values = check_if_contains_low_values(ndvi_data)\n\n            print(f\"Cube {nc_file} - Class: {cube_class}, Overall Average NDVI: {overall_avg_ndvi}, Number of Masked Values: {num_masked_values}\")\n            print(f\"Cube {nc_file} passed all sanity checks.\\n\")\n\n        except AssertionError as e:\n            error_message = f\"Cube {nc_file} failed sanity check: {str(e)}\"\n            print(error_message)\n            error_list.append(error_message)\n        except Exception as e:\n            error_message = f\"Cube {nc_file} encountered an error: {str(e)}\"\n            print(error_message)\n            bad_cubes.append(nc_path)\n\n# Display all errors\nif error_list:\n    print(\"\\nSummary of errors:\")\n    for error in error_list:\n        print(error)\nelse:\n    print(\"\\nAll cubes passed sanity checks.\")\n\n# Display list of all bad cubes\nif bad_cubes:\n    print(\"\\nList of bad cubes:\")\n    for bad_cube in bad_cubes:\n        print(bad_cube)\n\n\n\n2.5.2 5. Train/Test Split\nIn this section, we will perform the train and test split for our time series data. Here\u2019s a structured plan for what we will accomplish:\n\nIdentify Suitable Starting Date:\n\nWe have observed that the initial portion of our time series (particularly in the year 2016) contains a significant number of NaN values.\nTo address this, we will conduct a brief analysis to determine an appropriate starting date for our time series prediction. This step is crucial to ensure we have sufficient data for effective interpolation especially at the beginning of our time series.\n\nPerform Train/Test Split:\n\nOnce a reasonable starting date is identified, we will proceed to split the data into train and test sets.\nThe train set will be used to train our model, while the test set will be used to evaluate its performance.\n\n\nBy carefully selecting a starting point and splitting the data, we aim to enhance the accuracy and reliability of our time series predictions and interpolation of missing values.\n\n2.5.2.1 5.1 Identify Suitable Starting Date\n\nfile_path = base_dir / \"data\" / \"data_final_updated\"\nnc_files = [file for file in os.listdir(file_path) if file.endswith('.nc')]\nprint(f\"Number of files: {len(nc_files)}\")\n\nNumber of files: 100\n\n\nUpon analyzing our dataset, we observed that for the year 2016, there are only sporadic pixels containing NDVI values over all cubes, with the majority being NaNs. To ensure the quality and completeness of our data for time series prediction, we will exclude the year 2016 from our dataset.\n\n# Initialize a dictionary to store the results\nresults = {}\n\n# Iterate over each .nc file and perform the calculations\nfor nc_file in nc_files:\n    full_path = os.path.join(file_path, nc_file)\n    ds = xr.open_dataset(full_path)\n    \n    ndvi_data = ds['NDVI'].values  # Extract NDVI data\n    time_data = ds['time'].values  # Extract time data\n    \n    for i, time_step in enumerate(time_data):\n        # Count the number of non-NaN values for the current time step\n        non_nan_count = np.sum(~np.isnan(ndvi_data[i, ...]))\n        \n        # Convert timestamp to readable date format if necessary\n        if isinstance(time_step, np.datetime64):\n            time_step = str(time_step)\n        \n        # Store the results\n        if time_step in results:\n            results[time_step] += non_nan_count\n        else:\n            results[time_step] = non_nan_count\n\n# Output the results\nfor time_step, non_nan_count in results.items():\n    print(f\"Date: {time_step} - Number of non-NaN NDVI values: {non_nan_count}\")\n\nDate: 2016-01-01T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-01-06T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-01-11T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-01-16T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-01-21T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-01-26T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-01-31T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-02-05T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-02-10T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-02-15T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-02-20T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-02-25T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-03-01T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-03-06T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-03-11T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-03-16T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-03-21T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-03-26T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-03-31T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-04-05T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-04-10T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-04-15T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-04-20T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-04-25T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-04-30T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-05-05T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-05-10T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-05-15T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-05-20T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-05-25T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-05-30T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-06-04T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-06-09T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-06-14T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-06-19T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-06-24T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-06-29T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-07-04T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-07-09T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-07-14T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-07-19T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-07-24T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-07-29T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-08-03T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-08-08T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-08-13T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-08-18T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-08-23T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-08-28T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-09-02T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-09-07T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-09-12T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-09-17T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-09-22T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-09-27T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-10-02T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-10-07T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-10-12T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-10-17T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-10-22T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-10-27T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-11-01T00:00:00.000000000 - Number of non-NaN NDVI values: 735\nDate: 2016-11-06T00:00:00.000000000 - Number of non-NaN NDVI values: 15236\nDate: 2016-11-11T00:00:00.000000000 - Number of non-NaN NDVI values: 16074\nDate: 2016-11-16T00:00:00.000000000 - Number of non-NaN NDVI values: 29670\nDate: 2016-11-21T00:00:00.000000000 - Number of non-NaN NDVI values: 16149\nDate: 2016-11-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1212\nDate: 2016-12-01T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-12-06T00:00:00.000000000 - Number of non-NaN NDVI values: 29670\nDate: 2016-12-11T00:00:00.000000000 - Number of non-NaN NDVI values: 63\nDate: 2016-12-16T00:00:00.000000000 - Number of non-NaN NDVI values: 455\nDate: 2016-12-21T00:00:00.000000000 - Number of non-NaN NDVI values: 0\nDate: 2016-12-26T00:00:00.000000000 - Number of non-NaN NDVI values: 29644\nDate: 2016-12-31T00:00:00.000000000 - Number of non-NaN NDVI values: 154887\nDate: 2017-01-05T00:00:00.000000000 - Number of non-NaN NDVI values: 258414\nDate: 2017-01-10T00:00:00.000000000 - Number of non-NaN NDVI values: 362279\nDate: 2017-01-15T00:00:00.000000000 - Number of non-NaN NDVI values: 813709\nDate: 2017-01-20T00:00:00.000000000 - Number of non-NaN NDVI values: 174809\nDate: 2017-01-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1231339\nDate: 2017-01-30T00:00:00.000000000 - Number of non-NaN NDVI values: 638865\nDate: 2017-02-04T00:00:00.000000000 - Number of non-NaN NDVI values: 599125\nDate: 2017-02-09T00:00:00.000000000 - Number of non-NaN NDVI values: 441039\nDate: 2017-02-14T00:00:00.000000000 - Number of non-NaN NDVI values: 650624\nDate: 2017-02-19T00:00:00.000000000 - Number of non-NaN NDVI values: 485039\nDate: 2017-02-24T00:00:00.000000000 - Number of non-NaN NDVI values: 553548\nDate: 2017-03-01T00:00:00.000000000 - Number of non-NaN NDVI values: 290621\nDate: 2017-03-06T00:00:00.000000000 - Number of non-NaN NDVI values: 707662\nDate: 2017-03-11T00:00:00.000000000 - Number of non-NaN NDVI values: 617145\nDate: 2017-03-16T00:00:00.000000000 - Number of non-NaN NDVI values: 601657\nDate: 2017-03-21T00:00:00.000000000 - Number of non-NaN NDVI values: 297106\nDate: 2017-03-26T00:00:00.000000000 - Number of non-NaN NDVI values: 512963\nDate: 2017-03-31T00:00:00.000000000 - Number of non-NaN NDVI values: 460329\nDate: 2017-04-05T00:00:00.000000000 - Number of non-NaN NDVI values: 641623\nDate: 2017-04-10T00:00:00.000000000 - Number of non-NaN NDVI values: 186476\nDate: 2017-04-15T00:00:00.000000000 - Number of non-NaN NDVI values: 864421\nDate: 2017-04-20T00:00:00.000000000 - Number of non-NaN NDVI values: 539825\nDate: 2017-04-25T00:00:00.000000000 - Number of non-NaN NDVI values: 332112\nDate: 2017-04-30T00:00:00.000000000 - Number of non-NaN NDVI values: 413190\nDate: 2017-05-05T00:00:00.000000000 - Number of non-NaN NDVI values: 643990\nDate: 2017-05-10T00:00:00.000000000 - Number of non-NaN NDVI values: 527493\nDate: 2017-05-15T00:00:00.000000000 - Number of non-NaN NDVI values: 668972\nDate: 2017-05-20T00:00:00.000000000 - Number of non-NaN NDVI values: 418498\nDate: 2017-05-25T00:00:00.000000000 - Number of non-NaN NDVI values: 668569\nDate: 2017-05-30T00:00:00.000000000 - Number of non-NaN NDVI values: 440164\nDate: 2017-06-04T00:00:00.000000000 - Number of non-NaN NDVI values: 838649\nDate: 2017-06-09T00:00:00.000000000 - Number of non-NaN NDVI values: 400726\nDate: 2017-06-14T00:00:00.000000000 - Number of non-NaN NDVI values: 802162\nDate: 2017-06-19T00:00:00.000000000 - Number of non-NaN NDVI values: 576634\nDate: 2017-06-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1064392\nDate: 2017-06-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1396831\nDate: 2017-07-04T00:00:00.000000000 - Number of non-NaN NDVI values: 904167\nDate: 2017-07-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1014362\nDate: 2017-07-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1235566\nDate: 2017-07-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1039258\nDate: 2017-07-24T00:00:00.000000000 - Number of non-NaN NDVI values: 801631\nDate: 2017-07-29T00:00:00.000000000 - Number of non-NaN NDVI values: 964584\nDate: 2017-08-03T00:00:00.000000000 - Number of non-NaN NDVI values: 711300\nDate: 2017-08-08T00:00:00.000000000 - Number of non-NaN NDVI values: 794347\nDate: 2017-08-13T00:00:00.000000000 - Number of non-NaN NDVI values: 980194\nDate: 2017-08-18T00:00:00.000000000 - Number of non-NaN NDVI values: 921815\nDate: 2017-08-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1200440\nDate: 2017-08-28T00:00:00.000000000 - Number of non-NaN NDVI values: 625530\nDate: 2017-09-02T00:00:00.000000000 - Number of non-NaN NDVI values: 874742\nDate: 2017-09-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1180835\nDate: 2017-09-12T00:00:00.000000000 - Number of non-NaN NDVI values: 1023301\nDate: 2017-09-17T00:00:00.000000000 - Number of non-NaN NDVI values: 265021\nDate: 2017-09-22T00:00:00.000000000 - Number of non-NaN NDVI values: 560183\nDate: 2017-09-27T00:00:00.000000000 - Number of non-NaN NDVI values: 971147\nDate: 2017-10-02T00:00:00.000000000 - Number of non-NaN NDVI values: 995119\nDate: 2017-10-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1150902\nDate: 2017-10-12T00:00:00.000000000 - Number of non-NaN NDVI values: 983103\nDate: 2017-10-17T00:00:00.000000000 - Number of non-NaN NDVI values: 1193391\nDate: 2017-10-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1383536\nDate: 2017-10-27T00:00:00.000000000 - Number of non-NaN NDVI values: 969612\nDate: 2017-11-01T00:00:00.000000000 - Number of non-NaN NDVI values: 869122\nDate: 2017-11-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1040188\nDate: 2017-11-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1093029\nDate: 2017-11-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1307050\nDate: 2017-11-21T00:00:00.000000000 - Number of non-NaN NDVI values: 842467\nDate: 2017-11-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1031091\nDate: 2017-12-01T00:00:00.000000000 - Number of non-NaN NDVI values: 700918\nDate: 2017-12-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1223201\nDate: 2017-12-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1007335\nDate: 2017-12-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1116969\nDate: 2017-12-21T00:00:00.000000000 - Number of non-NaN NDVI values: 895879\nDate: 2017-12-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1105935\nDate: 2017-12-31T00:00:00.000000000 - Number of non-NaN NDVI values: 640445\nDate: 2018-01-05T00:00:00.000000000 - Number of non-NaN NDVI values: 895603\nDate: 2018-01-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1127206\nDate: 2018-01-15T00:00:00.000000000 - Number of non-NaN NDVI values: 838006\nDate: 2018-01-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1196178\nDate: 2018-01-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1099987\nDate: 2018-01-30T00:00:00.000000000 - Number of non-NaN NDVI values: 855080\nDate: 2018-02-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1167105\nDate: 2018-02-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1214804\nDate: 2018-02-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1003651\nDate: 2018-02-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1036841\nDate: 2018-02-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1088853\nDate: 2018-03-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1247952\nDate: 2018-03-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1067825\nDate: 2018-03-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1000610\nDate: 2018-03-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1011396\nDate: 2018-03-21T00:00:00.000000000 - Number of non-NaN NDVI values: 753163\nDate: 2018-03-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1265163\nDate: 2018-03-31T00:00:00.000000000 - Number of non-NaN NDVI values: 1265194\nDate: 2018-04-05T00:00:00.000000000 - Number of non-NaN NDVI values: 726434\nDate: 2018-04-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1166807\nDate: 2018-04-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1033506\nDate: 2018-04-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1250892\nDate: 2018-04-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1450697\nDate: 2018-04-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1147316\nDate: 2018-05-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1359881\nDate: 2018-05-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1450309\nDate: 2018-05-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1416305\nDate: 2018-05-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1326400\nDate: 2018-05-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1309329\nDate: 2018-05-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1450126\nDate: 2018-06-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1436022\nDate: 2018-06-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1461019\nDate: 2018-06-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1317819\nDate: 2018-06-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1449900\nDate: 2018-06-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1460091\nDate: 2018-06-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1351880\nDate: 2018-07-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1167428\nDate: 2018-07-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1209222\nDate: 2018-07-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1065911\nDate: 2018-07-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1393458\nDate: 2018-07-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1293343\nDate: 2018-07-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1304142\nDate: 2018-08-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1403274\nDate: 2018-08-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1253138\nDate: 2018-08-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1267069\nDate: 2018-08-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1309457\nDate: 2018-08-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1184789\nDate: 2018-08-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1175814\nDate: 2018-09-02T00:00:00.000000000 - Number of non-NaN NDVI values: 1124725\nDate: 2018-09-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1296429\nDate: 2018-09-12T00:00:00.000000000 - Number of non-NaN NDVI values: 1467258\nDate: 2018-09-17T00:00:00.000000000 - Number of non-NaN NDVI values: 1264949\nDate: 2018-09-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1331015\nDate: 2018-09-27T00:00:00.000000000 - Number of non-NaN NDVI values: 1311157\nDate: 2018-10-02T00:00:00.000000000 - Number of non-NaN NDVI values: 877577\nDate: 2018-10-07T00:00:00.000000000 - Number of non-NaN NDVI values: 842026\nDate: 2018-10-12T00:00:00.000000000 - Number of non-NaN NDVI values: 946839\nDate: 2018-10-17T00:00:00.000000000 - Number of non-NaN NDVI values: 1086916\nDate: 2018-10-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1290187\nDate: 2018-10-27T00:00:00.000000000 - Number of non-NaN NDVI values: 1201566\nDate: 2018-11-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1211751\nDate: 2018-11-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1391392\nDate: 2018-11-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1180698\nDate: 2018-11-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1206150\nDate: 2018-11-21T00:00:00.000000000 - Number of non-NaN NDVI values: 1219878\nDate: 2018-11-26T00:00:00.000000000 - Number of non-NaN NDVI values: 997135\nDate: 2018-12-01T00:00:00.000000000 - Number of non-NaN NDVI values: 913512\nDate: 2018-12-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1109780\nDate: 2018-12-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1284869\nDate: 2018-12-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1276355\nDate: 2018-12-21T00:00:00.000000000 - Number of non-NaN NDVI values: 842122\nDate: 2018-12-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1041945\nDate: 2018-12-31T00:00:00.000000000 - Number of non-NaN NDVI values: 1402546\nDate: 2019-01-05T00:00:00.000000000 - Number of non-NaN NDVI values: 766200\nDate: 2019-01-10T00:00:00.000000000 - Number of non-NaN NDVI values: 572778\nDate: 2019-01-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1007771\nDate: 2019-01-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1217585\nDate: 2019-01-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1107193\nDate: 2019-01-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1194286\nDate: 2019-02-04T00:00:00.000000000 - Number of non-NaN NDVI values: 989976\nDate: 2019-02-09T00:00:00.000000000 - Number of non-NaN NDVI values: 998637\nDate: 2019-02-14T00:00:00.000000000 - Number of non-NaN NDVI values: 489968\nDate: 2019-02-19T00:00:00.000000000 - Number of non-NaN NDVI values: 709547\nDate: 2019-02-24T00:00:00.000000000 - Number of non-NaN NDVI values: 900770\nDate: 2019-03-01T00:00:00.000000000 - Number of non-NaN NDVI values: 729155\nDate: 2019-03-06T00:00:00.000000000 - Number of non-NaN NDVI values: 824081\nDate: 2019-03-11T00:00:00.000000000 - Number of non-NaN NDVI values: 971608\nDate: 2019-03-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1388846\nDate: 2019-03-21T00:00:00.000000000 - Number of non-NaN NDVI values: 809416\nDate: 2019-03-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1224888\nDate: 2019-03-31T00:00:00.000000000 - Number of non-NaN NDVI values: 1188069\nDate: 2019-04-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1323358\nDate: 2019-04-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1162629\nDate: 2019-04-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1192496\nDate: 2019-04-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1152134\nDate: 2019-04-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1308515\nDate: 2019-04-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1377644\nDate: 2019-05-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1063192\nDate: 2019-05-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1331953\nDate: 2019-05-15T00:00:00.000000000 - Number of non-NaN NDVI values: 909097\nDate: 2019-05-20T00:00:00.000000000 - Number of non-NaN NDVI values: 896371\nDate: 2019-05-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1216011\nDate: 2019-05-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1294520\nDate: 2019-06-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1499912\nDate: 2019-06-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1300057\nDate: 2019-06-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1476098\nDate: 2019-06-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1486866\nDate: 2019-06-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1466269\nDate: 2019-06-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1393771\nDate: 2019-07-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1408219\nDate: 2019-07-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1527057\nDate: 2019-07-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1275057\nDate: 2019-07-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1279632\nDate: 2019-07-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1302231\nDate: 2019-07-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1304670\nDate: 2019-08-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1193378\nDate: 2019-08-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1208343\nDate: 2019-08-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1463950\nDate: 2019-08-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1440332\nDate: 2019-08-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1417718\nDate: 2019-08-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1319719\nDate: 2019-09-02T00:00:00.000000000 - Number of non-NaN NDVI values: 1367711\nDate: 2019-09-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1224311\nDate: 2019-09-12T00:00:00.000000000 - Number of non-NaN NDVI values: 1270193\nDate: 2019-09-17T00:00:00.000000000 - Number of non-NaN NDVI values: 1275820\nDate: 2019-09-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1532260\nDate: 2019-09-27T00:00:00.000000000 - Number of non-NaN NDVI values: 1266551\nDate: 2019-10-02T00:00:00.000000000 - Number of non-NaN NDVI values: 1370441\nDate: 2019-10-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1414227\nDate: 2019-10-12T00:00:00.000000000 - Number of non-NaN NDVI values: 1533796\nDate: 2019-10-17T00:00:00.000000000 - Number of non-NaN NDVI values: 1475055\nDate: 2019-10-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1372160\nDate: 2019-10-27T00:00:00.000000000 - Number of non-NaN NDVI values: 1337412\nDate: 2019-11-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1430690\nDate: 2019-11-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1202601\nDate: 2019-11-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1178391\nDate: 2019-11-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1068666\nDate: 2019-11-21T00:00:00.000000000 - Number of non-NaN NDVI values: 1107350\nDate: 2019-11-26T00:00:00.000000000 - Number of non-NaN NDVI values: 626652\nDate: 2019-12-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1035307\nDate: 2019-12-06T00:00:00.000000000 - Number of non-NaN NDVI values: 839038\nDate: 2019-12-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1140745\nDate: 2019-12-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1264021\nDate: 2019-12-21T00:00:00.000000000 - Number of non-NaN NDVI values: 954351\nDate: 2019-12-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1213662\nDate: 2019-12-31T00:00:00.000000000 - Number of non-NaN NDVI values: 1039856\nDate: 2020-01-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1144339\nDate: 2020-01-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1292090\nDate: 2020-01-15T00:00:00.000000000 - Number of non-NaN NDVI values: 910265\nDate: 2020-01-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1120426\nDate: 2020-01-25T00:00:00.000000000 - Number of non-NaN NDVI values: 994905\nDate: 2020-01-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1256509\nDate: 2020-02-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1197279\nDate: 2020-02-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1106574\nDate: 2020-02-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1169250\nDate: 2020-02-19T00:00:00.000000000 - Number of non-NaN NDVI values: 854661\nDate: 2020-02-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1367902\nDate: 2020-02-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1191210\nDate: 2020-03-05T00:00:00.000000000 - Number of non-NaN NDVI values: 936542\nDate: 2020-03-10T00:00:00.000000000 - Number of non-NaN NDVI values: 664485\nDate: 2020-03-15T00:00:00.000000000 - Number of non-NaN NDVI values: 638823\nDate: 2020-03-20T00:00:00.000000000 - Number of non-NaN NDVI values: 893348\nDate: 2020-03-25T00:00:00.000000000 - Number of non-NaN NDVI values: 722596\nDate: 2020-03-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1304162\nDate: 2020-04-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1150964\nDate: 2020-04-09T00:00:00.000000000 - Number of non-NaN NDVI values: 830741\nDate: 2020-04-14T00:00:00.000000000 - Number of non-NaN NDVI values: 884877\nDate: 2020-04-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1015297\nDate: 2020-04-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1298293\nDate: 2020-04-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1476600\nDate: 2020-05-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1377043\nDate: 2020-05-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1313016\nDate: 2020-05-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1215302\nDate: 2020-05-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1430985\nDate: 2020-05-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1420325\nDate: 2020-05-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1123239\nDate: 2020-06-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1265310\nDate: 2020-06-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1492647\nDate: 2020-06-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1310120\nDate: 2020-06-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1530476\nDate: 2020-06-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1320043\nDate: 2020-06-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1396954\nDate: 2020-07-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1350658\nDate: 2020-07-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1376557\nDate: 2020-07-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1220621\nDate: 2020-07-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1125643\nDate: 2020-07-23T00:00:00.000000000 - Number of non-NaN NDVI values: 953489\nDate: 2020-07-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1419714\nDate: 2020-08-02T00:00:00.000000000 - Number of non-NaN NDVI values: 1421451\nDate: 2020-08-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1328570\nDate: 2020-08-12T00:00:00.000000000 - Number of non-NaN NDVI values: 1309194\nDate: 2020-08-17T00:00:00.000000000 - Number of non-NaN NDVI values: 1359476\nDate: 2020-08-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1393558\nDate: 2020-08-27T00:00:00.000000000 - Number of non-NaN NDVI values: 1336536\nDate: 2020-09-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1426648\nDate: 2020-09-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1178498\nDate: 2020-09-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1284034\nDate: 2020-09-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1338585\nDate: 2020-09-21T00:00:00.000000000 - Number of non-NaN NDVI values: 1427975\nDate: 2020-09-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1398051\nDate: 2020-10-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1322598\nDate: 2020-10-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1301989\nDate: 2020-10-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1450704\nDate: 2020-10-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1411401\nDate: 2020-10-21T00:00:00.000000000 - Number of non-NaN NDVI values: 1209509\nDate: 2020-10-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1298224\nDate: 2020-10-31T00:00:00.000000000 - Number of non-NaN NDVI values: 1208211\nDate: 2020-11-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1077463\nDate: 2020-11-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1332641\nDate: 2020-11-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1238394\nDate: 2020-11-20T00:00:00.000000000 - Number of non-NaN NDVI values: 976203\nDate: 2020-11-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1442804\nDate: 2020-11-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1468960\nDate: 2020-12-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1291332\nDate: 2020-12-10T00:00:00.000000000 - Number of non-NaN NDVI values: 783388\nDate: 2020-12-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1309828\nDate: 2020-12-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1267438\nDate: 2020-12-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1025053\nDate: 2020-12-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1428678\nDate: 2021-01-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1354112\nDate: 2021-01-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1294689\nDate: 2021-01-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1265808\nDate: 2021-01-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1047103\nDate: 2021-01-24T00:00:00.000000000 - Number of non-NaN NDVI values: 849325\nDate: 2021-01-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1155481\nDate: 2021-02-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1314973\nDate: 2021-02-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1222713\nDate: 2021-02-13T00:00:00.000000000 - Number of non-NaN NDVI values: 655904\nDate: 2021-02-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1278593\nDate: 2021-02-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1235474\nDate: 2021-02-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1326062\nDate: 2021-03-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1105634\nDate: 2021-03-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1051033\nDate: 2021-03-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1129931\nDate: 2021-03-20T00:00:00.000000000 - Number of non-NaN NDVI values: 810931\nDate: 2021-03-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1385991\nDate: 2021-03-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1366627\nDate: 2021-04-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1347199\nDate: 2021-04-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1272275\nDate: 2021-04-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1160426\nDate: 2021-04-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1100122\nDate: 2021-04-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1220997\nDate: 2021-04-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1184735\nDate: 2021-05-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1507479\nDate: 2021-05-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1369440\nDate: 2021-05-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1298532\nDate: 2021-05-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1284081\nDate: 2021-05-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1475327\nDate: 2021-05-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1242700\nDate: 2021-06-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1249449\nDate: 2021-06-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1468316\nDate: 2021-06-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1488158\nDate: 2021-06-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1420875\nDate: 2021-06-23T00:00:00.000000000 - Number of non-NaN NDVI values: 938676\nDate: 2021-06-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1108215\nDate: 2021-07-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1325758\nDate: 2021-07-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1397584\nDate: 2021-07-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1312019\nDate: 2021-07-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1160595\nDate: 2021-07-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1072330\nDate: 2021-07-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1233585\nDate: 2021-08-02T00:00:00.000000000 - Number of non-NaN NDVI values: 1425644\nDate: 2021-08-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1189717\nDate: 2021-08-12T00:00:00.000000000 - Number of non-NaN NDVI values: 1130541\nDate: 2021-08-17T00:00:00.000000000 - Number of non-NaN NDVI values: 1244330\nDate: 2021-08-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1429885\nDate: 2021-08-27T00:00:00.000000000 - Number of non-NaN NDVI values: 1400988\nDate: 2021-09-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1235652\nDate: 2021-09-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1490848\nDate: 2021-09-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1309929\nDate: 2021-09-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1493453\nDate: 2021-09-21T00:00:00.000000000 - Number of non-NaN NDVI values: 1352477\nDate: 2021-09-26T00:00:00.000000000 - Number of non-NaN NDVI values: 850782\nDate: 2021-10-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1189767\nDate: 2021-10-06T00:00:00.000000000 - Number of non-NaN NDVI values: 946703\nDate: 2021-10-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1308464\nDate: 2021-10-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1324177\nDate: 2021-10-21T00:00:00.000000000 - Number of non-NaN NDVI values: 840014\nDate: 2021-10-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1342931\nDate: 2021-10-31T00:00:00.000000000 - Number of non-NaN NDVI values: 1108334\nDate: 2021-11-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1184649\nDate: 2021-11-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1323340\nDate: 2021-11-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1204549\nDate: 2021-11-20T00:00:00.000000000 - Number of non-NaN NDVI values: 704829\nDate: 2021-11-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1371307\nDate: 2021-11-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1298220\nDate: 2021-12-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1085287\nDate: 2021-12-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1110481\nDate: 2021-12-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1208832\nDate: 2021-12-20T00:00:00.000000000 - Number of non-NaN NDVI values: 904836\nDate: 2021-12-25T00:00:00.000000000 - Number of non-NaN NDVI values: 574242\nDate: 2021-12-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1169220\nDate: 2022-01-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1189770\nDate: 2022-01-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1309437\nDate: 2022-01-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1278740\nDate: 2022-01-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1296185\nDate: 2022-01-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1404603\nDate: 2022-01-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1021273\nDate: 2022-02-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1339798\nDate: 2022-02-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1361600\nDate: 2022-02-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1395969\nDate: 2022-02-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1226100\nDate: 2022-02-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1288531\nDate: 2022-02-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1384529\nDate: 2022-03-05T00:00:00.000000000 - Number of non-NaN NDVI values: 1035053\nDate: 2022-03-10T00:00:00.000000000 - Number of non-NaN NDVI values: 1245686\nDate: 2022-03-15T00:00:00.000000000 - Number of non-NaN NDVI values: 1231153\nDate: 2022-03-20T00:00:00.000000000 - Number of non-NaN NDVI values: 1341335\nDate: 2022-03-25T00:00:00.000000000 - Number of non-NaN NDVI values: 1199186\nDate: 2022-03-30T00:00:00.000000000 - Number of non-NaN NDVI values: 1186419\nDate: 2022-04-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1534061\nDate: 2022-04-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1214160\nDate: 2022-04-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1230675\nDate: 2022-04-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1014514\nDate: 2022-04-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1256100\nDate: 2022-04-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1460838\nDate: 2022-05-04T00:00:00.000000000 - Number of non-NaN NDVI values: 1197087\nDate: 2022-05-09T00:00:00.000000000 - Number of non-NaN NDVI values: 1476932\nDate: 2022-05-14T00:00:00.000000000 - Number of non-NaN NDVI values: 1490087\nDate: 2022-05-19T00:00:00.000000000 - Number of non-NaN NDVI values: 1262806\nDate: 2022-05-24T00:00:00.000000000 - Number of non-NaN NDVI values: 1448367\nDate: 2022-05-29T00:00:00.000000000 - Number of non-NaN NDVI values: 1476842\nDate: 2022-06-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1192456\nDate: 2022-06-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1402978\nDate: 2022-06-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1453343\nDate: 2022-06-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1217817\nDate: 2022-06-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1181398\nDate: 2022-06-28T00:00:00.000000000 - Number of non-NaN NDVI values: 1409386\nDate: 2022-07-03T00:00:00.000000000 - Number of non-NaN NDVI values: 1307337\nDate: 2022-07-08T00:00:00.000000000 - Number of non-NaN NDVI values: 1471812\nDate: 2022-07-13T00:00:00.000000000 - Number of non-NaN NDVI values: 1392438\nDate: 2022-07-18T00:00:00.000000000 - Number of non-NaN NDVI values: 1367636\nDate: 2022-07-23T00:00:00.000000000 - Number of non-NaN NDVI values: 1298967\nDate: 2022-07-28T00:00:00.000000000 - Number of non-NaN NDVI values: 975686\nDate: 2022-08-02T00:00:00.000000000 - Number of non-NaN NDVI values: 1227866\nDate: 2022-08-07T00:00:00.000000000 - Number of non-NaN NDVI values: 1220408\nDate: 2022-08-12T00:00:00.000000000 - Number of non-NaN NDVI values: 1253823\nDate: 2022-08-17T00:00:00.000000000 - Number of non-NaN NDVI values: 971412\nDate: 2022-08-22T00:00:00.000000000 - Number of non-NaN NDVI values: 1262454\nDate: 2022-08-27T00:00:00.000000000 - Number of non-NaN NDVI values: 1391227\nDate: 2022-09-01T00:00:00.000000000 - Number of non-NaN NDVI values: 1434055\nDate: 2022-09-06T00:00:00.000000000 - Number of non-NaN NDVI values: 1347668\nDate: 2022-09-11T00:00:00.000000000 - Number of non-NaN NDVI values: 1164327\nDate: 2022-09-16T00:00:00.000000000 - Number of non-NaN NDVI values: 1358083\nDate: 2022-09-21T00:00:00.000000000 - Number of non-NaN NDVI values: 1160230\nDate: 2022-09-26T00:00:00.000000000 - Number of non-NaN NDVI values: 1369278\nDate: 2022-10-01T00:00:00.000000000 - Number of non-NaN NDVI values: 908823\nDate: 2022-10-06T00:00:00.000000000 - Number of non-NaN NDVI values: 954979\n\n\nAdditionally we plot the date of the first non-nan value occurence of each pixel overall cubes. Most pixel\u2019s first non-nan value occurence is on the 2017-01-15. And the latest occurence is on the 2017-05-05. This might motivate us to use the 2017-05-05 as starting value for our training data time series.\n\n# List to store the first non-NaN time indices for all pixels\nfirst_non_nan_indices = []\n\n# Loop over all NetCDF files\nfor nc_file in nc_files:\n    nc_path = os.path.join(file_path, nc_file)\n    \n    # Load the current NetCDF file\n    data = xr.open_dataset(nc_path)\n    ndvi_data = data['NDVI'].values\n    \n    # Get the size of the data\n    time_len, x_len, y_len = ndvi_data.shape\n    \n    # Find the first non-NaN time index for each pixel\n    for x in range(x_len):\n        for y in range(y_len):\n            first_non_nan_time_index = np.nan\n            for t in range(time_len):\n                if not np.isnan(ndvi_data[t, x, y]):\n                    first_non_nan_time_index = t\n                    break\n            first_non_nan_indices.append(first_non_nan_time_index)\n\n# Convert time indices to date values\nstart_date = pd.Timestamp('2016-01-01')\ndates = [start_date + pd.Timedelta(days=int(index) * 5) for index in first_non_nan_indices if not np.isnan(index)]\n\n# Count occurrences of each date\ndate_counts = pd.Series(dates).value_counts().sort_index()\n\n# Plot the distribution of first non-NaN dates as a bar plot\nplt.figure(figsize=(12, 6))\nplt.bar(date_counts.index, date_counts.values, edgecolor='black', alpha=0.7)\nplt.title('Distribution of First Non-NaN NDVI Dates for All Pixels')\nplt.xlabel('Date')\nplt.ylabel('Count of Pixels')\nplt.grid(True)\n\nplt.xticks(rotation=45)  # Rotate date labels for better readability\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Calculate the median of the date values\nmedian_date_num = np.median([date.toordinal() for date in dates])\nmedian_date = pd.Timestamp.fromordinal(int(median_date_num))\nmedian_date\n\nTimestamp('2017-01-15 00:00:00')\n\n\n\nmax(dates)\n\nTimestamp('2017-05-05 00:00:00')\n\n\nAnalyzing the distribution of non-NaN NDVI values over time reveals regular intervals during which fewer than 600,000 (of a possible 128 x 128 x 100 = 1,638,400 - so less than 40%) pixels show valid values for some measurement periods (every 5 days). This observation highlights the poor data quality and suggests that the prediction performance of our time series models will be significantly negatively affected by this substantial amount of missing data.\nHowever, we have chosen to start our analysis from July 2017, as the first period of missing data (&lt; 600k valid pixel values) ends here, and the next three measurement dates contain more than 1,000,000 valid pixels. As depicted by the red LOESS trend line in the graph, the number of valid pixels reaches a higher and more consistent level starting from this point, except for some subsequent periods with a low number of valid pixel values, particularly at the end of 2018 and the beginning of 2019.\nThus, we set our training period from July 4, 2017, to June 28, 2021, and the test period from July 3, 2021, to October 6, 2022. This ensures we have three complete seasonal cycles (apart from the periods with few data) available for training our models.\n\nimport matplotlib.dates as mdates\nimport statsmodels.api as sm\n\n# Extract data for plotting\ndates = list(results.keys())\nnon_nan_counts = list(results.values())\n\n# Convert dates to a readable format if they are in string format\nif isinstance(dates[0], str):\n    dates = [np.datetime64(date) for date in dates]\n\n# Convert dates to a numeric format for LOESS\nnumeric_dates = mdates.date2num(dates)\n\n# Fit LOESS model\nloess = sm.nonparametric.lowess(non_nan_counts, numeric_dates, frac=0.1)\n\n# Plot the results as a histogram and a LOESS smoothed line\nplt.figure(figsize=(10, 6))\nplt.bar(dates, non_nan_counts, alpha=0.6, label='Non-NaN NDVI values')\nplt.plot(dates, loess[:, 1], color='red', linewidth=2, linestyle='--', alpha=0.7, label='LOESS trend')\n\nplt.xlabel('Time')\nplt.ylabel('Number of non-NaN NDVI values')\nplt.title('Number of non-NaN NDVI values over time')\nplt.xticks(rotation=45)\nplt.grid(True)\n\n# Set x-axis to display quarterly ticks\nax = plt.gca()\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Set major ticks to every 3 months (quarterly)\nax.xaxis.set_major_formatter(mdates.DateFormatter('%m %Y'))  # Format the ticks to show the month and year\n\n# Set y-axis lower bound to 500,000\nplt.ylim(bottom=500000)\n\nplt.xticks(rotation=45)  # Rotate date labels for better readability\nplt.tight_layout()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n2.5.2.2 5.2 Perform Train and Test Split\nIn this step, we will split the time series data into separate train and test datasets for each cube.\n\n# Define Output directories\noutput_folder_train = base_dir / \"data\" / \"data_train\"\noutput_folder_test = base_dir / \"data\" / \"data_test\"\n\n\n# Perform Train-Test Split and Save in Separate Files\nfor idx, nc_file in enumerate(nc_files, start=1):\n    full_path = os.path.join(file_path, nc_file)\n    \n    # Load the NetCDF file\n    dataset = xr.open_dataset(full_path)\n    \n    # Train-Test Split\n    train_data = dataset.sel(time=slice('2017-07-04', '2021-06-28'))\n    test_data = dataset.sel(time=slice('2021-07-03', '2022-10-06'))\n    \n    # Generate the new filenames and full paths for the output files\n    base_filename = os.path.splitext(os.path.basename(nc_file))[0]  # Remove the .nc extension\n    train_filename = os.path.join(output_folder_train, f'{base_filename}_train.nc')\n    test_filename = os.path.join(output_folder_test, f'{base_filename}_test.nc')\n\n    # Save Train and Test Data\n    train_data.to_netcdf(train_filename)\n    test_data.to_netcdf(test_filename)\n \n    print(f\"[{idx}/{len(nc_files)}] Train and test datasets saved for {os.path.basename(nc_file)}\")\n\n\n\n\n2.5.3 6. Prepare Data for Final Training\nGiven the significant number of NaN values in our dataset and time series, we have decided to implement two distinct approaches to effectively handle these missing values.\nApproach A: Handling NaNs with Outliers and Cloud Mask Integration\n\nReplace NaNs with an Outlier:\n\nWe will replace NaN values with a clearly identifiable outlier value (e.g., -9999.0). This allows the model to recognize and handle these outliers during training.\n\nUse Cloud Mask as an Exogenous Feature:\n\nIn addition, we will incorporate the cloud mask as an exogenous feature in our model. This helps the model learn the relationship between cloud coverage (as indicated by the cloud mask for a specific pixel during a specific time period) and the outlier value (-9999.0). This way, the model can understand that the -9999.0 value is likely due to cloud presence, improving its ability to make accurate predictions despite these outliers.\n\n\nApproach B: Interpolation with STL Decomposition\n\nSTL Interpolation:\n\nIn this approach, we will use Seasonal-Trend decomposition using Loess (STL) to interpolate NaN values. STL decomposes the time series into seasonal, trend, and residual components, allowing for a more nuanced interpolation of missing values.\n\nWhy Use STL Interpolation:\n\nCaptures Seasonal and Trend Components:\n\nSTL is particularly effective for time series data with strong seasonal and trend components, which is the case for our NDVI data. By decomposing the data, STL can accurately interpolate missing values by considering the underlying seasonal patterns and long-term trends.\n\nRobust to Outliers:\n\nSTL is robust to outliers and can provide a more reliable interpolation compared to simpler methods, which might be biased by irregularities in the data.\n\nFlexible and Adaptable:\n\nSTL is flexible and can handle a wide range of time series characteristics, making it a versatile choice for our dataset with its complex patterns and missing values.\n\n\n\n\n2.5.3.1 6.1 Prepare data for Approach A:\n\n# Define input and output dirs\nfile_path = base_dir / \"data\" / \"data_train\"\noutput_path = base_dir / \"data\" / \"data_A_9999\"\n\n\nnc_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.nc')]\n\n# Iterate over each .nc file and replace NaNs in the NDVI variable with -9999\nfor idx, nc_file in enumerate(nc_files, start=1):\n    # Load the NetCDF file\n    dataset = xr.open_dataset(nc_file)\n    \n    # Replace NaNs in the NDVI variable with -9999\n    dataset['NDVI'] = dataset['NDVI'].fillna(-9999)\n    \n    # Generate the new filename and the full path for the output file\n    output_file = os.path.join(output_path, f'ds_A_{os.path.basename(nc_file)}')\n    \n    # Save the modified dataset\n    dataset.to_netcdf(output_file)\n    \n    print(f\"[{idx}/{len(nc_files)}] Cube: {os.path.basename(nc_file)} | NaNs replaced and saved\")\n\n\n\n2.5.3.2 6.2 Prepare data for Approach B:\nSTL Interpolation\n\n# Paths to the directory containing the NetCDF files\nfile_path = base_dir / \"data\" / \"data_train\"\noutput_path = base_dir / \"data\" / \"data_B_interpolated\"\nos.makedirs(output_path, exist_ok=True)\n\n\n# List all NetCDF files in the input directory\nnc_files = [f for f in os.listdir(file_path) if f.endswith('.nc')]\n\n# Iterate over each file\nfor nc_file in nc_files:\n    full_path = os.path.join(file_path, nc_file)\n    ds = xr.open_dataset(full_path)\n    \n    interpolated_data = []\n    \n    # Iterate over each pixel and apply STL interpolation\n    for x in ds.x:\n        for y in ds.y:\n            ndvi_pixel = ds['NDVI'].sel(x=x, y=y)\n            if ndvi_pixel.isnull().all():\n                # print(f\"Pixel (x={x}, y={y}) contains only NaNs and will be skipped.\")\n                interpolated_data.append(np.full(ndvi_pixel.shape, np.nan))  # Append NaN array\n            else:\n                interpolated_ndvi = stl_interpolate(ndvi_pixel)\n                interpolated_data.append(interpolated_ndvi.values)\n    \n    # Reshape the interpolated data to match the original dimensions\n    interpolated_ndvi_array = np.array(interpolated_data).reshape((ds.sizes['x'], ds.sizes['y'], ds.sizes['time'])).transpose(2, 0, 1)\n    \n    # Create a new DataArray with the interpolated NDVI values\n    interpolated_da = xr.DataArray(interpolated_ndvi_array, coords=[ds.time, ds.x, ds.y], dims=['time', 'x', 'y'], name='NDVI')\n    \n    # Create a new Dataset with the interpolated NDVI values and original attributes\n    new_ds = xr.Dataset({'NDVI': interpolated_da, 'Cloud_Mask': ds['Cloud_Mask']}, coords=ds.coords, attrs=ds.attrs)\n    \n    # Save the new dataset to a NetCDF file\n    output_file = os.path.join(output_path,  f'ds_B_{os.path.basename(nc_file)}')\n    new_ds.to_netcdf(output_file)\n    \n    print(f\"Interpolated file saved: {output_file}\")"
    },
    {
        "objectID": "Notebooks/1_Data_Preprocessing_Final.html#challenges-in-data-preparation",
        "href": "Notebooks/1_Data_Preprocessing_Final.html#challenges-in-data-preparation",
        "title": "2\u00a0 Overview",
        "section": "2.6 Challenges in Data Preparation",
        "text": "2.6 Challenges in Data Preparation\nThe data preparation process was extremely time-consuming and challenging. This notebook is the final result of various approaches, ideas, and steps. A significant portion of our work involved experimenting with different strategies which do not appear in this notebook as they did not work out. We encountered several key challenges:\n\nData Volume: Initially, we started with approximately 5000 cubes. However, executing some steps, such as the initial NDVI calculation based on Sentinel-2 data, was infeasible due to limited computing power, leading to frequent process failures. Consequently, we reduced the dataset to 100 cubes, selecting those with the fewest missing values. Despite this reduction, many steps, like interpolating missing values, were time-intensive and often failed due to memory constraints, prolonging our work.\nData Format: None of us had prior experience with DataCubes, requiring time to understand the data format and how it was stored and loaded (see src/data_processing/my_loader.py). The lack of proper documentation for the DataLoader and MiniCubes meant we had to rely heavily on trial and error to correctly load the data in the desired format.\nData Quality and Missing Data: The dataset contained many missing values, attributed to Sentinel-2 data periods with missing band values and pixels masked by the cloud mask. After deliberation, we decided to exclude the entire year 2016 due to its extensive missing values, despite metadata indicating the measurement period started on January 1, 2016. Further investigation revealed periodic intervals with few valid pixel values, detrimental to time series prediction and definetly will negatively impact our models\u2019 predictive performance. To address missing values, we tested two approaches:\n\nSetting Missing Values to an Outlier: Assigning missing values an extreme outlier value of -9999.0 (valid NDVI range: -1.0 to 1.0) and including the cloud mask as an exogenous factor to help the model learn the relationship between cloud presence and outliers. However the presence of clouds is not the only cause for missing values. They also occur without cloud indications.\nInterpolation of Missing Values: Using STL decomposition for interpolation, specifically suitable for our data. This method was computationally and time-intensive. To avoid backward fill interpolation and accurately interpolate missing values, we restricted the time period to start from a point with relatively many valid pixels (July 4, 2017). Additionally, we decided to interpolate missing values only in the training data and evaluate model performance using actual test data (predicted vs.\u00a0true test data), ignoring missing values in the test data to avoid distorting the evaluation metrics further."
    },
    {
        "objectID": "Notebooks/1_Data_Preprocessing_Final.html#challenges-in-modelling",
        "href": "Notebooks/1_Data_Preprocessing_Final.html#challenges-in-modelling",
        "title": "2\u00a0 Overview",
        "section": "2.7 Challenges in Modelling",
        "text": "2.7 Challenges in Modelling\nDuring the process of building the models, we encountered several problems.\n\nData for approach A\nProcedure for the LSTM model:\nAfter the model was created and the predictions were made, the data was denormalized. When looking at the results, we noticed that all prediction results were disproportionately high (&gt; 0.99). Due to the natural range of the NDVI from -1 to 1 and studying the baseline data, we concluded that the results are illogical and must be incorrect. Unfortunately, due to the limited time and the long processing times of the code, we were ultimately unable to determine the exact cause and find an appropriate solution. It is possible that the masking of the -9999 values did not work correctly, so that the -9999 values were regarded as \u201cnormal\u201d values by the model. Incorrect normalization could also have been a possible reason.\nDue to the comparability, we then decided to completely exclude this data set for all models from our project.\nReducing Cubes: complete cubes Furthermore, our initial aim was to use all 100 cubes for our project. This quickly proved to be impossible with the memory and computing power available to us. Despite the use of GPU, only very few cubes could be processed. We therefore had to reduce our data basis to 4 cubes. We sorted the 100 cubes in descending order of completeness. Complete means that the cube contains as few pixels as possible that contain NaN values across all timesteps. The 4 best cubes are therefore the following:\n\nCube 665\nCube 80\nCube 1203\nCube 1301 Beim Model Random Forest konnten wir letztendlich nur einen Cube nutzen, aufgrund der extrem langen Precessing time des models. Weitere Informationen sind im"
    },
    {
        "objectID": "Notebooks/2.1_SARIMA.html#description-and-overview",
        "href": "Notebooks/2.1_SARIMA.html#description-and-overview",
        "title": "3\u00a0 SARIMA Model",
        "section": "3.1 Description and Overview",
        "text": "3.1 Description and Overview\n1. Load Packages\n2. Define base directory\n3. Grid Search for Cube 655 (most complete cube) - Stationarity Check: Augmented Dickey Fuller Test - Differencing for non-stationary pixels - Grid search for p,q,P and Q to define most common parameter combination - Get most common parameter combination for cube 665\n4. Fit SARIMA model and make predictions - Stationarity Check: Augmented Dickey Fuller Test - Differencing for non-stationary pixels - Fit SARIMA model to each pixel in the four most complete cubes - Calculate training and test mean squared error (mse) and best akaike information criterion (aic) - Save fitted and predicted NDVI values to seperate .nc-files"
    },
    {
        "objectID": "Notebooks/2.1_SARIMA.html#load-packages",
        "href": "Notebooks/2.1_SARIMA.html#load-packages",
        "title": "3\u00a0 SARIMA Model",
        "section": "3.2 1. Load packages",
        "text": "3.2 1. Load packages\nFirst we import the necessary packages.\n\nimport itertools\nimport os\nimport random\nimport warnings\nfrom collections import Counter\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import adfuller"
    },
    {
        "objectID": "Notebooks/2.1_SARIMA.html#define-base-directory",
        "href": "Notebooks/2.1_SARIMA.html#define-base-directory",
        "title": "3\u00a0 SARIMA Model",
        "section": "3.3 2. Define base directory",
        "text": "3.3 2. Define base directory\n\n# Define base_dir for consistent path management\n\nnotebook_dir = Path(os.getcwd()).resolve()\n\nbase_dir = notebook_dir.parent\n\nprint(base_dir)\n\n/home/cgoehler/team-extra/ndvi-time-series-prediction"
    },
    {
        "objectID": "Notebooks/2.1_SARIMA.html#grid-search-for-cube-655-most-complete-cube",
        "href": "Notebooks/2.1_SARIMA.html#grid-search-for-cube-655-most-complete-cube",
        "title": "3\u00a0 SARIMA Model",
        "section": "3.4 3. Grid Search for Cube 655 (most complete cube)",
        "text": "3.4 3. Grid Search for Cube 655 (most complete cube)\nSelecting the best parameter combinations for a SARIMA model involves a systematic approach. Initially, the data must be checked for stationarity using tests like the Augmented Dickey-Fuller (ADF) test. Once stationarity is confirmed, a grid search is applied in order to identify suitable values for p, q, P and Q parameters. Due to limited computing power we apply the grid search only for 30 random pixels in the most complete cube (Cube 665).\nThe final model is selected based on the Akaike Information Criterion (AIC), which helps in identifying the model with the best fit by minimizing this value (Sim et al. 2022). The Training Mean Squared Error (MSE) is used to validate the effectiveness of the model, by minimizing it. This systematic approach ensures that the chosen SARIMA model effectively captures the underlying patterns in the NDVI time series data.\nTherefore we follow those steps: - Iterate over each of the 30 random pixel in the cube, ignoring pixels that only contain NaNs 1. Apply an Augmented Dickey Fuller Test & Differencing for d (0 or 1), D = 1 in order to check for stationarity and apply differencing 2. Grid Search for p, q, P & Q by applying different parameter combination to the SARIMA model - Get most common parameter combination for the 30 random pixel in cube 665\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Function to perform the Augmented Dickey-Fuller test\ndef adf_test(time_series):\n    \"\"\"\n    Perform the Augmented Dickey-Fuller test.\n\n    Parameters:\n    time_series (np.ndarray): The time series data to be tested.\n\n    Returns:\n    float: The ADF test statistic.\n    float: The p-value of the test.\n    \"\"\"\n    result = adfuller(time_series)\n    adf_statistic = result[0]\n    p_value = result[1]\n    return adf_statistic, p_value\n\n\n# Function to determine differencing orders\ndef determine_differencing(time_series, seasonal_period):\n    \"\"\"\n    Determine the differencing orders for a given time series.\n\n    Parameters:\n    time_series (xarray.DataArray): The time series data.\n    seasonal_period (int): The seasonal period for the time series.\n\n    Returns:\n    int: Order of non-seasonal differencing (d).\n    int: Order of seasonal differencing (D).\n    \"\"\"\n    time_series_values = time_series.values.flatten()\n    if len(time_series_values) &lt; 2:\n        raise ValueError(\"Time series must have more than one data point.\")\n    adf_statistic, p_value = adf_test(time_series_values)\n    d = 1 if p_value &gt;= 0.05 else 0\n    D = 1\n    return d, D\n\n\n# Function to fit SARIMAX model and return fitted model, training MSE, and AIC\ndef fit_sarimax_model(time_series, order, seasonal_order):\n    \"\"\"\n    Fit a SARIMAX model and return the fitted model, training MSE, and AIC.\n\n    Parameters:\n    time_series (np.ndarray): The time series data to fit.\n    order (tuple): The order of the SARIMAX model (p, d, q).\n    seasonal_order (tuple): The seasonal order of the SARIMAX model (P, D, Q, s).\n\n    Returns:\n    SARIMAXResults: The fitted SARIMAX model.\n    float: The mean squared error of the training data.\n    float: The AIC of the fitted model.\n    \"\"\"\n    model = SARIMAX(\n        time_series,\n        order=order,\n        seasonal_order=seasonal_order,\n        enforce_stationarity=False,\n        enforce_invertibility=False,\n    )\n    results = model.fit(disp=False)\n    fitted_values = results.fittedvalues\n    training_mse = mean_squared_error(time_series, fitted_values)\n    aic = results.aic\n    return results, training_mse, aic\n\n\n# Function to perform grid search\ndef grid_search_sarimax(\n    time_series, p_range, d_range, q_range, P_range, D_range, Q_range, S_range\n):\n    \"\"\"\n    Perform a grid search to find the best SARIMAX model parameters.\n\n    Parameters:\n    time_series (np.ndarray): The time series data to fit.\n    p_range (range): Range of p values to consider.\n    d_range (range): Range of d values to consider.\n    q_range (range): Range of q values to consider.\n    P_range (range): Range of P values to consider.\n    D_range (range): Range of D values to consider.\n    Q_range (range): Range of Q values to consider.\n    S_range (list): List of seasonal periods to consider.\n\n    Returns:\n    SARIMAXResults: The best fitted SARIMAX model.\n    tuple: The best parameter combination.\n    float: The mean squared error of the best model.\n    float: The AIC of the best model.\n    \"\"\"\n    best_aic = float(\"inf\")\n    best_mse = float(\"inf\")\n    best_params = None\n    best_model = None\n    for p, d, q, P, D, Q, S in itertools.product(\n        p_range, d_range, q_range, P_range, D_range, Q_range, S_range\n    ):\n        try:\n            order = (p, d, q)\n            seasonal_order = (P, D, Q, S)\n            model, training_mse, aic = fit_sarimax_model(\n                time_series, order, seasonal_order\n            )\n            if aic &lt; best_aic:\n                best_aic = aic\n                best_mse = training_mse\n                best_params = (order, seasonal_order)\n                best_model = model\n        except Exception as e:\n            print(f\"Error with parameters {order} and {seasonal_order}: {e}\")\n            continue\n    return best_model, best_params, best_mse, best_aic\n\n\n# Define the seasonal period\nseasonal_period = 73\n\n# Define parameter ranges for grid search\np_range = range(0, 2)\nq_range = range(0, 2)\nP_range = range(0, 2)\nQ_range = range(0, 2)\nS_range = [seasonal_period]\n\n# Generate 30 random pixel coordinates\ngrid_size = 128\nrandom_pixel_pairs = [\n    (random.randint(0, grid_size - 1), random.randint(0, grid_size - 1))\n    for _ in range(30)\n]\n\n# Specific file for Cube_665\ndata_folder_train = base_dir / \"data\" / \"data_train\"\ndata_folder_test = base_dir / \"data\" / \"data_test\"\nnc_file_train = \"ds_B_Cube_665_train.nc\"\nnc_file_test = \"Cube_665_test.nc\"\n\n# Open the specific dataset\nds_train = xr.open_dataset(os.path.join(data_folder_train, nc_file_train))\nndvi_data_train = ds_train[\"NDVI\"]\n\nds_test = xr.open_dataset(os.path.join(data_folder_test, nc_file_test))\nndvi_data_test = ds_test[\"NDVI\"]\n\n# Initialize arrays to store predictions and MSEs\npredictions = np.full((30, 93), np.nan)\ntrain_mse_array = np.full(30, np.nan)\ntest_mse_array = np.full(30, np.nan)\n\n# List to store parameter combinations\nparameter_combinations = []\n\n# Iterate over the selected pixels\nfor idx, (x, y) in enumerate(random_pixel_pairs):\n    try:\n        ndvi_pixel_train = ndvi_data_train.isel(x=x, y=y)\n        if np.all(np.isnan(ndvi_pixel_train)):\n            continue  # Skip pixels with only NaNs\n\n        d, D = determine_differencing(ndvi_pixel_train, seasonal_period)\n        d_range = [d]\n        D_range = [D]\n        best_model, best_params, best_mse, best_aic = grid_search_sarimax(\n            ndvi_pixel_train.values.flatten(),\n            p_range,\n            d_range,\n            q_range,\n            P_range,\n            D_range,\n            Q_range,\n            S_range,\n        )\n\n        # Append the best parameter combination\n        parameter_combinations.append(best_params)\n\n        # Forecast\n        forecast_steps = 93  # Fixed forecast steps to 93\n        forecast = best_model.get_forecast(steps=forecast_steps).predicted_mean\n\n        # Store predictions\n        predictions[idx, :] = forecast\n\n        # Calculate training MSE\n        train_mse_array[idx] = best_mse\n\n        # Test data for the same pixel\n        ndvi_pixel_test = ndvi_data_test.isel(x=x, y=y)\n        test_time_index = pd.date_range(\n            start=\"2021-07-03\", periods=len(ndvi_pixel_test.time), freq=\"5D\"\n        )\n        test_series = pd.Series(ndvi_pixel_test.values.flatten(), index=test_time_index)\n\n        # Create a time index for the forecast data\n        original_time_index = pd.date_range(\n            start=\"2017-07-04\", periods=len(ndvi_pixel_train.time), freq=\"5D\"\n        )\n        forecast_time_index = pd.date_range(\n            start=original_time_index[-1] + pd.DateOffset(days=5),\n            periods=forecast_steps,\n            freq=\"5D\",\n        )\n        forecast_series = pd.Series(forecast, index=forecast_time_index)\n\n        # Ensure the forecast and test data have the same time dimension\n        common_indices = forecast_series.index.intersection(test_series.index)\n\n        # Subset forecast and test series to only include the common indices\n        aligned_forecast_series = forecast_series.loc[common_indices]\n        aligned_test_series = test_series.loc[common_indices]\n\n        # Drop NaNs from both series\n        aligned_forecast_series = aligned_forecast_series.dropna()\n        aligned_test_series = aligned_test_series.dropna()\n\n        # Align the series again to ensure matching lengths after dropping NaNs\n        final_common_indices = aligned_forecast_series.index.intersection(\n            aligned_test_series.index\n        )\n        aligned_forecast_series = aligned_forecast_series.loc[final_common_indices]\n        aligned_test_series = aligned_test_series.loc[final_common_indices]\n\n        # Ensure lengths match after alignment\n        if len(aligned_forecast_series) != len(aligned_test_series):\n            raise ValueError(\n                \"Aligned forecast and test series have different lengths after dropping NaNs\"\n            )\n\n        # Calculate test MSE\n        test_mse = mean_squared_error(aligned_test_series, aligned_forecast_series)\n        test_mse_array[idx] = test_mse\n\n        # print(f\"Pixel ({x}, {y}): Best Params: {best_params}, Best Train MSE: {best_mse}, Best AIC: {best_aic}, Test MSE: {test_mse}\")\n    except Exception as e:\n        # print(f\"Failed to process pixel ({x}, {y}): {e}\")\n        continue\n\n# Find the most common parameter combination\nmost_common_params = Counter(parameter_combinations).most_common(1)[0]\nprint(\n    f\"The most common parameter combination is: {most_common_params[0]} with a count of {most_common_params[1]}\"\n)\n\nKeyboardInterrupt:"
    },
    {
        "objectID": "Notebooks/2.1_SARIMA.html#fit-sarima-model-and-make-predictions",
        "href": "Notebooks/2.1_SARIMA.html#fit-sarima-model-and-make-predictions",
        "title": "3\u00a0 SARIMA Model",
        "section": "3.5 4. Fit SARIMA model and make predictions",
        "text": "3.5 4. Fit SARIMA model and make predictions\nWe fit the SARIMA model to the NDVI time series of each pixel of the four most complete cubes. Predictions of the NDVI are made for the next 93 time steps (2021-07-03 to 2022-10-06). The model\u2019s performance is evaluated based on training and test mean squared error (MSEs). We iterate over all pixel in the selected four most complete cubes (I: Cube 665, II: Cube 80, III: Cube 1203, IV: Cube 1301).\nThe analysis includes the following steps: - Augmented Dickey Fuller (ADF) Test is carried out to check each pixel for stationarity. The function returns the ADF test statistics and the p-value. - After that the differencing order is determined, which is required to make the time series stationary based on the ADF test restults. - The non-seasonal differencing parameter d is set to 1 (for non-stationary pixel) or 0 (for stationary pixel) - The seasonal differencing paramter D is set to 1 for all pixel. Research has shown that the value of D is typically determined by the seasonality present in the data and can be set to 1 in order to effectively capture the seasonal patterns in the NDVI series (Silva et al. 2018). This aligns with the general approach in time series analysis, where (D = 1) is often sufficient to remove seasonality, especially when the data exhibits clear and repetitive seasonal cycles. - Fit the SARIMA Model with SARIMAX((p,d,q)(P,D,Q)(S)) - The SARIMAX() function of the statsmodels package fits a pixelwise SARIMA model to the NDVI time series of each pixel and returns the fitted model, training MSE and AIC - We predict the next 93 time steps (from 2021-07-03 to 2022-10-06) of the NDVI time series for each pixel in each of the cubes - Parameter combination of p, q, P and Q are predefined: p, q, P, Q = 1, 0, 0, 0 as this was the most common parameter combination in the grid search for the 30 random pixel in the most complete cube 665 - Explanation of the different parameters: - p=1: the model uses value of the previous time step to predict the current value - d=0: no differencing is applied (the time series is stationary), d=1 differencing is applied (the time series is non-stationary) - q=0: the model uses no previous forecast error to predict the current value - P=0: no seasonal autoregressive terms are used - D=1: the model includes one seasonal differencing component - Q=0: no seasonal moving average terms are used - S=73: in our data with a frequence of 5 days we have 73 time steps a year - The training MSE (between training and fitted data), the best training AIC and the test MSE (between predicted and test data) are calculated - The fitted values together with the training MSE and best training AIC are saved in a separate netcdf file ('/data/data_fitted/SARIMA_fitted_Cube_XY.nc') per cube - The predicted values together with the test MSE are saved in a separate netcdf file ('/data/data_predictions/SARIMA_predicted_Cube_XY.nc') per cube\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# definition of functions for Augmented Dickey Fuller Test, Differencing, Fitting SARIMAX model\ndef adf_test(time_series):\n    \"\"\"\n    Perform the Augmented Dickey-Fuller test.\n\n    Parameters:\n    time_series (np.ndarray): The time series data to be tested.\n\n    Returns:\n    float: The ADF test statistic.\n    float: The p-value of the test.\n    \"\"\"\n    result = adfuller(time_series)\n    adf_statistic = result[0]\n    p_value = result[1]\n    return adf_statistic, p_value\n\n\ndef determine_differencing(time_series, seasonal_period):\n    \"\"\"\n    Determine the differencing orders for a given time series.\n\n    Parameters:\n    time_series (xarray.DataArray): The time series data.\n    seasonal_period (int): The seasonal period for the time series.\n\n    Returns:\n    int: Order of non-seasonal differencing (d).\n    int: Order of seasonal differencing (D).\n    \"\"\"\n    time_series_values = time_series.values.flatten()\n    if len(time_series_values) &lt; 2:\n        raise ValueError(\"Time series must have more than one data point.\")\n    adf_statistic, p_value = adf_test(time_series_values)\n    if p_value &gt;= 0.05:\n        d = 1\n    else:\n        d = 0\n    D = 1\n    return d, D\n\n\ndef fit_sarimax_model(time_series, order, seasonal_order):\n    \"\"\"\n    Fit a SARIMAX model and return the fitted model, training MSE, and AIC.\n\n    Parameters:\n    time_series (np.ndarray): The time series data to fit.\n    order (tuple): The order of the SARIMAX model (p, d, q).\n    seasonal_order (tuple): The seasonal order of the SARIMAX model (P, D, Q, s).\n\n    Returns:\n    SARIMAXResults: The fitted SARIMAX model.\n    float: The mean squared error of the training data.\n    float: The AIC of the fitted model.\n    \"\"\"\n    model = SARIMAX(\n        time_series,\n        order=order,\n        seasonal_order=seasonal_order,\n        enforce_stationarity=False,\n        enforce_invertibility=False,\n    )\n    results = model.fit(disp=False)\n    fitted_values = results.fittedvalues\n    training_mse = mean_squared_error(time_series, fitted_values)\n    aic = results.aic\n    return results, training_mse, aic\n\n\n# Define the seasonal period\nseasonal_period = 73\n\n# Define fixed SARIMAX parameters\np, q, P, Q = 1, 0, 0, 0\n\n# Folder with 100 NetCDF datasets\ndata_folder_train = base_dir / \"data\" / \"data_train\"\ndata_folder_test = base_dir / \"data\" / \"data_test\"\nnc_files_train = [f for f in os.listdir(data_folder_train) if f.endswith(\".nc\")]\nnc_files_test = [f for f in os.listdir(data_folder_test) if f.endswith(\".nc\")]\n\n# Ensure that the test files correspond to the training files\nif len(nc_files_train) != len(nc_files_test):\n    raise ValueError(\n        \"The number of training files does not match the number of test files\"\n    )\n\n# Iterate over all NetCDF files\nfor nc_file_train, nc_file_test in zip(nc_files_train, nc_files_test):\n    print(f\"Processing file {nc_file_train}\")\n    ds_train = xr.open_dataset(os.path.join(data_folder_train, nc_file_train))\n    ndvi_data_train = ds_train[\"NDVI\"]\n\n    ds_test = xr.open_dataset(os.path.join(data_folder_test, nc_file_test))\n    ndvi_data_test = ds_test[\"NDVI\"]\n\n    # Initialize arrays to store predictions, fitted values, MSEs, and AICs\n    predictions = np.full(\n        (ndvi_data_test.sizes[\"x\"], ndvi_data_test.sizes[\"y\"], 93), np.nan\n    )\n    fitted_values_array = np.full(\n        (\n            ndvi_data_train.sizes[\"x\"],\n            ndvi_data_train.sizes[\"y\"],\n            len(ndvi_data_train.time),\n        ),\n        np.nan,\n    )\n    train_mse_array = np.full(\n        (ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan\n    )\n    train_aic_array = np.full(\n        (ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan\n    )\n    test_mse_array = np.full(\n        (ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan\n    )\n    d_array = np.full((ndvi_data_train.sizes[\"x\"], ndvi_data_train.sizes[\"y\"]), np.nan)\n\n    # Iterate over all pixels in the dataset\n    for x in range(ndvi_data_train.sizes[\"x\"]):\n        for y in range(ndvi_data_train.sizes[\"y\"]):\n            try:\n                ndvi_pixel_train = ndvi_data_train.isel(x=x, y=y)\n                if np.all(np.isnan(ndvi_pixel_train)):\n                    continue  # Skip pixels with only NaNs\n\n                d, D = determine_differencing(ndvi_pixel_train, seasonal_period)\n                order = (p, d, q)\n                seasonal_order = (P, D, Q, seasonal_period)\n\n                model, training_mse, aic = fit_sarimax_model(\n                    ndvi_pixel_train.values.flatten(), order, seasonal_order\n                )\n\n                # Store d\n                d_array[x, y] = d\n\n                # Store fitted values\n                fitted_values_array[x, y, :] = model.fittedvalues\n\n                # Store training MSE and AIC\n                train_mse_array[x, y] = training_mse\n                train_aic_array[x, y] = aic\n\n                # Forecast\n                forecast_steps = 93  # Fixed forecast steps to 93\n                forecast = model.get_forecast(steps=forecast_steps).predicted_mean\n\n                # Store predictions\n                predictions[x, y, :] = forecast\n\n                # Test data for the same pixel\n                ndvi_pixel_test = ndvi_data_test.isel(x=x, y=y)\n                test_time_index = pd.date_range(\n                    start=\"2021-07-03\", periods=len(ndvi_pixel_test.time), freq=\"5D\"\n                )\n                test_series = pd.Series(\n                    ndvi_pixel_test.values.flatten(), index=test_time_index\n                )\n\n                # Create a time index for the forecast data\n                original_time_index = pd.date_range(\n                    start=\"2017-07-04\", periods=len(ndvi_pixel_train.time), freq=\"5D\"\n                )\n                forecast_time_index = pd.date_range(\n                    start=original_time_index[-1] + pd.DateOffset(days=5),\n                    periods=forecast_steps,\n                    freq=\"5D\",\n                )\n                forecast_series = pd.Series(forecast, index=forecast_time_index)\n\n                # Ensure the forecast and test data have the same time dimension\n                common_indices = forecast_series.index.intersection(test_series.index)\n\n                # Subset forecast and test series to only include the common indices\n                aligned_forecast_series = forecast_series.loc[common_indices]\n                aligned_test_series = test_series.loc[common_indices]\n\n                # Drop NaNs from both series\n                aligned_forecast_series = aligned_forecast_series.dropna()\n                aligned_test_series = aligned_test_series.dropna()\n\n                # Align the series again to ensure matching lengths after dropping NaNs\n                final_common_indices = aligned_forecast_series.index.intersection(\n                    aligned_test_series.index\n                )\n                aligned_forecast_series = aligned_forecast_series.loc[\n                    final_common_indices\n                ]\n                aligned_test_series = aligned_test_series.loc[final_common_indices]\n\n                # Ensure lengths match after alignment\n                if len(aligned_forecast_series) != len(aligned_test_series):\n                    raise ValueError(\n                        \"Aligned forecast and test series have different lengths after dropping NaNs\"\n                    )\n\n                # Calculate test MSE\n                test_mse = mean_squared_error(\n                    aligned_test_series, aligned_forecast_series\n                )\n                test_mse_array[x, y] = test_mse\n\n                # print(f\"  Pixel ({x}, {y}): d={d}, D={D}, Training MSE: {training_mse}, AIC: {aic}, Test MSE: {test_mse}\")\n            except Exception as e:\n                print(\n                    f\"  Failed to process pixel ({x}, {y}) in file {nc_file_train}: {e}\"\n                )\n                continue\n\n    # Ensure output directories exist\n    fitted_output_dir = base_dir / \"data\" / \"data_fitted\"\n    predicted_output_dir = base_dir / \"data\" / \"data_predictions\" \n    os.makedirs(fitted_output_dir, exist_ok=True)\n    os.makedirs(predicted_output_dir, exist_ok=True)\n\n    # Save fitted values and training MSEs/AICs to a new NetCDF file\n    fitted_output_ds = xr.Dataset(\n        {\n            \"fitted_values\": ([\"x\", \"y\", \"time\"], fitted_values_array),\n            \"train_mse\": ([\"x\", \"y\"], train_mse_array),\n            \"train_aic\": ([\"x\", \"y\"], train_aic_array),\n            \"d\": ([\"x\", \"y\"], d_array),\n        },\n        coords={\n            \"time\": ndvi_data_train.coords[\"time\"],\n            \"x\": ndvi_data_train.coords[\"x\"],\n            \"y\": ndvi_data_train.coords[\"y\"],\n        },\n    )\n    fitted_output_file = os.path.join(\n        fitted_output_dir, f'SARIMA_fitted_{nc_file_train.split(\".\")[0]}.nc'\n    )\n    fitted_output_ds.to_netcdf(fitted_output_file)\n    print(f\"Fitted values saved to '{fitted_output_file}'\")\n\n    # Save predictions and test MSEs to a new NetCDF file\n    prediction_output_ds = xr.Dataset(\n        {\n            \"predictions\": ([\"x\", \"y\", \"time\"], predictions),\n            \"test_mse\": ([\"x\", \"y\"], test_mse_array),\n        },\n        coords={\n            \"time\": pd.date_range(start=\"2021-07-03\", periods=93, freq=\"5D\"),\n            \"x\": ndvi_data_train.coords[\"x\"],\n            \"y\": ndvi_data_train.coords[\"y\"],\n        },\n    )\n    prediction_output_file = os.path.join(\n        predicted_output_dir, f'SARIMA_predicted_{nc_file_train.split(\".\")[0]}.nc'\n    )\n    prediction_output_ds.to_netcdf(prediction_output_file)\n    print(f\"Predictions saved to '{prediction_output_file}'\")\n\nProcessing file ds_B_Cube_665_train.nc\n\n\nKeyboardInterrupt: \n\n\n\n\n\n\nAzad, Abdus Samad, Rajalingam Sokkalingam, Hanita Daud, Sajal Kumar Adhikary, Hifsa Khurshid, Siti Nur Athirah Mazlan, and Muhammad Babar Ali Rabbani. 2022. \u201cWater Level Prediction Through Hybrid SARIMA and ANN Models Based on Time Series Analysis: Red Hills Reservoir Case Study.\u201d Sustainability (Switzerland) 14 (February). https://doi.org/10.3390/su14031843.\n\n\nSilva, Claudionor Ribeiro da, S\u00e9rgio Lu\u00eds Dias Machado, Aracy Alves de Ara\u00fajo, and Carlos Alberto Matias de Abreu Junior. 2018. \u201cAnalysis of the Phenology Dynamics of Brazilian Caatinga Species with NDVI Time Series.\u201d Cerne 24 (January): 48\u201358. https://doi.org/10.1590/01047760201824012487.\n\n\nSim, Ho Jen, Choo Wei Chong, Khairil Anwar Abu Kassim, Ching Siew Mooi, and Zhang Yuruixian. 2022. \u201cForecasting Road Traffic Fatalities in Malaysia Using Seasonal Autoregressive Integrated Moving Average (SARIMA) Model.\u201d Pertanika Journal of Science and Technology 30 (April): 897\u2013911. https://doi.org/10.47836/pjst.30.2.03."
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html",
        "href": "Notebooks/2.2_LSTM.html",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "",
        "text": "5 LSTM Model"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#overview",
        "href": "Notebooks/2.2_LSTM.html#overview",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "4.1 Overview",
        "text": "4.1 Overview\n\nLoading the necessary packages\nDefining the base directory\nInitially, a parameter search is conducted using GridSearch (0. Gridsearch). During this process, data for a cube is loaded and prepared to enable the model to determine the optimal parameters.\nSubsequently, the code for the final LSTM model is executed:\n\nLoad and normalize the data for all utilized cubes\nCreate sequences for the model\nPrepare the data for model training: flatten, reshape into the correct dimensions, combine data from all cubes, and mask out NaN values\nConstruct the model\nTrain the model\nGenerate predictions for the test dataset\nDenormalize the predictions\nSave the predictions"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#requirements",
        "href": "Notebooks/2.2_LSTM.html#requirements",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "4.2 Requirements",
        "text": "4.2 Requirements\nThe notebook was build using GPU ressources and the Kernel Python 3.9 TensorFlow 2.6.6 CUDA.\n\n4.2.1 Load packages\n\nimport os\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Masking, Dropout, Input, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import ParameterGrid\nfrom tensorflow.keras.models import load_model\nfrom pathlib import Path\nimport glob\nimport netCDF4 as nc\n\n\n\n4.2.2 Define base directory\n\n# Define base_dir for consistent path management\nnotebook_dir = Path(os.getcwd()).resolve()\nbase_dir = notebook_dir\nprint(base_dir)\n\n/home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#gridsearch",
        "href": "Notebooks/2.2_LSTM.html#gridsearch",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "4.3 0. GridSearch",
        "text": "4.3 0. GridSearch\nTo find the best parameters for the model with the given data, we performed a Gridsearch. Unfortunately, the computational capacity was insufficient to conduct the Gridsearch for all 4 cubes. Thus, we only used one cube as data basis (Cube 665). In the following, the data of the cube is loaded and prepared for the model. It was then tested with all parameter combinations. The best parameters are presented as output.\n\ndef load_data(train_file, test_file):\n    \"\"\"\n    Load and extract NDVI data from .nc files.\n\n    Parameters:\n    train_file (str): Path to the training data file.\n    test_file (str): Path to the testing data file.\n\n    Returns:\n    tuple: Training and test NDVI data.\n    \"\"\"\n    ds_train = xr.open_dataset(train_file)\n    ndvi_train = ds_train['NDVI'].values\n    ds_test = xr.open_dataset(test_file)\n    ndvi_test = ds_test['NDVI'].values\n    return ndvi_train, ndvi_test\n\ndef prepare_data(ndvi_train, ndvi_test, sequence_length, pred_length):\n    \"\"\"\n    Normalize and prepare data for LSTM model training.\n\n    Parameters:\n    ndvi_train (np.ndarray): Training NDVI data.\n    ndvi_test (np.ndarray): Test NDVI data.\n    sequence_length (int): Length of input sequences.\n    pred_length (int): Length of prediction sequences.\n\n    Returns:\n    tuple: Prepared training and test data, input shape, and output units.\n    \"\"\"\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    ndvi_train_normalized = scaler.fit_transform(ndvi_train.reshape(-1, 1)).reshape(ndvi_train.shape)\n    ndvi_test_normalized = scaler.transform(ndvi_test.reshape(-1, 1)).reshape(ndvi_test.shape)\n\n    def create_sequences(data, seq_length, pred_length):\n        X, y = [], []\n        for i in range(len(data) - seq_length - pred_length + 1):\n            X.append(data[i:i + seq_length])\n            y.append(data[i + seq_length:i + seq_length + pred_length])\n        return np.array(X), np.array(y)\n\n    X_train, Y_train = create_sequences(ndvi_train_normalized, sequence_length, pred_length)\n    X_test, Y_test = create_sequences(ndvi_test_normalized, sequence_length, pred_length)\n\n    X_train_flattened = X_train.reshape(X_train.shape[0], X_train.shape[1], -1)\n    Y_train_flattened = Y_train.reshape(Y_train.shape[0], Y_train.shape[1], -1)\n    X_test_flattened = X_test.reshape(X_test.shape[0], X_test.shape[1], -1)\n    Y_test_flattened = Y_test.reshape(Y_test.shape[0], Y_test.shape[1], -1)\n\n    X_train_flattened = np.nan_to_num(X_train_flattened, nan=0)\n    Y_train_flattened = np.nan_to_num(Y_train_flattened, nan=0)\n    X_test_flattened = np.nan_to_num(X_test_flattened, nan=0)\n    Y_test_flattened = np.nan_to_num(Y_test_flattened, nan=0)\n\n    input_shape = (X_train_flattened.shape[1], X_train_flattened.shape[2])\n    output_units = Y_train_flattened.shape[1] * Y_train_flattened.shape[2]\n\n    return X_train_flattened, Y_train_flattened, X_test_flattened, Y_test_flattened, input_shape, output_units\n\ndef build_model(input_shape, output_units, params):\n    \"\"\"\n    Build and compile the LSTM model.\n\n    Parameters:\n    input_shape (tuple): Shape of the input data.\n    output_units (int): Number of output units.\n    params (dict): Dictionary containing model parameters.\n\n    Returns:\n    Sequential: Compiled LSTM model.\n    \"\"\"\n    lstm_units = params['lstm_units']\n    dropout_rate = params['dropout_rate']\n    learning_rate = params['learning_rate']\n    optimizer_name = params['optimizer']\n    use_batch_norm = params['use_batch_norm']\n\n    if optimizer_name == 'adam':\n        optimizer = Adam(learning_rate=learning_rate)\n    elif optimizer_name == 'rmsprop':\n        optimizer = RMSprop(learning_rate=learning_rate)\n    else:\n        optimizer = SGD(learning_rate=learning_rate)\n\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    model.add(Masking(mask_value=0))\n    model.add(LSTM(lstm_units, return_sequences=True))\n    if use_batch_norm:\n        model.add(BatchNormalization())\n    model.add(Dropout(dropout_rate))\n    model.add(LSTM(lstm_units))\n    if use_batch_norm:\n        model.add(BatchNormalization())\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(output_units))\n    model.compile(optimizer=optimizer, loss='mse')\n    return model\n\ndef train_and_evaluate(params, input_shape, output_units, X_train, Y_train, X_test, Y_test):\n    \"\"\"\n    Train and evaluate the LSTM model based on provided parameters.\n\n    Parameters:\n    params (dict): Dictionary containing model parameters.\n    input_shape (tuple): Shape of the input data.\n    output_units (int): Number of output units.\n    X_train (np.ndarray): Training input data.\n    Y_train (np.ndarray): Training target data.\n    X_test (np.ndarray): Test input data.\n    Y_test (np.ndarray): Test target data.\n\n    Returns:\n    dict: Training result including parameters and loss.\n    \"\"\"\n    model = build_model(input_shape, output_units, params)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], -1), epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.2, callbacks=[early_stopping], verbose=0)\n    loss = model.evaluate(X_test, Y_test.reshape(Y_test.shape[0], -1), verbose=0)\n    \n    result = {\n        'lstm_units': params['lstm_units'],\n        'dropout_rate': params['dropout_rate'],\n        'batch_size': params['batch_size'],\n        'epochs': params['epochs'],\n        'learning_rate': params['learning_rate'],\n        'optimizer': params['optimizer'],\n        'use_batch_norm': params['use_batch_norm'],\n        'loss': loss\n    }\n\n    return result\n\n# Paths to training and test data\ntrain_file = base_dir / 'data/data_interpolated/ds_B_Cube_665_train.nc'\ntest_file = base_dir / 'data/data_test/Cube_665_test.nc'\n\n# Example sequence and prediction lengths\nsequence_length = 70\npred_length = 23\n\n# Load and prepare data\nndvi_train, ndvi_test = load_data(train_file, test_file)\nX_train, Y_train, X_test, Y_test, input_shape, output_units = prepare_data(ndvi_train, ndvi_test, sequence_length, pred_length)\n\n# Define the parameter grid\nparam_grid = {\n    'lstm_units': [50, 100],\n    'dropout_rate': [0.2, 0.3],\n    'batch_size': [16, 32],\n    'epochs': [30, 50],\n    'learning_rate': [0.001, 0.01],\n    'optimizer': ['adam', 'rmsprop'],\n    'use_batch_norm': [True, False]\n}\n\n# Initialize results list\nresults = []\n\n# Create grid\ngrid = ParameterGrid(param_grid)\n\n# Loop through all parameter combinations\nfor params in grid:\n    result = train_and_evaluate(params, input_shape, output_units, X_train, Y_train, X_test, Y_test)\n    results.append(result)\n\n# Create DataFrame from results\nresults_df = pd.DataFrame(results)\nprint(results_df)\n\n# Find the best parameter combination\nbest_params = results_df.loc[results_df['loss'].idxmin()]\n\nprint(\"Best Parameters:\")\nprint(best_params)\n\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_665_train.nc with shape: (292, 128, 128)\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1203_train.nc with shape: (292, 128, 128)\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_80_train.nc with shape: (292, 128, 128)\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1301_train.nc with shape: (292, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1203_test.nc with shape: (93, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_80_test.nc with shape: (93, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1301_test.nc with shape: (93, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_665_test.nc with shape: (93, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized test data shape: (93, 128, 128)\nNormalized test data shape: (93, 128, 128)\nNormalized test data shape: (93, 128, 128)\nNormalized test data shape: (93, 128, 128)\n\n\nThe result is a list with the best values for each parameter examined. These are used for the final model."
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#load-and-normalize-the-data",
        "href": "Notebooks/2.2_LSTM.html#load-and-normalize-the-data",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "5.1 1. Load and normalize the data",
        "text": "5.1 1. Load and normalize the data\nFirst, all Files from the defined directory are loaded.\nData needs to be normalized for an LSTM to ensure that the network converges faster during training. This avoids issues related to vanishing or exploding gradients. Normalization also helps to ensure that all input features contribute equally to the learning process to improve the model\u2019s performance and stability.\n\ndef load_nc_file(file_path):\n    \"\"\"\n    Load a single .nc file.\n\n    Parameters:\n    file_path (str): Path to the .nc file.\n\n    Returns:\n    xarray.Dataset: Loaded dataset.\n    \"\"\"\n    return xr.open_dataset(file_path)\n\ndef extract_ndvi(ds):\n    \"\"\"\n    Extract NDVI data from the dataset.\n\n    Parameters:\n    ds (xarray.Dataset): Dataset containing NDVI data.\n\n    Returns:\n    np.ndarray: Extracted NDVI values.\n    \"\"\"\n    return ds['NDVI'].values\n\n# Example: Path to the data\ntrain_dir = base_dir / 'data/data_interpolated/'\ntest_dir = base_dir / 'data/data_test/'\n\n# List of training and test files\ntrain_files = glob.glob(str(train_dir / '*.nc'))\ntest_files = glob.glob(str(test_dir / '*.nc'))\n\n# Initialize training and test data lists\nndvi_train_list = []\nndvi_test_list = []\n\n# Load training data\nfor file in train_files:\n    ds = load_nc_file(file)\n    ndvi_train = extract_ndvi(ds)\n    ndvi_train_list.append(ndvi_train)\n    print(f\"Loaded train file: {file} with shape: {ndvi_train.shape}\")\n\n# Load test data\nfor file in test_files:\n    ds = load_nc_file(file)\n    ndvi_test = extract_ndvi(ds)\n    ndvi_test_list.append(ndvi_test)\n    print(f\"Loaded test file: {file} with shape: {ndvi_test.shape}\")\n\ndef normalize_data(ndvi_data):\n    \"\"\"\n    Normalize the NDVI data using MinMaxScaler.\n\n    Parameters:\n    ndvi_data (np.ndarray): NDVI data to be normalized.\n\n    Returns:\n    np.ndarray: Normalized NDVI data.\n    MinMaxScaler: Scaler used for normalization.\n    \"\"\"\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    flattened_data = ndvi_data.reshape(-1, 1)\n    normalized_data = scaler.fit_transform(flattened_data).reshape(ndvi_data.shape)\n    return normalized_data, scaler\n\n# Normalize training and test data\nndvi_train_normalized_list = []\nndvi_test_normalized_list = []\nscalers = []  # List of scalers for each file\n\nfor ndvi_train in ndvi_train_list:\n    normalized_data, scaler = normalize_data(ndvi_train)\n    ndvi_train_normalized_list.append(normalized_data)\n    scalers.append(scaler)\n    print(f\"Normalized train data shape: {normalized_data.shape}\")\n\nfor ndvi_test in ndvi_test_list:\n    normalized_data, scaler = normalize_data(ndvi_test)\n    ndvi_test_normalized_list.append(normalized_data)\n    print(f\"Normalized test data shape: {normalized_data.shape}\")\n\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_665_train.nc with shape: (292, 128, 128)\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1203_train.nc with shape: (292, 128, 128)\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_80_train.nc with shape: (292, 128, 128)\nLoaded train file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_interpolated/ds_B_Cube_1301_train.nc with shape: (292, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1203_test.nc with shape: (93, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_80_test.nc with shape: (93, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_1301_test.nc with shape: (93, 128, 128)\nLoaded test file: /home/sc.uni-leipzig.de/kt501gqiy/_LIM_lectures/2024_SoSe/data/data_test/Cube_665_test.nc with shape: (93, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized train data shape: (292, 128, 128)\nNormalized test data shape: (93, 128, 128)\nNormalized test data shape: (93, 128, 128)\nNormalized test data shape: (93, 128, 128)\nNormalized test data shape: (93, 128, 128)"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#data-preparation",
        "href": "Notebooks/2.2_LSTM.html#data-preparation",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "5.2 2. Data preparation",
        "text": "5.2 2. Data preparation\nThe following is carried out here: - Creation of the sequences that the model needs for training: For seasonal data, the sequence length should typically capture at least one full seasonal cycle. This allows the model to learn the repeating patterns and trends effectively. Thus, we decided to set the sequence length to 70 (365 days divided by 5 days temporal resolution = 73). - Flattening the data and dividing it into sequences: Flattening and reshaping ensure the data is compatible with LSTM input requirements: - The variable X should be a 3D array of shape (num_samples, time_steps, num_features). - num_samples: Number of training samples. - time_steps: Length of the sequence to be fed into the LSTM. - num_features: Number of features at each time step. - The variable Y should be a 2D array of shape (num_samples, num_targets). - num_targets: Number of targets to predict - Combination of the data from all cubes - Masking nan values: LSTMs cannot handle NaN values because they disrupt the computations required for training. They can lead to invalid loss values and prevent the network from learning properly. - Reshape the combined data arrays to the required dimensions\n\nsequence_length = 70\npred_length = 23\n\ndef create_sequences(data, seq_length, pred_length):\n    \"\"\"\n    Create sequences and targets for the model.\n\n    Parameters:\n    data (np.ndarray): Input data for sequence creation.\n    seq_length (int): Length of the input sequences.\n    pred_length (int): Length of the prediction sequences.\n\n    Returns:\n    np.ndarray: Array of input sequences.\n    np.ndarray: Array of target sequences.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data) - seq_length - pred_length + 1):\n        X.append(data[i:i + seq_length])\n        y.append(data[i + seq_length:i + seq_length + pred_length])\n    return np.array(X), np.array(y)\n\ndef flatten_data(data):\n    \"\"\"\n    Flattens the height and width dimensions into a single feature dimension.\n    \n    Args:\n        data (numpy.ndarray): The data to be flattened.\n\n    Returns:\n        numpy.ndarray: The flattened data.\n    \"\"\"\n    return data.reshape(data.shape[0], data.shape[1], -1)\n\ndef prepare_data(ndvi_data_list, sequence_length, pred_length):\n    \"\"\"\n    Prepares the data for model training or testing with sequences.\n    \n    Args:\n        ndvi_data_list (list): List of NDVI data arrays.\n        sequence_length (int): The length of the sequence.\n        pred_length (int): The length of the prediction.\n\n    Returns:\n        tuple: Flattened input and target sequences.\n    \"\"\"\n    X_list, Y_list = [], []\n    for ndvi_data in ndvi_data_list:\n        X, Y = create_sequences(ndvi_data, sequence_length, pred_length)\n        X_list.append(flatten_data(X))\n        Y_list.append(flatten_data(Y))\n    return X_list, Y_list\n\ndef combine_data(X_list, Y_list):\n    \"\"\"\n    Combines and flattens the list of data arrays into single arrays.\n    \n    Args:\n        X_list (list): List of input sequences.\n        Y_list (list): List of target sequences.\n\n    Returns:\n        tuple: Combined and flattened input and target sequences.\n    \"\"\"\n    X_combined = np.concatenate(X_list, axis=0)\n    Y_combined = np.concatenate(Y_list, axis=0)\n    return X_combined, Y_combined\n\ndef check_nan(data, name):\n    \"\"\"\n    Checks for NaN values in the data.\n    \n    Args:\n        data (numpy.ndarray): The data to be checked.\n        name (str): The name of the data for printing.\n\n    Returns:\n        None\n    \"\"\"\n    nan_in_data = np.any(np.isnan(data))\n    print(f\"NaN in {name}: {nan_in_data}\")\n\ndef reshape_combined_data(X, Y, sequence_length):\n    \"\"\"\n    Reshapes the combined data arrays to the required dimensions.\n    \n    Args:\n        X (numpy.ndarray): Combined input sequences.\n        Y (numpy.ndarray): Combined target sequences.\n        sequence_length (int): The length of the sequence.\n\n    Returns:\n        tuple: Reshaped input and target sequences.\n    \"\"\"\n    X = X.reshape((X.shape[0], sequence_length, -1))\n    Y = Y.reshape((Y.shape[0], -1))\n    return X, Y\n\n# Prepare training data\nX_train_list, Y_train_list = prepare_data(ndvi_train_normalized_list, sequence_length, pred_length)\n\n# Prepare testing data\nX_test_list, Y_test_list = prepare_data(ndvi_test_normalized_list, sequence_length, pred_length)\n\n# Combine training data\nX_train_combined, Y_train_combined = combine_data(X_train_list, Y_train_list)\n\n# Combine testing data\nX_test_combined, Y_test_combined = combine_data(X_test_list, Y_test_list)\n\n# Replace NaNs with zeros\nX_train_combined = np.nan_to_num(X_train_combined, nan=0)\nY_train_combined = np.nan_to_num(Y_train_combined, nan=0)\nX_test_combined = np.nan_to_num(X_test_combined, nan=0)\nY_test_combined = np.nan_to_num(Y_test_combined, nan=0)\n\n# Check for NaNs in the data\ncheck_nan(X_train_combined, \"X_train_combined\")\ncheck_nan(Y_train_combined, \"Y_train_combined\")\ncheck_nan(X_test_combined, \"X_test_combined\")\ncheck_nan(Y_test_combined, \"Y_test_combined\")\n\n# Reshape combined data\nX_train_combined, Y_train_combined = reshape_combined_data(X_train_combined, Y_train_combined, sequence_length)\nX_test_combined, Y_test_combined = reshape_combined_data(X_test_combined, Y_test_combined, sequence_length)\n\n# Print final shapes of combined data\nprint(f\"Final X_train_combined shape: {X_train_combined.shape}\")\nprint(f\"Final Y_train_combined shape: {Y_train_combined.shape}\")\nprint(f\"Final X_test_combined shape: {X_test_combined.shape}\")\nprint(f\"Final Y_test_combined shape: {Y_test_combined.shape}\")\n\nNaN in X_train_combined: False\nNaN in Y_train_combined: False\nNaN in X_test_combined: False\nNaN in Y_test_combined: False\nFinal X_train_combined shape: (800, 70, 16384)\nFinal Y_train_combined shape: (800, 376832)\nFinal X_test_combined shape: (4, 70, 16384)\nFinal Y_test_combined shape: (4, 376832)"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#create-model",
        "href": "Notebooks/2.2_LSTM.html#create-model",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "5.3 3. Create Model",
        "text": "5.3 3. Create Model\nHere the model is created and compiled. For the model are used four cubes (see 1_Data_Preprocessing). After GridSearch, the best hyperparameters are set to the model. A callback for Early stopping is also added: Early stopping is a regularization technique used during training of neural networks to prevent overfitting\n\n# Hyperparameters\nlstm_units = 50\ndropout_rate = 0.2\nbatch_size = 32\nepochs = 50\nlearning_rate = 0.001\noptimizer = 'rmsprop'\nuse_batch_norm = True\n\n# Define input and output shapes\ninput_shape = (X_train_combined.shape[1], X_train_combined.shape[2])  # (n_steps, num_pixels * num_features)\noutput_units = Y_train_combined.shape[1]  # num_pixels * num_features\n\n# Model creation\ndef create_lstm_model(input_shape, output_units, lstm_units, dropout_rate, use_batch_norm, learning_rate):\n    \"\"\"\n    Creates and compiles an LSTM model with the specified parameters.\n    \n    Args:\n        input_shape (tuple): Shape of the input data (n_steps, num_pixels * num_features).\n        output_units (int): Number of output units (num_pixels * num_features).\n        lstm_units (int): Number of LSTM units.\n        dropout_rate (float): Dropout rate for regularization.\n        use_batch_norm (bool): Whether to use batch normalization.\n        learning_rate (float): Learning rate for the optimizer.\n\n    Returns:\n        model (Sequential): Compiled LSTM model.\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    model.add(Masking(mask_value=0))\n    model.add(LSTM(lstm_units, return_sequences=True))\n    if use_batch_norm:\n        model.add(BatchNormalization())\n    model.add(Dropout(dropout_rate))\n    model.add(LSTM(lstm_units))\n    if use_batch_norm:\n        model.add(BatchNormalization())\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(output_units))\n\n    # Create optimizer instance\n    optimizer_instance = RMSprop(learning_rate=learning_rate)\n\n    # Compile model\n    model.compile(optimizer=optimizer_instance, loss='mse')\n\n    return model\n\n# Create the LSTM model\nmodel = create_lstm_model(input_shape, output_units, lstm_units, dropout_rate, use_batch_norm, learning_rate)\n\n# Print the model summary\nmodel.summary()\n\n# Callback for Early Stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n2024-07-28 15:33:01.781971: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-07-28 15:33:01.982418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -&gt; device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:05:00.0, compute capability: 7.0\n\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmasking (Masking)            (None, 70, 16384)         0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 70, 50)            3287000   \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 70, 50)            200       \n_________________________________________________________________\ndropout (Dropout)            (None, 70, 50)            0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 50)                20200     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 50)                200       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense (Dense)                (None, 376832)            19218432  \n=================================================================\nTotal params: 22,526,032\nTrainable params: 22,525,832\nNon-trainable params: 200\n_________________________________________________________________"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#model-training",
        "href": "Notebooks/2.2_LSTM.html#model-training",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "5.4 4. Model training",
        "text": "5.4 4. Model training\nThe code trains the LSTM model using the combined training data (X_train_combined and Y_train_combined) with early stopping to halt training if performance on the validation split (20% of the training data) stops improving. After training, it evaluates the model\u2019s performance and prints the test loss.\n\n# Train the model\nhistory = model.fit(X_train_combined, Y_train_combined, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n\n# Evaluation\nloss = model.evaluate(X_test_combined, Y_test_combined, verbose=1)\nprint(f\"Test loss: {loss}\")\n\n# Save the model\n# model.save('LSTM.h5')\n\n2024-07-28 15:33:18.306972: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nEpoch 1/50\n\n\n2024-07-28 15:33:23.339418: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8401\n\n\n20/20 [==============================] - 10s 204ms/step - loss: 0.3286 - val_loss: 0.3497\nEpoch 2/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2917 - val_loss: 0.3134\nEpoch 3/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2443 - val_loss: 0.2575\nEpoch 4/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1949 - val_loss: 0.2546\nEpoch 5/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1454 - val_loss: 0.1960\nEpoch 6/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.0958 - val_loss: 0.1884\nEpoch 7/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.0634 - val_loss: 0.1391\nEpoch 8/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.0467 - val_loss: 0.1035\nEpoch 9/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.0398 - val_loss: 0.0580\nEpoch 10/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.0356 - val_loss: 0.0411\nEpoch 11/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.0343 - val_loss: 0.0530\nEpoch 12/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.0312 - val_loss: 0.0555\nEpoch 13/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.0337 - val_loss: 0.0460\nEpoch 14/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.0347 - val_loss: 0.0447\nEpoch 15/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.0313 - val_loss: 0.0450\n1/1 [==============================] - 2s 2s/step - loss: 0.0513\nTest loss: 0.051280610263347626"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#prediction-and-denormalization",
        "href": "Notebooks/2.2_LSTM.html#prediction-and-denormalization",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "5.5 5. Prediction and Denormalization",
        "text": "5.5 5. Prediction and Denormalization\nAfter the prediction is made, the results are denormalized using the inverse transformation of the scaler to revert them back to their original scale. This ensures that the test data and the model\u2019s predictions are in the same scale as the original dataset for accurate comparison.\n\n# Load the model\n# model = load_model('LSTM.h5')\n\n# Make predictions\npredictions = model.predict(X_test_combined)\n\n# Denormalize the predictions\ndef denormalize_data(scaler, normalized_data):\n    return scaler.inverse_transform(normalized_data.reshape(-1, 1)).reshape(normalized_data.shape)\n\n# Ensuring that the test data and the predictions are transformed back to their original scale\nlast_scaler = scalers[-1]\nY_test_denormalized = denormalize_data(last_scaler, Y_test_combined)\npredictions_denormalized = denormalize_data(last_scaler, predictions)"
    },
    {
        "objectID": "Notebooks/2.2_LSTM.html#save-predictions",
        "href": "Notebooks/2.2_LSTM.html#save-predictions",
        "title": "4\u00a0 Long Short-Term Memory (LSTM)",
        "section": "5.6 6. Save predictions",
        "text": "5.6 6. Save predictions\nHere, the dimensions and time steps are configured. The mean squared error (MSE) between true and predicted values is calculated. Finally, the reshaped predicted values along with the MSE were saved into NetCDF files.\n\n# Configuration\ntime_steps = 23\nx_dim = 128\ny_dim = 128\n\noutput_dir = 'data/test_predictions'\n\n# Generate time stamps\ntime_stamps = pd.date_range(start='2021-07-03', periods=time_steps, freq='5D')\n\n# Mapping of file names to test samples\nfile_to_sample = {}\n\n# Names of test files corresponding to the predictions\ntest_file_names = [os.path.basename(f) for f in test_files]\n\n# Reshape combined data back to the original form\nY_test_combined = Y_test_combined.reshape((Y_test_combined.shape[0], time_steps, x_dim, y_dim))\n\n# Calculate MSE values\nmse_list = []\nfor i in range(len(test_files)):\n    true_values = Y_test_combined[i].reshape(-1)\n    predicted_values = predictions_denormalized[i].reshape(-1)\n    mse = mean_squared_error(true_values, predicted_values)\n    mse_list.append(mse)\n\n# Extract test samples and map to file names\nfor i, file_name in enumerate(test_file_names):\n    sample = predictions_denormalized[i].reshape(time_steps, x_dim, y_dim)\n    mse = mse_list[i]  # MSE value for the current file\n    \n    # Create output file path\n    output_file = os.path.join(output_dir, 'LSTM_predicted_' + file_name)\n    \n    with nc.Dataset(output_file, 'w', format='NETCDF4') as ds:\n        ds.createDimension('time', time_steps)\n        ds.createDimension('x', x_dim)\n        ds.createDimension('y', y_dim)\n        \n        # Save time coordinate as Datetime64\n        time = ds.createVariable('time', 'f8', ('time',))\n        time.units = 'days since 1970-01-01 00:00:00'\n        time.calendar = 'gregorian'\n        time[:] = nc.date2num(time_stamps.to_pydatetime(), units=time.units, calendar=time.calendar)\n        \n        # Save X and Y coordinates as integers\n        x = ds.createVariable('x', 'i4', ('x',))\n        y = ds.createVariable('y', 'i4', ('y',))\n        ndvi = ds.createVariable('NDVI', 'f4', ('time', 'x', 'y'))\n        mse_var = ds.createVariable('MSE', 'f4')  # Variable for the MSE value\n        \n        x[:] = np.arange(x_dim)\n        y[:] = np.arange(y_dim)\n        \n        ndvi[:, :, :] = sample\n        mse_var.assignValue(mse)  # Save MSE value\n        \n    # Save mapping\n    file_to_sample[file_name] = output_file\n\n# Check mappings and MSE values\nprint(file_to_sample)\nprint(\"MSE values:\", mse_list)\n\n{'Cube_1203_test.nc': 'data/test_predictions/LSTM_predicted_Cube_1203_test.nc', 'Cube_80_test.nc': 'data/test_predictions/LSTM_predicted_Cube_80_test.nc', 'Cube_1301_test.nc': 'data/test_predictions/LSTM_predicted_Cube_1301_test.nc', 'Cube_665_test.nc': 'data/test_predictions/LSTM_predicted_Cube_665_test.nc'}\nMSE values: [0.092216894, 0.20980994, 0.29330707, 0.13620757]\n\n\n\n\n\n\nGuo, Yan, Lifeng Zhang, Yi He, Shengpeng Cao, Hongzhe Li, Ling Ran, Yujie Ding, and Mikalai Filonchyk. 2024. \u201cLSTM Time Series NDVI Prediction Method Incorporating Climate Elements: A Case Study of Yellow River Basin, China.\u201d Journal of Hydrology 629 (February): 130518. https://doi.org/10.1016/j.jhydrol.2023.130518.\n\n\nReddy, D. Sushma, and P. Rama Chandra Prasad. 2018. \u201cPrediction of Vegetation Dynamics Using NDVI Time Series Data and LSTM.\u201d Modeling Earth Systems and Environment 4 (April): 409\u201319. https://doi.org/10.1007/s40808-018-0431-3.\n\n\nRhif, Manel, Ali Ben Abbes, Beatriz Martinez, and Imed Riadh Farah. 2020. \u201cDeep Learning Models Performance for NDVI Time Series Prediction: A Case Study on North West Tunisia.\u201d In 2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS). IEEE. https://doi.org/10.1109/m2garss47143.2020.9105149.\n\n\nSherif, Khaled, Mohamed Azmy, Khder Alakkari, Mostafa Abotaleb, and El-Sayed M. El-Kenawy. 2023. \u201cDeep Learning in IoT: An LSTM Approach for NDVI Forecasting.\u201d In 2023 3rd International Conference on Electronic Engineering (ICEEM). IEEE. https://doi.org/10.1109/iceem58740.2023.10319616.\n\n\nVasilakos, Christos, George E. Tsekouras, and Dimitris Kavroudakis. 2022. \u201cLSTM-Based Prediction of Mediterranean Vegetation Dynamics Using NDVI Time-Series Data.\u201d Land 11 (6): 923. https://doi.org/10.3390/land11060923."
    },
    {
        "objectID": "Notebooks/2.3_Random_Forest.html",
        "href": "Notebooks/2.3_Random_Forest.html",
        "title": "5\u00a0 Random Forest Model",
        "section": "",
        "text": "ensemble learning method for classification and regression\nconstructs decision trees at training time\nfor classification: output is class selected by most trees\nfor regression: mean or average prediction of individual trees\ntrees tend to overfit training sets - low bias but high variance\nrandom forests to average over multiple decision trees trained on different parts of the same training set -&gt; reduces variance (but small increase in bias and some loss of interpretability)\nfew hundred to several thousand trees necessary depending on size of training set\n\nhere only 100 trees due to processing time\n\n\n\n5.0.1 Overview\n\nimport packages\nload data and define variables\ntrain model and make prediction for testing period\n\ninput to model\n\nlags = previous x time steps to predict the next step\nn_estimators: number of trees\n\n\ncalcualte MSE for predicted and testing data\nsave data (predicted NDVI and MSE) to netCDF file\n\n\nimport xarray as xr\nimport numpy as np\nimport pandas as pd\nfrom darts import TimeSeries\nfrom darts.models import RandomForest\nfrom sklearn.metrics import mean_squared_error\nimport netCDF4 as nc\nimport matplotlib.pyplot as plt\n\n\n# Function to load data from NetCDF file and defining variables\ndef load_nc_file(file_path):\n    ds = xr.open_dataset(file_path)\n    ndvi = ds['NDVI']\n    times = ds['time']\n    x = ds['x']\n    y = ds['y']\n    return ndvi, times, x, y\n\n# turn data into darts TimeSeries\ndef prepare_darts_timeseries(ndvi_data, times):\n    series_list = []\n    for i in range(ndvi_data.shape[1]):  # iterate over x dimension\n        for j in range(ndvi_data.shape[2]):  # iterate over y dimension\n            values = ndvi_data[:, i, j]\n            # replace nan values by zeros (assuming only pixels with just NaNs exist)\n            values = np.nan_to_num(values, nan=0.0)\n            time_index = pd.to_datetime(times, unit='s')\n            series = TimeSeries.from_times_and_values(time_index, values)\n            series_list.append(series)\n    return series_list\n\n# preparing data by turning them into darts TimeSeries\ndef prediction_series(train_ndvi_data, train_times, test_times):\n    # Train Random Forest model\n    model = RandomForest(\n            lags=25,\n            n_estimators=100)\n    pred_series = []\n    for i in range(train_ndvi_data.shape[1]):  # iterate over x dimension\n        print(f'{i}/{train_ndvi_data.shape[1]}', end='\\r')\n        for j in range(train_ndvi_data.shape[2]):  # iterate over y dimension\n            values = train_ndvi_data[:, i, j]\n            # replace NaN values by zeros (assuming they only exist in pixels that are completely NaN)\n            values = np.nan_to_num(values, nan=0.0)\n            time_index = pd.to_datetime(train_times, unit='s')\n            series = TimeSeries.from_times_and_values(time_index, values)\n            # train model on training series\n            model.fit(series)\n            # predict using random forest model\n            pred = model.predict(n=len(test_times))\n            pred_series.append(pred)\n    return pred_series\n\n# Save predictions and MSE to a new NetCDF file\ndef save_to_nc_file(output_file, pred_data, mse_data, times, x, y):\n    with nc.Dataset(output_file, 'w', format='NETCDF4') as ds:\n        ds.createDimension('time', len(times))\n        ds.createDimension('x', len(x))\n        ds.createDimension('y', len(y))\n        \n        time_var = ds.createVariable('time', 'f4', ('time',))\n        x_var = ds.createVariable('x', 'f4', ('x',))\n        y_var = ds.createVariable('y', 'f4', ('y',))\n        pred_var = ds.createVariable('pred_ndvi', 'f4', ('time', 'x', 'y'))\n        mse_var = ds.createVariable('mse', 'f4', ('x', 'y'))\n        \n        time_var[:] = times\n        x_var[:] = x\n        y_var[:] = y\n        \n        pred_ndvi = np.array([pred.values().flatten() for pred in pred_data]).reshape((len(times), len(x), len(y)))\n        pred_var[:] = pred_ndvi\n        \n        mse_var[:] = np.array(mse_list).reshape((len(x), len(y)))\n\n\n# Load training and testing data\ntrain_ndvi, train_times, train_x, train_y = load_nc_file(train_path+'ds_B_Cube_665_train.nc')\ntest_ndvi, test_times, test_x, test_y = load_nc_file(test_path+'Cube_665_test.nc')\n\n\n# make prediction\npred_series = prediction_series(train_ndvi, train_times, test_times)\nprint(\"done with prediction\")\n\ndone with prediction\n\n\n\n# turn testing data into darts TimeSeries\ntest_series = prepare_darts_timeseries(test_ndvi, test_times)\n\n\n# Calculate MSE for each pixel between prediction and testing data\nmse_list = []\nfor pred, actual in zip(pred_series, test_series):\n    mask = np.isfinite(actual.values().flatten())\n    mse = mean_squared_error(actual.values().flatten()[mask], pred.values().flatten()[mask])\n    mse_list.append(mse)\nprint(\"done with mse\")\n\n# Save the data (prediction and MSE)\nsave_to_nc_file('/home/the-patrician42/team-extra/ndvi-time-series-prediction/data/data_predictions/'\n                +'Random_Forest_Cube_665.nc', pred_series, mse_list, test_times, test_x, test_y)\nprint(\"done saving\")\n\ndone with mse\ndone saving"
    },
    {
        "objectID": "Notebooks/3_Evaluation.html",
        "href": "Notebooks/3_Evaluation.html",
        "title": "6\u00a0 Load packages and define base directory",
        "section": "",
        "text": "7 Analysing prediction results and comparing models\nAs mentioned in the Data Preprocessing chapter (Challenges in Modelling) the criterion for selection of cubes was the completenes of the time series. The four cubes with the fewest measurement and time gaps were chosen (I: Cube 665, II: Cube 80, III: Cube 1203, IV: Cube 1301). For comparison we plot the NDVI time series original data with the prediction results of the different models for one example pixel per cube. The example pixel is chosen by the minimum test MSE of the SARIMA model for each cube, as this model seems to capture the course of the NDVI time series the best. The MSE is calculated with:\n\\(MSE = \\frac{1}{n} \\sum_{i=1}^{n}(Y_i - \\bar Y_i)^2\\)\nWith - \\(Y_i\\) \u2026 observed value - \\(\\bar Y_i\\) \u2026 predicted value\n# Function to load data from NetCDF file and defining variables\ndef load_nc_file(file_path, ndvi_var_name, x_dim_name, y_dim_name, mse_name=\"\"):\n    ds = xr.open_dataset(file_path)\n    try:\n        ndvi = ds[ndvi_var_name]\n        times = ds['time']\n        x = ds[x_dim_name] # lat for lstm\n        y = ds[y_dim_name] # lon for lstm\n        mse = ds[mse_name]\n        return ndvi, times, x, y, mse\n    except:\n        ndvi = ds[ndvi_var_name]\n        times = ds['time']\n        x = ds[x_dim_name] # lat for lstm\n        y = ds[y_dim_name] # lon for lstm\n        return ndvi, times, x, y\n# Function to determine the pixel with the lowest test mse (SARIMA) in each cube\ndef find_min_test_mse(datasets, cube_names):\n    \"\"\"\n    Find and print the pixel with the lowest test MSE in each dataset.\n    \n    Parameters:\n    datasets (list of str): List of paths to the NetCDF datasets.\n    cube_names (list of str): List of names for each cube corresponding to the datasets.\n    \"\"\"\n    for i, dataset_path in enumerate(datasets):\n        pred = xr.open_dataset(dataset_path)\n        test_mse_values = pred['test_mse'].values\n        min_test_mse_value = np.nanmin(test_mse_values)\n        min_test_mse_index = np.unravel_index(np.nanargmin(test_mse_values), test_mse_values.shape)\n        min_test_mse_coords = (pred['x'][min_test_mse_index[0]].item(), pred['y'][min_test_mse_index[1]].item())\n\n        print(f\"The minimum test MSE for {cube_names[i]} is: {min_test_mse_value} for pixel ({min_test_mse_coords})\")\n\n# Paths to the datasets\ndatasets = [\n    base_dir / \"data\" / \"data_predictions\" / f'SARIMA_predicted_ds_B_Cube_{cube}_train.nc'\n    for cube in [665, 80, 1203, 1301]\n]\n\n# Names of the cubes\ncube_names = [\n    'Cube 665',\n    'Cube 80',\n    'Cube 1203',\n    'Cube 1301'\n]\n\n# Call the function\nfind_min_test_mse(datasets, cube_names)\n\nThe minimum test MSE for Cube 665 is: 0.0006814400117864885 for pixel ((93, 32))\nThe minimum test MSE for Cube 80 is: 0.003545171194203701 for pixel ((46, 84))\nThe minimum test MSE for Cube 1203 is: 0.0006025049006614665 for pixel ((49, 42))\nThe minimum test MSE for Cube 1301 is: 0.0058360315822941905 for pixel ((122, 106))\n# cubes considered\ncubes = [665, 80, 1203, 1301]\n# pixel with best MSE for SARIMA for each cube\nbest_xy = [[93, 32], [46, 84], [49, 42], [122, 106] ]\n\nlstm_cube_mse_list = []\nsarima_cube_mse_list = []\nlstm_pixel_mse_list = []\nsarima_pixel_mse_list = []\nfor i, cube in enumerate(cubes):\n    # directory to each cube\n    directory = base_dir / \"data\"\n\n    # file names of training and testing data as well as predictions by LSTM and SARIMA model\n    train_nc = directory / f'data_train/ds_B_Cube_{cube}_train.nc'\n    test_nc = directory / f'data_test/Cube_{cube}_test.nc'\n    lstm_nc = directory / f'data_predictions/LSTM_predicted_Cube_{cube}_test.nc'\n    sarima_nc = directory / f'data_predictions/SARIMA_predicted_ds_B_Cube_{cube}_train.nc'\n    if cube == 665:\n        rf_nc = directory / 'data_predictions/Random_Forest_Cube_665.nc'\n        rf_ndvi, rf_times, rf_x, rf_y, rf_mse = load_nc_file(rf_nc, 'pred_ndvi', 'x', 'y', 'mse')\n        rf_ndvi.where(rf_ndvi==0.0)\n\n    # import datasets and define variables\n    train_ndvi, train_times, train_x, train_y = load_nc_file(train_nc, 'NDVI', 'x', 'y')\n    test_ndvi, test_times, test_x, test_y = load_nc_file(test_nc, 'NDVI', 'x', 'y')\n    lstm_ndvi, lstm_times, lstm_x, lstm_y, lstm_mse = load_nc_file(lstm_nc, 'NDVI', 'x', 'y', 'MSE')\n    sarima_ndvi, sarima_times, sarima_x, sarima_y, sarima_mse = load_nc_file(sarima_nc, 'predictions', 'x', 'y', 'test_mse')\n\n    x = best_xy[i][0]\n    y = best_xy[i][1]\n\n    # plot time series of training, testing and model data\n    fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n\n\n    if cube == 665:\n        plt.plot(train_times, train_ndvi[:,x,y],\n             label='interpolated train NDVI')\n        plt.plot(test_times, test_ndvi[:,x,y],\n             label='test NDVI')\n        plt.plot(lstm_times, lstm_ndvi[:,x,y],\n             label='LSTM NDVI')\n        plt.plot(sarima_times, sarima_ndvi[x,y,:],\n             label='SARIMA NDVI')\n        rf_ndvi[:,x,y][np.where(rf_ndvi[:,x,y]==0.)] = np.nan\n        plt.plot(test_times, rf_ndvi[:,x,y],\n                 label='Random Forest NDVI')\n        plt.legend()\n\n    else:\n        plt.plot(train_times, train_ndvi[:,x,y],\n             label='interpolated train NDVI')\n        plt.plot(test_times, test_ndvi[:,x,y],\n             label='test NDVI')\n        plt.plot(lstm_times, lstm_ndvi[:,x,y],\n             label='LSTM NDVI')\n        plt.plot(sarima_times, sarima_ndvi[x,y,:],\n             label='SARIMA NDVI')\n        plt.legend()\n\n\n    # calculate pixel MSE for LSTM\n    mask = np.isfinite(test_ndvi[:,x,y])\n    mask = mask[0:len(lstm_times)]\n    lstm_pixel_mse = mean_squared_error(test_ndvi[:,x,y][0:len(lstm_times)][mask],\n                                        lstm_ndvi[:,x,y][mask])\n    print(f'Cube {cube} \\nPixel x = {x}, y = {y}')\n    print(f'Pixel MSE from SARIMA: {sarima_mse[x,y].values}')\n    print(f'Cube MSE from SARIMA: {sarima_mse.mean().values}')\n    print(f'Pixel MSE from LSTM: {lstm_pixel_mse}')\n    print(f'Cube MSE from LSTM: {lstm_mse.values}')\n    if cube == 665:\n        print(f'Pixel MSE from Random Forest: {rf_mse[x,y].values}')\n        print(f'Cube MSE from Random Forest: {rf_mse.mean().values}')\n\n    # append cube MSE\n    lstm_cube_mse_list.append(lstm_mse)\n    sarima_cube_mse_list.append(sarima_mse.mean().values)\n    # append pixel MSE\n    lstm_pixel_mse_list.append(lstm_pixel_mse)\n    sarima_pixel_mse_list.append(sarima_mse[x,y].values)\n\n    plt.show()\n\nCube 665 \nPixel x = 93, y = 32\nPixel MSE from SARIMA: 0.0006814400117864885\nCube MSE from SARIMA: 0.007087799911625777\nPixel MSE from LSTM: 0.0004515084729064256\nCube MSE from LSTM: 0.13620756566524506\nPixel MSE from Random Forest: 0.005258710123598576\nCube MSE from Random Forest: 0.01674204133450985\n\n\n\n\n\nCube 80 \nPixel x = 46, y = 84\nPixel MSE from SARIMA: 0.003545171194203701\nCube MSE from SARIMA: 0.05865153656459794\nPixel MSE from LSTM: 0.1886572241783142\nCube MSE from LSTM: 0.20980994403362274\n\n\n\n\n\nCube 1203 \nPixel x = 49, y = 42\nPixel MSE from SARIMA: 0.0006025049006614665\nCube MSE from SARIMA: 0.059179499826998834\nPixel MSE from LSTM: 0.0010537380585446954\nCube MSE from LSTM: 0.09221689403057098\n\n\n\n\n\nCube 1301 \nPixel x = 122, y = 106\nPixel MSE from SARIMA: 0.0058360315822941905\nCube MSE from SARIMA: 0.06264543209138485\nPixel MSE from LSTM: 0.005719645880162716\nCube MSE from LSTM: 0.2933070659637451\nFor model evaluation we choose the test Mean Squared Error (MSE), calcuated as the difference between the test and predicted data. The MSE is a widely recognized metric for evaluating the performance of predictive models (Arumugam and Natarajan 2023). We compare the mean test MSE over all pixels of a cube in a bar plot for all models.\n# Positions of the bars on the x-axis\nbar_width = 0.3\nindices = np.arange(len(cubes))\nrf_index = 0\n\n# Create the bar plot\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\nax1 = plt.subplot(121)\n# Bar positions for each model\nbar1 = ax1.bar(indices, lstm_cube_mse_list, bar_width, label='LSTM')\nbar3 = ax1.bar(indices + bar_width, sarima_cube_mse_list, bar_width, label='SARIMA')\n# Adding the single MSE value as an additional bar\nax1.bar(rf_index + 2 * bar_width, rf_mse.mean().values, bar_width, label='Random Forest', color='m')\n\n# Adding labels, title, and legend\nax1.set_xlabel('Cubes',\n              fontsize=16,\n              fontweight='semibold')\nax1.set_ylabel('MSE',\n              fontsize=16,\n              fontweight='semibold')\nax1.set_xticks(indices + bar_width /2)\nax1.set_xticklabels(cubes)\nax1.set_title('Cube MSE',\n              fontsize=16,\n              fontweight='semibold')\nax1.legend()\n\nax2 = plt.subplot(122)\n\n# Bar positions for each model\nbar2 = ax2.bar(indices, lstm_pixel_mse_list, bar_width, label='LSTM')\nbar4 = ax2.bar(indices + bar_width, sarima_pixel_mse_list, bar_width, label='SARIMA')\n# Adding the single MSE value as an additional bar\nax2.bar(rf_index + 2 * bar_width, rf_mse[x,y].values, bar_width, label='Random Forest', color='m')\n\n# Adding labels, title, and legend\nax2.set_xlabel('Cubes',\n              fontsize=16,\n              fontweight='semibold')\nax2.set_ylabel('MSE',\n              fontsize=16,\n              fontweight='semibold')\nax2.set_xticks(indices + bar_width / 2)\nax2.set_xticklabels(cubes)\nax2.set_title('Pixel MSE',\n              fontsize=16,\n              fontweight='semibold')\nax2.legend()\n\n# Display the plot\nplt.show()"
    },
    {
        "objectID": "Notebooks/3_Evaluation.html#description-of-model-performance-based-on-example-pixels-plot-visual-comparison",
        "href": "Notebooks/3_Evaluation.html#description-of-model-performance-based-on-example-pixels-plot-visual-comparison",
        "title": "6\u00a0 Load packages and define base directory",
        "section": "7.1 Description of model performance based on example pixels plot (visual comparison)",
        "text": "7.1 Description of model performance based on example pixels plot (visual comparison)\nThe prediction results for each model differ a lot.\nThe SARIMA model capture best the seasonality and trend of the NDVI time series for the 93 prediction steps. For the example pixels of cube 665 and 1203 the seasonality of the test data is followed by the SARIMA model predictions. For the example pixel of cube 80 the SARIMA model predictions fail in predicting the NDVI amplitude in the last months of 2021, but in general are able to follow the seasonality of the data. The SARIMA model predictions follow the NDVI time series for the prediction steps in the example pixel of cube 1301 besides the test data artefact around the turn of the year from 2021 to 2022.\nThe Random Forest NDVI predictions in cube 665 show strong fluctuations and higher deviations from the test NDVI data then the other models, indicating less accurate predictions. Due to processing time it was not able to run a Random Forest model for the other three cubes.\nThe LSTM model only predicted a short term period (23 time steps) for the NDVI time series. For the example pixels in cube 665 and 1301 the LSTM model predictions follow the course of the test data for the first 23 time steps. For the example pixels of cube 80 and 1203 the LSTM model predictions fail in predicting the"
    },
    {
        "objectID": "Notebooks/3_Evaluation.html#description-of-model-performance-based-on-mean-test-mse-per-cube-and-model",
        "href": "Notebooks/3_Evaluation.html#description-of-model-performance-based-on-mean-test-mse-per-cube-and-model",
        "title": "6\u00a0 Load packages and define base directory",
        "section": "8.1 Description of model performance based on mean test MSE per cube and model",
        "text": "8.1 Description of model performance based on mean test MSE per cube and model\nThe test MSE of the LSTM model is higher for all cubes than the test MSE of the SARIMA model. That means that the SARIMA model performed better in predictions for all cubes than the LSTM model.\nSARIMA performed best in predicting the NDVI time series of cube 665 with a test MSE of ~0.0071. The test MSE for cubes 80, 1203 and 1301 is varying around ~0.06 (with ~0.0587, ~0.0592 and ~0.0627 respectively).\nLSTM performed best in predicting the NDVI time series of cube 1203 with a test MST of ~0.0922. The test MSE for cubes 665, 80 and 1301 is varying between 0.14 and 0.3 (with ~0.1362, ~0.2098 and ~0.2933 respectively)."
    },
    {
        "objectID": "Notebooks/3_Evaluation.html#description-of-the-models-performance-based-on-pixelwise-test-mse-per-example-pixel-of-each-cube-and-model",
        "href": "Notebooks/3_Evaluation.html#description-of-the-models-performance-based-on-pixelwise-test-mse-per-example-pixel-of-each-cube-and-model",
        "title": "6\u00a0 Load packages and define base directory",
        "section": "8.2 Description of the models performance based on pixelwise test MSE per example pixel of each cube and model",
        "text": "8.2 Description of the models performance based on pixelwise test MSE per example pixel of each cube and model\n\nadd description\n\n\nSchwierigkeiten bei Vergleich LSTM nur f\u00fcr viel k\u00fcrzeren Zeitraum verf\u00fcgbare predictions (Modell Konfiguration) RF nur f\u00fcr einen cube predictions verf\u00fcgbar\n\n\nbeantwortung der research question & final conclusion compare three different models for calculating ndvi predictions from time series data.\n\n\n\n\n\nArumugam, Vignesh, and Vijayalakshmi Natarajan. 2023. \u201cTime Series Modeling and Forecasting Using Autoregressive Integrated Moving Average and Seasonal Autoregressive Integrated Moving Average Models.\u201d Instrumentation Mesure Metrologie 22 (August): 161\u201368. https://doi.org/10.18280/i2m.220404."
    },
    {
        "objectID": "references.html",
        "href": "references.html",
        "title": "References",
        "section": "",
        "text": "Arumugam, Vignesh, and Vijayalakshmi Natarajan. 2023. \u201cTime Series\nModeling and Forecasting Using Autoregressive Integrated Moving Average\nand Seasonal Autoregressive Integrated Moving Average Models.\u201d\nInstrumentation Mesure Metrologie 22 (August): 161\u201368. https://doi.org/10.18280/i2m.220404.\n\n\nAzad, Abdus Samad, Rajalingam Sokkalingam, Hanita Daud, Sajal Kumar\nAdhikary, Hifsa Khurshid, Siti Nur Athirah Mazlan, and Muhammad Babar\nAli Rabbani. 2022. \u201cWater Level Prediction Through Hybrid SARIMA\nand ANN Models Based on Time Series Analysis: Red Hills Reservoir Case\nStudy.\u201d Sustainability (Switzerland) 14 (February). https://doi.org/10.3390/su14031843.\n\n\nBreiman, Leo. 2001. \u201cRandom Forests.\u201d Machine\nLearning 45: 5\u201332.\n\n\nCardinale, Bradley J., J. Emmett Duffy, Andrew Gonzalez, David U.\nHooper, Charles Perrings, Patrick Venail, Anita Narwani, et al. 2012.\n\u201cBiodiversity Loss and Its Impact on Humanity.\u201d\nNature 486 (June): 59\u201367. https://doi.org/10.1038/nature11148.\n\n\nCavalli, Stefano, Gabriele Penzotti, Michele Amoretti, Stefano Caselli,\net al. 2021. \u201cA Machine Learning Approach for NDVI Forecasting\nBased on Sentinel-2 Data.\u201d In ICSOFT, 473\u201380.\n\n\nHiggins, Steven I., Timo Conradi, and Edward Muhoko. 2023. \u201cShifts\nin Vegetation Activity of Terrestrial Ecosystems Attributable to Climate\nTrends.\u201d Nature Geoscience 16 (2): 147\u201353. https://doi.org/10.1038/s41561-022-01114-x.\n\n\nHuang, Sha, Lina Tang, Joseph P. Hupy, Yang Wang, and Guofan Shao. 2020.\n\u201cA Commentary Review on the Use of Normalized Difference\nVegetation Index (NDVI) in the Era of Popular Remote Sensing.\u201d\nJournal of Forestry Research 32 (1): 1\u20136. https://doi.org/10.1007/s11676-020-01155-1.\n\n\nMahecha, Miguel D., Fabian Gans, Gunnar Brandt, Rune Christiansen, Sarah\nE. Cornell, Normann Fomferra, Guido Kraemer, et al. 2020. \u201cEarth\nSystem Data Cubes Unravel Global Multivariate Dynamics.\u201d\nEarth System Dynamics 11 (1): 201\u201334. https://doi.org/10.5194/esd-11-201-2020.\n\n\nMontero, David, C\u00e9sar Aybar, Miguel D. Mahecha, Francesco Martinuzzi,\nMaximilian S\u00f6chting, and Sebastian Wieneke. 2023. \u201cA Standardized\nCatalogue of Spectral Indices to Advance the Use of Remote Sensing in\nEarth System Research.\u201d Scientific Data 10 (1). https://doi.org/10.1038/s41597-023-02096-0.\n\n\nMontero Loaiza, David, Guido Kraemer, Anca Anghelea, Cesar Aybar\nCamacho, Gunnar Brandt, Gustau Camps-Valls, Felix Cremer, et al. 2023.\n\u201cData Cubes for Earth System Research: Challenges Ahead,\u201d\nJuly. https://doi.org/10.31223/x58m2v.\n\n\nReddy, D. Sushma, and P. Rama Chandra Prasad. 2018. \u201cPrediction of\nVegetation Dynamics Using NDVI Time Series Data and LSTM.\u201d\nModeling Earth Systems and Environment 4 (April): 409\u201319. https://doi.org/10.1007/s40808-018-0431-3.\n\n\nRhif, Manel, Ali Ben Abbes, Beatriz Martinez, and Imed Riadh Farah.\n2020. \u201cDeep Learning Models Performance for NDVI Time Series\nPrediction: A Case Study on North West Tunisia.\u201d In 2020\nMediterranean and Middle-East Geoscience and Remote Sensing Symposium\n(M2GARSS). IEEE. https://doi.org/10.1109/m2garss47143.2020.9105149.\n\n\nSherif, Khaled, Mohamed Azmy, Khder Alakkari, Mostafa Abotaleb, and\nEl-Sayed M. El-Kenawy. 2023. \u201cDeep Learning in IoT: An LSTM\nApproach for NDVI Forecasting.\u201d In 2023 3rd International\nConference on Electronic Engineering (ICEEM). IEEE. https://doi.org/10.1109/iceem58740.2023.10319616.\n\n\nShumway, Robert H, David S Stoffer, Robert H Shumway, and David S\nStoffer. 2017. \u201cARIMA Models.\u201d Time Series Analysis and\nIts Applications: With R Examples, 75\u2013163.\n\n\nSilva, Claudionor Ribeiro da, S\u00e9rgio Lu\u00eds Dias Machado, Aracy Alves de\nAra\u00fajo, and Carlos Alberto Matias de Abreu Junior. 2018. \u201cAnalysis\nof the Phenology Dynamics of Brazilian Caatinga Species with NDVI Time\nSeries.\u201d Cerne 24 (January): 48\u201358. https://doi.org/10.1590/01047760201824012487.\n\n\nSim, Ho Jen, Choo Wei Chong, Khairil Anwar Abu Kassim, Ching Siew Mooi,\nand Zhang Yuruixian. 2022. \u201cForecasting Road Traffic Fatalities in\nMalaysia Using Seasonal Autoregressive Integrated Moving Average\n(SARIMA) Model.\u201d Pertanika Journal of Science and\nTechnology 30 (April): 897\u2013911. https://doi.org/10.47836/pjst.30.2.03.\n\n\nVasilakos, Christos, George E. Tsekouras, and Dimitris Kavroudakis.\n2022. \u201cLSTM-Based Prediction of Mediterranean Vegetation Dynamics\nUsing NDVI Time-Series Data.\u201d Land 11 (6): 923. https://doi.org/10.3390/land11060923.\n\n\nWang, Siyuan, Bojuan Yang, Qichun Yang, Linlin Lu, Xiaoyue Wang, and\nYaoyao Peng. 2016. \u201cTemporal Trends and Spatial Variability of\nVegetation Phenology over the Northern Hemisphere During\n1982-2012.\u201d PLoS ONE 11 (June). https://doi.org/10.1371/journal.pone.0157134."
    }
]